from fastapi import APIRouter, HTTPException, Depends
from typing import List, Optional
from datetime import datetime, timedelta
import sys
import os
import pandas as pd
import numpy as np
import glob
import logging

# å°å…¥å¾Œç«¯æ¨¡çµ„
root_dir = os.path.dirname(os.path.dirname(os.path.dirname(__file__)))
sys.path.insert(0, root_dir)

from src.detection.final_optimized_detector import FinalOptimizedShockDetector
from src.detection.realtime_adaptive_detector import RealtimeAdaptiveShockDetector
from src.prediction.realtime_shock_predictor import RealtimeShockPredictor

router = APIRouter()

# è³‡æ–™æ¨¡å‹
from pydantic import BaseModel

class ShockwaveData(BaseModel):
    id: str
    location_name: str
    latitude: float
    longitude: float
    intensity: float
    propagation_speed: float
    estimated_arrival: datetime
    affected_area: float
    description: Optional[str] = None
    alternative_routes: Optional[List[dict]] = None

class ShockwaveAlert(BaseModel):
    id: str
    severity: str
    title: str
    description: str
    timestamp: datetime
    location: dict
    recommendations: List[str]

def load_station_data():
    """è¼‰å…¥çœŸå¯¦çš„æ¸¬ç«™æ•¸æ“š"""
    try:
        etag_path = os.path.join(root_dir, 'data', 'Taiwan', 'Etag.csv')
        df = pd.read_csv(etag_path)
        
        # æ¸…ç†æ•¸æ“šï¼Œç§»é™¤å¤šé¤˜çš„åˆ—
        df = df.dropna(subset=['ç·¯åº¦(åŒ—ç·¯)', 'ç¶“åº¦(æ±ç¶“)'])
        
        stations = []
        for _, row in df.iterrows():
            # è§£æç·¯åº¦å’Œç¶“åº¦
            lat_str = str(row['ç·¯åº¦(åŒ—ç·¯)']).replace('N', '').strip()
            lng_str = str(row['ç¶“åº¦(æ±ç¶“)']).replace('E', '').strip()
            
            try:
                lat = float(lat_str)
                lng = float(lng_str)
                
                station = {
                    'id': int(row['ID']),  # ç¢ºä¿è½‰æ›ç‚ºPython int
                    'station_id': str(row['ç·¨è™Ÿ']),
                    'direction': str(row['æ–¹å‘']),
                    'start_ic': str(row['äº¤æµé“(èµ·)']),
                    'end_ic': str(row['äº¤æµé“(è¿„)']),
                    'latitude': float(lat),
                    'longitude': float(lng),
                    'name': f"{row['äº¤æµé“(èµ·)']} - {row['äº¤æµé“(è¿„)']}"
                }
                stations.append(station)
            except (ValueError, TypeError):
                continue
                
        return stations
    except Exception as e:
        print(f"è¼‰å…¥æ¸¬ç«™æ•¸æ“šå¤±æ•—: {e}")
        return []

def find_station_info(station_id, stations_info):
    """ç²¾ç¢ºåŒ¹é…ç«™é»è³‡è¨Š"""
    # ç›´æ¥åŒ¹é…
    for info in stations_info:
        if info['station_id'] == station_id:
            return info
    
    # è™•ç†æ ¼å¼å·®ç•°ï¼š01F0339S -> 01F-033.9S
    try:
        # è§£æç«™é»IDï¼š01F0339S
        if len(station_id) >= 8 and station_id.startswith(('01F', '03F')):
            highway = station_id[:3]  # 01F æˆ– 03F
            km_part = station_id[3:7]  # 0339
            direction = station_id[-1]  # S æˆ– N
            
            # è½‰æ›ç‚ºæ¨™æº–æ ¼å¼ï¼š01F-033.9S
            km_major = km_part[:3].lstrip('0') or '0'  # 033 -> 33
            km_minor = km_part[3]  # 9
            
            standard_format = f"{highway}-{km_major}.{km_minor}{direction}"
            
            # å°‹æ‰¾åŒ¹é…
            for info in stations_info:
                if info['station_id'] == standard_format:
                    return info
                    
            # å˜—è©¦å…¶ä»–å¯èƒ½çš„æ ¼å¼
            alt_formats = [
                f"{highway}-0{km_major}.{km_minor}{direction}",  # 01F-033.9S
                f"{highway}-{km_major}{direction}",              # 01F-33S
                f"{highway}0{km_major}.{km_minor}{direction}",   # 01F033.9S
            ]
            
            for alt_format in alt_formats:
                for info in stations_info:
                    if info['station_id'] == alt_format:
                        return info
    except:
        pass
    
    return None

@router.get("/active", response_model=dict)
async def get_active_shockwaves():
    """ç²å–ç•¶å‰æ´»èºçš„éœ‡æ³¢ - ä½¿ç”¨çœŸå¯¦æª¢æ¸¬ç³»çµ±"""
    try:
        current_time = datetime.now()
        
        # ğŸ”§ ä¿®æ­£ï¼šä½¿ç”¨çœŸå¯¦çš„éœ‡æ³¢æª¢æ¸¬ç³»çµ±
        try:
            # 1. åˆå§‹åŒ–é©æ‡‰æ€§æª¢æ¸¬å™¨å’Œé æ¸¬å™¨
            detector = RealtimeAdaptiveShockDetector()
            # ä½¿ç”¨ç›¸å°è·¯å¾‘ï¼Œå¾ API æ ¹ç›®éŒ„æ‰¾åˆ° data ç›®éŒ„
            predictor = RealtimeShockPredictor(
                data_dir=os.path.join(root_dir, 'data')
            )
            
            # 2. ç²å–æœ€æ–°çš„å³æ™‚è³‡æ–™æª”æ¡ˆ
            realtime_dir = os.path.join(root_dir, 'data', 'realtime_data')
            if not os.path.exists(realtime_dir):
                raise Exception("å³æ™‚è³‡æ–™ç›®éŒ„ä¸å­˜åœ¨ï¼Œè«‹ç¢ºä¿ TDX ç³»çµ±æ­£åœ¨é‹è¡Œ")
            
            # å°‹æ‰¾æœ€æ–°çš„è³‡æ–™æª”æ¡ˆ
            pattern = os.path.join(realtime_dir, "realtime_shock_data_*.csv")
            data_files = sorted(glob.glob(pattern), reverse=True)
            
            if not data_files:
                raise Exception("æ²’æœ‰æ‰¾åˆ°å³æ™‚è³‡æ–™æª”æ¡ˆï¼Œè«‹ç¢ºä¿ TDX è³‡æ–™æ”¶é›†å™¨æ­£åœ¨é‹è¡Œ")
            
            latest_file = data_files[0]
            file_age_minutes = (current_time - datetime.fromtimestamp(os.path.getmtime(latest_file))).total_seconds() / 60
            
            # æª¢æŸ¥æª”æ¡ˆæ–°é®®åº¦ï¼ˆè¶…é10åˆ†é˜è¦–ç‚ºéæ™‚ï¼‰
            if file_age_minutes > 10:
                raise Exception(f"æœ€æ–°è³‡æ–™æª”æ¡ˆéæ™‚ ({file_age_minutes:.1f} åˆ†é˜å‰)ï¼Œè«‹æª¢æŸ¥ TDX æ”¶é›†å™¨ç‹€æ…‹")
            
            # 3. è®€å–ä¸¦è™•ç†è³‡æ–™
            df = pd.read_csv(latest_file)
            if df.empty:
                raise Exception("è³‡æ–™æª”æ¡ˆç‚ºç©º")
            
            # 4. ä½¿ç”¨æª¢æ¸¬å™¨é€²è¡Œè¡æ“Šæ³¢åˆ†æ
            detected_shocks = []
            stations = df['station'].unique()
            
            for station in stations:
                station_data = df[df['station'] == station].sort_values(['hour', 'minute'])
                
                if len(station_data) >= 3:  # è‡³å°‘éœ€è¦3å€‹è³‡æ–™é»
                    try:
                        # æª¢æ¸¬è©²ç«™é»çš„è¡æ“Šæ³¢
                        shocks = detector.detect_realtime_shocks(station_data)
                        
                        for shock in shocks:
                            shock['station'] = station
                            shock['detection_time'] = current_time
                            detected_shocks.append(shock)
                    except Exception as e:
                        continue  # å–®å€‹ç«™é»å¤±æ•—ä¸å½±éŸ¿å…¶ä»–ç«™é»
            
            # 5. è¼‰å…¥æ¸¬ç«™ä½ç½®è³‡è¨Š
            stations_info = load_station_data()
            print(f"ğŸ” è¼‰å…¥äº† {len(stations_info)} å€‹æ¸¬ç«™")
            
            # 6. è½‰æ›ç‚ºAPIæ ¼å¼
            active_shockwaves_data = []
            
            for i, shock in enumerate(detected_shocks):
                station = shock.get('station', '')
                print(f"ğŸ” è™•ç†éœ‡æ³¢ {i+1}: æ¸¬ç«™ {station}")
                
                # æŸ¥æ‰¾æ¸¬ç«™è³‡è¨Š - ä½¿ç”¨æ–°çš„åŒ¹é…å‡½æ•¸
                station_info = find_station_info(station, stations_info)
                print(f"ğŸ” æ¸¬ç«™åŒ¹é…çµæœ: {station_info}")
                
                # ä½¿ç”¨çœŸå¯¦æª¢æ¸¬çš„æ•¸å€¼ - ç¢ºä¿è½‰æ›ç‚ºPythonåŸç”Ÿå‹åˆ¥
                speed_drop = float(shock.get('speed_drop', 0))
                intensity = min(10.0, max(1.0, speed_drop / 5.0))  # å°‡é€Ÿåº¦ä¸‹é™è½‰æ›ç‚ºå¼·åº¦ (1-10)
                propagation_speed = abs(float(shock.get('theoretical_wave_speed', 15.0)))
                confidence = 0.9 if shock.get('level') == 'severe' else 0.8 if shock.get('level') == 'moderate' else 0.7
                
                # å¦‚æœæœ‰æ¸¬ç«™è³‡è¨Šï¼Œä½¿ç”¨çœŸå¯¦ä½ç½®ï¼›å¦å‰‡ä½¿ç”¨é è¨­ä½ç½®
                if station_info:
                    latitude = station_info['latitude']
                    longitude = station_info['longitude']
                    location_name = station_info['name']
                else:
                    latitude = 25.0330 + (i * 0.01)  # é è¨­ä½ç½®
                    longitude = 121.5654 + (i * 0.01)
                    location_name = f"æ¸¬ç«™ {station}"
                
                # è¨ˆç®—é ä¼°åˆ°é”æ™‚é–“ï¼ˆåŸºæ–¼å‚³æ’­é€Ÿåº¦ï¼‰
                estimated_arrival_minutes = int(30 / (propagation_speed / 20)) if propagation_speed > 0 else 30
                
                # æ§‹å»ºçœŸå¯¦çš„è¡æ“Šæ³¢ç™¼ç”Ÿæ™‚é–“
                shock_start_time = shock.get('start_time', '00:00')
                shock_end_time = shock.get('end_time', '00:00')
                
                # å¾è³‡æ–™ä¸­ç²å–æ—¥æœŸ
                shock_date = df.iloc[0]['date'] if not df.empty else current_time.strftime('%Y/%m/%d')
                
                # è§£æè¡æ“Šæ³¢å¯¦éš›ç™¼ç”Ÿæ™‚é–“
                try:
                    start_hour, start_minute = map(int, shock_start_time.split(':'))
                    shock_datetime = datetime.strptime(f"{shock_date} {start_hour:02d}:{start_minute:02d}", '%Y/%m/%d %H:%M')
                except:
                    shock_datetime = current_time
                
                shockwave_data = {
                    "id": f"real_sw_{station}_{shock_start_time.replace(':', '')}",
                    "station_id": station,
                    "location_name": location_name,
                    "latitude": latitude,
                    "longitude": longitude,
                    "intensity": round(float(intensity), 1),
                    "propagation_speed": round(float(propagation_speed), 1),
                    "estimated_arrival": (current_time + timedelta(minutes=estimated_arrival_minutes)).isoformat(),
                    "affected_area": round(intensity * 0.5, 1),  # åŸºæ–¼å¼·åº¦è¨ˆç®—å½±éŸ¿ç¯„åœ
                    "description": f"åœ¨æ¸¬ç«™ {station} ({location_name}) æª¢æ¸¬åˆ°çœŸå¯¦äº¤é€šè¡æ“Šæ³¢ (ä¿¡å¿ƒåº¦: {confidence:.2f})",
                    "confidence": round(float(confidence), 3),
                    "detection_method": "RealtimeAdaptiveShockDetector",
                    "shock_occurrence_time": shock_datetime.isoformat(),  # çœŸå¯¦ç™¼ç”Ÿæ™‚é–“
                    "shock_start_time": shock_start_time,
                    "shock_end_time": shock_end_time,
                    "shock_duration": float(shock.get('duration', 0)),
                    "speed_drop": float(shock.get('speed_drop', 0)),
                    "data_source_time": datetime.fromtimestamp(os.path.getmtime(latest_file)).isoformat(),
                    "alternative_routes": []
                }
                
                # ç‚ºé«˜å¼·åº¦è¡æ“Šæ³¢æ·»åŠ æ›¿ä»£è·¯ç·šå»ºè­°
                if intensity >= 6.0:
                    shockwave_data["alternative_routes"] = [
                        {
                            "id": f"alt_real_{station}",
                            "name": "å»ºè­°ä½¿ç”¨æ›¿ä»£è·¯ç·š",
                            "additional_time": max(10, int(intensity * 3)),
                            "avoidance_success": min(90, int(confidence * 100))
                        }
                    ]
                
                active_shockwaves_data.append(shockwave_data)
            
            # 7. å›å‚³çµæœ
            response_data = {
                "shockwaves": active_shockwaves_data,
                "count": len(active_shockwaves_data),
                "last_updated": current_time.isoformat(),
                "data_source": "real_time_detector",  # æ¨™è¨˜ç‚ºçœŸå¯¦æª¢æ¸¬
                "data_file_used": os.path.basename(latest_file),
                "data_file_age_minutes": round(file_age_minutes, 1),
                "total_stations_analyzed": len(stations),
                "detection_summary": {
                    "files_checked": len(data_files),
                    "latest_file": os.path.basename(latest_file),
                    "stations_with_data": len(stations),
                    "shocks_detected": len(detected_shocks)
                }
            }
            
            return response_data
            
        except Exception as detection_error:
            # å¦‚æœçœŸå¯¦æª¢æ¸¬å¤±æ•—ï¼Œè¨˜éŒ„éŒ¯èª¤ä¸¦ä½¿ç”¨æœ€å°åŒ–çš„æ¨¡æ“¬è³‡æ–™
            logger = logging.getLogger(__name__)
            logger.error(f"çœŸå¯¦éœ‡æ³¢æª¢æ¸¬å¤±æ•—: {detection_error}")
            
            # å›é€€åˆ°è¼‰å…¥æ¸¬ç«™è³‡è¨Šä½†ä¸ç”Ÿæˆå‡éœ‡æ³¢
            stations = load_station_data()
            
            return {
                "shockwaves": [],  # ç©ºåˆ—è¡¨ï¼Œè¡¨ç¤ºç›®å‰æ²’æœ‰æª¢æ¸¬åˆ°éœ‡æ³¢
                "count": 0,
                "last_updated": current_time.isoformat(),
                "data_source": "detector_failed",
                "error": str(detection_error),
                "available_stations": len(stations),
                "status": "æª¢æ¸¬ç³»çµ±æš«æ™‚ç„¡æ³•ä½¿ç”¨ï¼Œè«‹æª¢æŸ¥è³‡æ–™æ”¶é›†ç‹€æ…‹"
            }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ç²å–éœ‡æ³¢è³‡æ–™å¤±æ•—: {str(e)}")

@router.get("/status")
async def get_system_status():
    """ç²å–ç³»çµ±ç‹€æ…‹ - ç”¨æ–¼è¨ºæ–·"""
    try:
        current_time = datetime.now()
        
        # æª¢æŸ¥å„å€‹çµ„ä»¶ç‹€æ…‹
        status_info = {
            "timestamp": current_time.isoformat(),
            "detector_available": False,
            "predictor_available": False,
            "data_files_status": {},
            "tdx_system_status": "unknown"
        }
        
        # æª¢æŸ¥æª¢æ¸¬å™¨
        try:
            detector = FinalOptimizedShockDetector()
            status_info["detector_available"] = True
        except Exception as e:
            status_info["detector_error"] = str(e)
        
        # æª¢æŸ¥é æ¸¬å™¨
        try:
            predictor = RealtimeShockPredictor(os.path.join(root_dir, 'data'))
            status_info["predictor_available"] = True
        except Exception as e:
            status_info["predictor_error"] = str(e)
        
        # æª¢æŸ¥è³‡æ–™æª”æ¡ˆç‹€æ…‹
        realtime_dir = os.path.join(root_dir, 'data', 'realtime_data')
        if os.path.exists(realtime_dir):
            pattern = os.path.join(realtime_dir, "realtime_shock_data_*.csv")
            data_files = sorted(glob.glob(pattern), reverse=True)
            
            if data_files:
                latest_file = data_files[0]
                file_age = (current_time - datetime.fromtimestamp(os.path.getmtime(latest_file))).total_seconds() / 60
                
                status_info["data_files_status"] = {
                    "total_files": len(data_files),
                    "latest_file": os.path.basename(latest_file),
                    "latest_file_age_minutes": round(file_age, 1),
                    "data_freshness": "fresh" if file_age < 10 else "stale" if file_age < 60 else "old"
                }
                
                # æª¢æŸ¥æœ€æ–°æª”æ¡ˆå…§å®¹
                try:
                    df = pd.read_csv(latest_file)
                    status_info["data_files_status"]["latest_file_records"] = len(df)
                    status_info["data_files_status"]["latest_file_stations"] = df['station'].nunique() if not df.empty else 0
                except:
                    status_info["data_files_status"]["latest_file_readable"] = False
            else:
                status_info["data_files_status"] = {"message": "æ²’æœ‰æ‰¾åˆ°è³‡æ–™æª”æ¡ˆ"}
        else:
            status_info["data_files_status"] = {"message": "è³‡æ–™ç›®éŒ„ä¸å­˜åœ¨"}
        
        # æª¢æŸ¥æ¸¬ç«™è³‡è¨Š
        try:
            stations = load_station_data()
            status_info["station_info"] = {
                "total_stations": len(stations),
                "stations_loaded": True
            }
        except Exception as e:
            status_info["station_info"] = {
                "stations_loaded": False,
                "error": str(e)
            }
        
        return status_info
        
    except Exception as e:
        return {
            "error": str(e),
            "status": "failed",
            "timestamp": datetime.now().isoformat()
        }

@router.get("/stations", response_model=dict)
async def get_station_list():
    """ç²å–æ‰€æœ‰æ¸¬ç«™åˆ—è¡¨"""
    try:
        stations = load_station_data()
        return {
            "stations": stations,
            "total_count": len(stations)
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ç²å–æ¸¬ç«™åˆ—è¡¨å¤±æ•—: {str(e)}")

@router.get("/predict")
async def predict_shockwave_propagation(
    shockwave_id: str,
    time_horizon: int = 60  # é æ¸¬æ™‚é–“ç¯„åœï¼ˆåˆ†é˜ï¼‰
):
    """é æ¸¬éœ‡æ³¢å‚³æ’­è·¯å¾‘å’Œåˆ°é”æ™‚é–“"""
    try:
        # é€™è£¡èª¿ç”¨éœ‡æ³¢é æ¸¬ç³»çµ±
        # predictor = RealtimeShockPredictor()
        # prediction = predictor.predict_propagation(shockwave_id, time_horizon)
        
        # æ¨¡æ“¬é æ¸¬çµæœ
        current_time = datetime.now()
        prediction_data = {
            "shockwave_id": shockwave_id,
            "propagation_path": [
                {
                    "station_id": "001",
                    "estimated_arrival": (current_time + timedelta(minutes=5)).isoformat(),
                    "intensity": 7.0,
                    "confidence": 0.92
                },
                {
                    "station_id": "002", 
                    "estimated_arrival": (current_time + timedelta(minutes=15)).isoformat(),
                    "intensity": 6.5,
                    "confidence": 0.87
                },
                {
                    "station_id": "003",
                    "estimated_arrival": (current_time + timedelta(minutes=25)).isoformat(), 
                    "intensity": 5.8,
                    "confidence": 0.78
                }
            ],
            "total_affected_distance": 15.2,
            "prediction_confidence": 0.85,
            "generated_at": current_time.isoformat()
        }
        
        return prediction_data
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"éœ‡æ³¢é æ¸¬å¤±æ•—: {str(e)}")

@router.get("/history")
async def get_shockwave_history(
    start_time: Optional[datetime] = None,
    end_time: Optional[datetime] = None,
    limit: int = 50
):
    """ç²å–æ­·å²éœ‡æ³¢è¨˜éŒ„"""
    try:
        if not end_time:
            end_time = datetime.now()
        if not start_time:
            start_time = end_time - timedelta(days=7)
            
        # é€™è£¡å¯¦ä½œæ­·å²éœ‡æ³¢æŸ¥è©¢
        return {
            "history": [],
            "start_time": start_time.isoformat(),
            "end_time": end_time.isoformat(),
            "total_count": 0
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ç²å–æ­·å²éœ‡æ³¢å¤±æ•—: {str(e)}")

@router.post("/alert/dismiss")
async def dismiss_shockwave_alert(alert_id: str):
    """é—œé–‰éœ‡æ³¢è­¦å ±"""
    try:
        # å¯¦ä½œè­¦å ±é—œé–‰é‚è¼¯
        return {
            "message": f"è­¦å ± {alert_id} å·²é—œé–‰",
            "dismissed_at": datetime.now().isoformat()
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"é—œé–‰è­¦å ±å¤±æ•—: {str(e)}")

@router.get("/statistics")
async def get_shockwave_statistics():
    """ç²å–éœ‡æ³¢çµ±è¨ˆè³‡æ–™"""
    try:
        # è¨ˆç®—éœ‡æ³¢çµ±è¨ˆ
        stats = {
            "today_total": 12,
            "active_count": 2,
            "average_intensity": 5.8,
            "most_affected_highway": "åœ‹é“1è™Ÿ",
            "prediction_accuracy": 0.87,
            "last_24h_trend": "increasing"
        }
        
        return stats
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ç²å–çµ±è¨ˆè³‡æ–™å¤±æ•—: {str(e)}")

@router.get("/debug/latest-data")
async def debug_latest_data():
    """é™¤éŒ¯ç«¯é»ï¼šæŸ¥çœ‹æœ€æ–°è³‡æ–™"""
    try:
        realtime_dir = os.path.join(root_dir, 'data', 'realtime_data')
        pattern = os.path.join(realtime_dir, "realtime_shock_data_*.csv")
        data_files = sorted(glob.glob(pattern), reverse=True)
        
        if not data_files:
            return {"message": "æ²’æœ‰è³‡æ–™æª”æ¡ˆ"}
        
        latest_file = data_files[0]
        df = pd.read_csv(latest_file)
        
        return {
            "file": os.path.basename(latest_file),
            "records": len(df),
            "stations": df['station'].nunique() if not df.empty else 0,
            "sample_data": df.head(5).to_dict('records') if not df.empty else [],
            "columns": list(df.columns) if not df.empty else []
        }
    except Exception as e:
        return {"error": str(e)}
