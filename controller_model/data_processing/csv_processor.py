"""
CSV è³‡æ–™é è™•ç†æ¨¡çµ„
è™•ç†åœ‹é“ä¸€è™Ÿå’Œåœ‹é“ä¸‰è™Ÿçš„æ•´åˆè³‡æ–™
"""

import pandas as pd
import numpy as np
import os
from typing import List, Dict, Any, Tuple
from loguru import logger
import jieba
import json
import yaml

# å°å…¥é…ç½®ç®¡ç†å™¨
import sys
from pathlib import Path
sys.path.append(str(Path(__file__).parent.parent))
from train_model.utils.config_manager import get_config_manager

class HighwayCSVProcessor:
    """åœ‹é“CSVè³‡æ–™è™•ç†å™¨"""
    
    def __init__(self, config_path: str = None):
        """åˆå§‹åŒ–è™•ç†å™¨"""
        # ä½¿ç”¨é…ç½®ç®¡ç†å™¨
        if config_path:
            os.environ['RAG_CONFIG_PATH'] = config_path
        
        self.config_manager = get_config_manager()
        self.config = self.config_manager.get_config()
        
        self.data_config = self.config['data_processing']
        self.chunking_config = self.config['chunking']
        
        # åˆå§‹åŒ–ä¸­æ–‡åˆ†è©
        jieba.initialize()
        
        logger.info("CSVè™•ç†å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def load_highway_data(self) -> Tuple[pd.DataFrame, pd.DataFrame]:
        """è¼‰å…¥åœ‹é“ä¸€è™Ÿå’Œä¸‰è™Ÿè³‡æ–™"""
        base_path = self.config_manager.resolve_path(self.data_config['input_data_path'])
        
        # æª¢æŸ¥æ•¸æ“šç›®éŒ„æ˜¯å¦å­˜åœ¨
        if not os.path.exists(base_path):
            logger.error(f"æ•¸æ“šç›®éŒ„ä¸å­˜åœ¨: {base_path}")
            raise FileNotFoundError(f"æ‰¾ä¸åˆ°æ•¸æ“šç›®éŒ„: {base_path}")
        
        # è¼‰å…¥åœ‹é“ä¸€è™Ÿè³‡æ–™
        highway1_path = os.path.join(base_path, self.data_config['highway1_file'])
        if not os.path.exists(highway1_path):
            logger.error(f"åœ‹é“ä¸€è™Ÿæ•¸æ“šæ–‡ä»¶ä¸å­˜åœ¨: {highway1_path}")
            raise FileNotFoundError(f"æ‰¾ä¸åˆ°åœ‹é“ä¸€è™Ÿæ•¸æ“šæ–‡ä»¶: {highway1_path}")
            
        try:
            highway1_df = pd.read_csv(highway1_path, encoding='utf-8')
            logger.info(f"è¼‰å…¥åœ‹é“ä¸€è™Ÿè³‡æ–™: {len(highway1_df)} ç­†è¨˜éŒ„")
        except Exception as e:
            logger.error(f"è®€å–åœ‹é“ä¸€è™Ÿæ•¸æ“šæ–‡ä»¶å¤±æ•—: {e}")
            raise
        
        # è¼‰å…¥åœ‹é“ä¸‰è™Ÿè³‡æ–™
        highway3_path = os.path.join(base_path, self.data_config['highway3_file'])
        if not os.path.exists(highway3_path):
            logger.error(f"åœ‹é“ä¸‰è™Ÿæ•¸æ“šæ–‡ä»¶ä¸å­˜åœ¨: {highway3_path}")
            raise FileNotFoundError(f"æ‰¾ä¸åˆ°åœ‹é“ä¸‰è™Ÿæ•¸æ“šæ–‡ä»¶: {highway3_path}")
            
        try:
            highway3_df = pd.read_csv(highway3_path, encoding='utf-8')
            logger.info(f"è¼‰å…¥åœ‹é“ä¸‰è™Ÿè³‡æ–™: {len(highway3_df)} ç­†è¨˜éŒ„")
        except Exception as e:
            logger.error(f"è®€å–åœ‹é“ä¸‰è™Ÿæ•¸æ“šæ–‡ä»¶å¤±æ•—: {e}")
            raise
        
        return highway1_df, highway3_df
    
    def clean_and_normalize_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """æ¸…ç†å’Œæ­£è¦åŒ–è³‡æ–™"""
        # è¤‡è£½è³‡æ–™æ¡†é¿å…ä¿®æ”¹åŸå§‹è³‡æ–™
        cleaned_df = df.copy()
        
        # è™•ç†ç¼ºå¤±å€¼
        cleaned_df = cleaned_df.fillna("")
        
        # æ­£è¦åŒ–æ–‡å­—æ¬„ä½
        text_columns = ['é‹ªé¢ç¨®é¡', 'æ§½åŒ–å€', 'å…§è·¯è‚©', 'å¤–è·¯è‚©', 'è¼”åŠ©è»Šé“1', 'è¼”åŠ©è»Šé“2', 'è¼”åŠ©è»Šé“3', 'é¿è»Šå½']
        for col in text_columns:
            if col in cleaned_df.columns:
                cleaned_df[col] = cleaned_df[col].astype(str).str.strip()
        
        # è™•ç†æ•¸å€¼æ¬„ä½
        numeric_columns = ['é‡Œç¨‹', 'ç¶“ç·¯åº¦åæ¨™Lon', 'ç¶“ç·¯åº¦åæ¨™Lat', 'è·¯å¹…å¯¬', 'å…¨è·¯å¹…å¯¬', 
                          'è»Šé“æ•¸', 'æ›²ç‡åŠå¾‘', 'ç¸±å‘å¡åº¦', 'æ©«å‘å¡åº¦']
        
        numeric_conversion_stats = {}
        for col in numeric_columns:
            if col in cleaned_df.columns:
                original_count = cleaned_df[col].notna().sum()
                # è½‰æ›æ•¸å€¼ï¼Œè¨˜éŒ„éŒ¯èª¤
                converted_series = pd.to_numeric(cleaned_df[col], errors='coerce')
                converted_count = converted_series.notna().sum()
                
                # è¨˜éŒ„è½‰æ›çµ±è¨ˆ
                numeric_conversion_stats[col] = {
                    'original_valid': original_count,
                    'converted_valid': converted_count,
                    'conversion_errors': original_count - converted_count
                }
                
                # ç”¨é©ç•¶çš„é è¨­å€¼å¡«å……
                if col in ['ç¶“ç·¯åº¦åæ¨™Lon', 'ç¶“ç·¯åº¦åæ¨™Lat']:
                    # åº§æ¨™ç”¨ NaNï¼Œå¾ŒçºŒå¯ä»¥éæ¿¾æˆ–ç‰¹åˆ¥è™•ç†
                    cleaned_df[col] = converted_series
                else:
                    # å…¶ä»–æ•¸å€¼ç”¨ 0 å¡«å……
                    cleaned_df[col] = converted_series.fillna(0)
                    
                # è¨˜éŒ„è½‰æ›è­¦å‘Š
                if numeric_conversion_stats[col]['conversion_errors'] > 0:
                    logger.warning(
                        f"æ¬„ä½ {col}: {numeric_conversion_stats[col]['conversion_errors']} å€‹å€¼ç„¡æ³•è½‰æ›ç‚ºæ•¸å€¼"
                    )
        
        # è¨˜éŒ„ç¸½é«”è½‰æ›çµ±è¨ˆ
        total_errors = sum(stats['conversion_errors'] for stats in numeric_conversion_stats.values())
        if total_errors > 0:
            logger.info(f"æ•¸å€¼æ¬„ä½è½‰æ›å®Œæˆï¼Œå…± {total_errors} å€‹è½‰æ›éŒ¯èª¤")
        
        logger.info(f"è³‡æ–™æ¸…ç†å®Œæˆï¼Œå‰©é¤˜ {len(cleaned_df)} ç­†è¨˜éŒ„")
        return cleaned_df
    
    def generate_text_descriptions(self, df: pd.DataFrame) -> List[str]:
        """å°‡çµæ§‹åŒ–è³‡æ–™è½‰æ›ç‚ºæ–‡å­—æè¿°"""
        descriptions = []
        
        for idx, row in df.iterrows():
            # åŸºæœ¬è³‡è¨Š
            desc = f"èª¿æŸ¥æ—¥æœŸ: {row['èª¿æŸ¥æ—¥æœŸ']}\n"
            desc += f"åœ‹é“ç·¨è™Ÿæ–¹å‘: {row['åœ‹é“ç·¨è™Ÿæ–¹å‘']}\n"
            desc += f"æ¨è™Ÿ: {row['æ¨è™Ÿ']} (é‡Œç¨‹ {row['é‡Œç¨‹']}å…¬å°º)\n"
            desc += f"ä½ç½®åº§æ¨™: ç¶“åº¦ {row['ç¶“ç·¯åº¦åæ¨™Lon']}, ç·¯åº¦ {row['ç¶“ç·¯åº¦åæ¨™Lat']}\n"
            
            # è·¯é¢è³‡è¨Š
            desc += f"é‹ªé¢ç¨®é¡: {row['é‹ªé¢ç¨®é¡']}\n"
            desc += f"è·¯å¹…å¯¬åº¦: {row['è·¯å¹…å¯¬']}å…¬å°º (å…¨è·¯å¹…å¯¬ {row['å…¨è·¯å¹…å¯¬']}å…¬å°º)\n"
            desc += f"è»Šé“æ•¸: {row['è»Šé“æ•¸']}å€‹\n"
            
            # è»Šé“å¯¬åº¦è³‡è¨Š
            lane_widths = []
            for i in range(1, 7):
                col = f'è»Šé“{i}å¯¬'
                if col in row and pd.notna(row[col]) and row[col] > 0:
                    lane_widths.append(f"è»Šé“{i}: {row[col]}å…¬å°º")
            if lane_widths:
                desc += f"è»Šé“å¯¬åº¦: {', '.join(lane_widths)}\n"
            
            # è·¯è‚©è³‡è¨Š
            if row.get('å…§è·¯è‚©') and row['å…§è·¯è‚©'] != 'ç„¡' and row.get('å…§è·¯è‚©å¯¬', 0) > 0:
                desc += f"å…§è·¯è‚©: æœ‰ (å¯¬åº¦ {row['å…§è·¯è‚©å¯¬']}å…¬å°º)\n"
            if row.get('å¤–è·¯è‚©') and row['å¤–è·¯è‚©'] != 'ç„¡' and row.get('å¤–è·¯è‚©å¯¬', 0) > 0:
                desc += f"å¤–è·¯è‚©: æœ‰ (å¯¬åº¦ {row['å¤–è·¯è‚©å¯¬']}å…¬å°º)\n"
            
            # è¼”åŠ©è»Šé“è³‡è¨Š
            aux_lanes = []
            for i in range(1, 4):
                col = f'è¼”åŠ©è»Šé“{i}'
                width_col = f'è¼”åŠ©è»Šé“{i}å¯¬'
                if col in row and row[col] and row[col] != 'ç„¡' and row.get(width_col, 0) > 0:
                    aux_lanes.append(f"è¼”åŠ©è»Šé“{i}: {row[width_col]}å…¬å°º")
            if aux_lanes:
                desc += f"è¼”åŠ©è»Šé“: {', '.join(aux_lanes)}\n"
            
            # å¹¾ä½•ç‰¹å¾µ
            if row.get('æ›²ç‡åŠå¾‘', 0) > 0:
                desc += f"æ›²ç‡åŠå¾‘: {row['æ›²ç‡åŠå¾‘']}å…¬å°º\n"
            desc += f"ç¸±å‘å¡åº¦: {row['ç¸±å‘å¡åº¦']}\n"
            desc += f"æ©«å‘å¡åº¦: {row['æ©«å‘å¡åº¦']}\n"
            
            descriptions.append(desc.strip())
        
        logger.info(f"ç”Ÿæˆ {len(descriptions)} å€‹æ–‡å­—æè¿°")
        return descriptions
    
    def chunk_text(self, text: str) -> List[str]:
        """å°‡é•·æ–‡æœ¬åˆ†å‰²æˆå°å¡Š"""
        chunk_size = self.chunking_config['chunk_size']
        chunk_overlap = self.chunking_config['chunk_overlap']
        separators = self.chunking_config['separators']
        
        # ä½¿ç”¨åˆ†éš”ç¬¦åˆ†å‰²æ–‡æœ¬
        chunks = [text]
        for separator in separators:
            new_chunks = []
            for chunk in chunks:
                new_chunks.extend(chunk.split(separator))
            chunks = new_chunks
        
        # éæ¿¾ç©ºç™½å¡Š
        chunks = [chunk.strip() for chunk in chunks if chunk.strip()]
        
        # å¦‚æœå¡Šå¤ªå¤§ï¼Œé€²ä¸€æ­¥åˆ†å‰²
        final_chunks = []
        for chunk in chunks:
            if len(chunk) <= chunk_size:
                final_chunks.append(chunk)
            else:
                # ä½¿ç”¨æ»‘å‹•çª—å£åˆ†å‰²å¤§å¡Š
                for i in range(0, len(chunk), chunk_size - chunk_overlap):
                    sub_chunk = chunk[i:i + chunk_size]
                    if sub_chunk.strip():
                        final_chunks.append(sub_chunk.strip())
        
        return final_chunks
    
    def process_all_data(self) -> List[Dict[str, Any]]:
        """è™•ç†æ‰€æœ‰è³‡æ–™ä¸¦ç”Ÿæˆè¨“ç·´æ ¼å¼"""
        # è¼‰å…¥è³‡æ–™
        highway1_df, highway3_df = self.load_highway_data()
        
        # æ¸…ç†è³‡æ–™
        highway1_clean = self.clean_and_normalize_data(highway1_df)
        highway3_clean = self.clean_and_normalize_data(highway3_df)
        
        # ç”Ÿæˆæ–‡å­—æè¿°
        highway1_texts = self.generate_text_descriptions(highway1_clean)
        highway3_texts = self.generate_text_descriptions(highway3_clean)
        
        # åˆä½µæ‰€æœ‰æ–‡æœ¬
        all_texts = highway1_texts + highway3_texts
        
        # åˆ†å‰²æ–‡æœ¬
        processed_data = []
        for i, text in enumerate(all_texts):
            chunks = self.chunk_text(text)
            for j, chunk in enumerate(chunks):
                processed_data.append({
                    'id': f'highway_data_{i}_{j}',
                    'text': chunk,
                    'source': 'highway1' if i < len(highway1_texts) else 'highway3',
                    'chunk_index': j,
                    'original_index': i
                })
        
        logger.info(f"è™•ç†å®Œæˆï¼Œç¸½å…±ç”Ÿæˆ {len(processed_data)} å€‹æ–‡æœ¬å¡Š")
        
        # çµ±è¨ˆè³‡æ–™ä¾†æºåˆ†å¸ƒ
        source_stats = {}
        for item in processed_data:
            source = item['source']
            source_stats[source] = source_stats.get(source, 0) + 1
        
        logger.info(f"è³‡æ–™ä¾†æºåˆ†å¸ƒ: {source_stats}")
        
        # æª¢æŸ¥æ–‡æœ¬å¡Šé•·åº¦åˆ†å¸ƒ
        text_lengths = [len(item['text']) for item in processed_data]
        if text_lengths:
            avg_length = sum(text_lengths) / len(text_lengths)
            min_length = min(text_lengths)
            max_length = max(text_lengths)
            logger.info(f"æ–‡æœ¬å¡Šé•·åº¦çµ±è¨ˆ - å¹³å‡: {avg_length:.1f}, æœ€å°: {min_length}, æœ€å¤§: {max_length}")
        
        return processed_data
    
    def save_processed_data(self, data: List[Dict[str, Any]], output_file: str = "processed_highway_data.json"):
        """å„²å­˜è™•ç†å¾Œçš„è³‡æ–™"""
        output_dir = self.config_manager.resolve_path(self.data_config['output_dir'])
        
        try:
            os.makedirs(output_dir, exist_ok=True)
            output_path = os.path.join(output_dir, output_file)
            
            # é©—è­‰è³‡æ–™å®Œæ•´æ€§
            if not data:
                logger.warning("æ²’æœ‰è³‡æ–™éœ€è¦å„²å­˜")
                return None
            
            # æª¢æŸ¥è³‡æ–™æ ¼å¼
            required_fields = ['id', 'text', 'source']
            for i, item in enumerate(data[:5]):  # æª¢æŸ¥å‰5å€‹é …ç›®
                missing_fields = [field for field in required_fields if field not in item]
                if missing_fields:
                    logger.error(f"è³‡æ–™é …ç›® {i} ç¼ºå°‘å¿…è¦æ¬„ä½: {missing_fields}")
                    raise ValueError(f"è³‡æ–™æ ¼å¼ä¸æ­£ç¢ºï¼Œç¼ºå°‘æ¬„ä½: {missing_fields}")
            
            import json
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            
            # é©—è­‰æª”æ¡ˆæ˜¯å¦æ­£ç¢ºå¯«å…¥
            if os.path.exists(output_path) and os.path.getsize(output_path) > 0:
                logger.info(f"è™•ç†å¾Œè³‡æ–™å·²å„²å­˜è‡³: {output_path}")
                logger.info(f"æª”æ¡ˆå¤§å°: {os.path.getsize(output_path)} bytes")
                logger.info(f"è³‡æ–™é …ç›®æ•¸é‡: {len(data)}")
                return output_path
            else:
                logger.error("æª”æ¡ˆå„²å­˜å¤±æ•—æˆ–æª”æ¡ˆç‚ºç©º")
                return None
                
        except Exception as e:
            logger.error(f"å„²å­˜è™•ç†å¾Œè³‡æ–™æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            raise

class DataProcessor:
    
    def __init__(self, config_path: str = "train_model/configs/rag_config.yaml"):
        """è¼‰å…¥è¨­å®š"""
        with open(config_path, 'r', encoding='utf-8') as f:
            self.config = yaml.safe_load(f)
        self.data_path = self.config['data_processing']['input_data_path']
    
    # æ‚¨ç¾æœ‰çš„CSVè™•ç†æ–¹æ³•ä¿æŒä¸è®Š...
    
    def load_json_data(self, filename: str) -> List[Dict[str, Any]]:
        """è¼‰å…¥JSONæª”æ¡ˆ"""
        file_path = os.path.join(self.data_path, filename)
        
        if not os.path.exists(file_path):
            print(f"âš ï¸ JSONæª”æ¡ˆä¸å­˜åœ¨: {file_path}")
            return []
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # çµ±ä¸€è½‰æ›ç‚ºåˆ—è¡¨æ ¼å¼
            if isinstance(data, dict):
                data = [data]
            
            # æ–°å¢ä¾†æºæ¨™è¨˜
            highway_type = "åœ‹é“ä¸€è™Ÿ" if "N01" in filename else "åœ‹é“ä¸‰è™Ÿ"
            for item in data:
                if isinstance(item, dict):
                    item['_highway_type'] = highway_type
                    item['_source_file'] = filename
                    item['_data_category'] = 'geometric_statistical'
            
            print(f"âœ… æˆåŠŸè¼‰å…¥JSON: {filename}, å…± {len(data)} ç­†è³‡æ–™")
            return data
            
        except Exception as e:
            print(f"âŒ è¼‰å…¥JSONå¤±æ•— {filename}: {e}")
            return []
    
    def json_to_text_chunks(self, json_data: List[Dict[str, Any]]) -> List[str]:
        """å°‡JSONè³‡æ–™è½‰æ›ç‚ºæ–‡å­—å¡Š"""
        if not json_data:
            return []
        
        text_chunks = []
        for item in json_data:
            if isinstance(item, dict):
                text_chunk = self._convert_json_item_to_text(item)
                if text_chunk:
                    text_chunks.append(text_chunk)
        
        print(f"âœ… JSONè½‰æ–‡å­—å®Œæˆ: ç”¢ç”Ÿ {len(text_chunks)} å€‹æ–‡å­—å¡Š")
        return text_chunks
    
    def _convert_json_item_to_text(self, item: Dict[str, Any]) -> str:
        """å°‡å–®å€‹JSONé …ç›®è½‰æ›ç‚ºæ–‡å­—"""
        text_parts = []
        
        # åŸºæœ¬è³‡è¨Š
        highway_type = item.get('_highway_type', 'æœªçŸ¥åœ‹é“')
        data_category = item.get('_data_category', 'æœªçŸ¥é¡åˆ¥')
        
        text_parts.append(f"è³‡æ–™ä¾†æº: {highway_type}")
        text_parts.append(f"è³‡æ–™é¡åˆ¥: {data_category}")
        
        # è™•ç†å…¶ä»–æ¬„ä½ï¼ˆæ’é™¤å…§éƒ¨æ¨™è¨˜ï¼‰
        for key, value in item.items():
            if key.startswith('_'):  # è·³éå…§éƒ¨æ¨™è¨˜
                continue
                
            if isinstance(value, dict):
                # è™•ç†åµŒå¥—å­—å…¸
                dict_items = []
                for k, v in value.items():
                    if not isinstance(v, (dict, list)):
                        dict_items.append(f"{k}={v}")
                if dict_items:
                    text_parts.append(f"{key}: {', '.join(dict_items[:5])}")
            
            elif isinstance(value, list):
                # è™•ç†åˆ—è¡¨
                if len(value) <= 5 and all(not isinstance(x, (dict, list)) for x in value):
                    text_parts.append(f"{key}: {', '.join(map(str, value))}")
                else:
                    text_parts.append(f"{key}: [åŒ…å«{len(value)}å€‹é …ç›®]")
            
            else:
                # è™•ç†ç°¡å–®å€¼
                text_parts.append(f"{key}: {value}")
        
        return " | ".join(text_parts[:20])  # é™åˆ¶æ¬„ä½æ•¸é‡é¿å…éé•·
    
    def process_all_data(self) -> Dict[str, List[str]]:
        """è™•ç†æ‰€æœ‰è³‡æ–™ä¾†æº"""
        result = {
            'csv_chunks': [],
            'json_chunks': [],
            'total_chunks': []
        }
        
        data_sources = self.config['data_processing'].get('data_sources', ['csv'])
        
        # è™•ç†CSVè³‡æ–™ (å¦‚æœåœ¨data_sourcesä¸­)
        if 'csv' in data_sources:
            csv_files = self.config['data_processing']['csv_files']
            for key, filename in csv_files.items():
                print(f"ğŸ”„ è™•ç†CSV: {filename}")
                # é€™è£¡å‘¼å«æ‚¨ç¾æœ‰çš„CSVè™•ç†æ–¹æ³•
                # csv_chunks = self.process_csv(filename)  # æ‚¨ç¾æœ‰çš„æ–¹æ³•
                # result['csv_chunks'].extend(csv_chunks)
        
        # è™•ç†JSONè³‡æ–™ (å¦‚æœåœ¨data_sourcesä¸­)
        if 'json' in data_sources:
            json_files = self.config['data_processing']['json_files']
            for key, filename in json_files.items():
                print(f"ğŸ”„ è™•ç†JSON: {filename}")
                json_data = self.load_json_data(filename)
                if json_data:
                    json_chunks = self.json_to_text_chunks(json_data)
                    result['json_chunks'].extend(json_chunks)
        
        # åˆä½µæ‰€æœ‰æ–‡å­—å¡Š
        result['total_chunks'] = result['csv_chunks'] + result['json_chunks']
        
        print(f"ğŸ“Š è™•ç†å®Œæˆ:")
        print(f"   CSVæ–‡å­—å¡Š: {len(result['csv_chunks'])}")
        print(f"   JSONæ–‡å­—å¡Š: {len(result['json_chunks'])}")
        print(f"   ç¸½è¨ˆ: {len(result['total_chunks'])}")
        
        return result
    
    def save_processed_data(self, data: Dict[str, List[str]]) -> None:
        """å„²å­˜è™•ç†å¾Œçš„è³‡æ–™åˆ°output_dir"""
        output_dir = self.config['data_processing']['output_dir']
        os.makedirs(output_dir, exist_ok=True)
        
        for data_type, chunks in data.items():
            if chunks:
                output_file = os.path.join(output_dir, f"{data_type}.json")
                with open(output_file, 'w', encoding='utf-8') as f:
                    json.dump(chunks, f, ensure_ascii=False, indent=2)
                print(f"ğŸ’¾ å·²å„²å­˜ {data_type}: {len(chunks)} å€‹æ–‡å­—å¡Š -> {output_file}")

if __name__ == "__main__":
    # æ¸¬è©¦è™•ç†å™¨
    processor = HighwayCSVProcessor()
    processed_data = processor.process_all_data()
    processor.save_processed_data(processed_data)
    
    print(f"è™•ç†å®Œæˆï¼ç”Ÿæˆäº† {len(processed_data)} å€‹æ–‡æœ¬å¡Š")
    print("ç¯„ä¾‹æ–‡æœ¬å¡Š:")
    for i in range(min(3, len(processed_data))):
        print(f"\n--- å¡Š {i+1} ---")
        print(processed_data[i]['text'][:200] + "...")