"""
å¢å¼·çš„ CSV è³‡æ–™è™•ç†å™¨
è§£æ±ºä»£è™Ÿæª¢ç´¢å•é¡Œï¼Œæ·»åŠ é§•é§›å‹å–„çš„è·¯æ®µæè¿°
"""

import pandas as pd
import numpy as np
import os
import json
from typing import List, Dict, Any, Tuple
from loguru import logger
import jieba
import shutil

# å°å…¥é…ç½®ç®¡ç†å™¨å’ŒåŸå§‹è™•ç†å™¨
import sys
from pathlib import Path
sys.path.append(str(Path(__file__).parent.parent))
from train_model.utils.config_manager import get_config_manager
from train_model.data_processing.csv_processor import HighwayCSVProcessor

class EnhancedHighwayCSVProcessor(HighwayCSVProcessor):
    """å¢å¼·çš„åœ‹é“CSVè³‡æ–™è™•ç†å™¨ - æ”¯æ´é§•é§›å‹å–„æè¿°"""
    
    def __init__(self, config_path: str = None):
        """åˆå§‹åŒ–å¢å¼·è™•ç†å™¨"""
        super().__init__(config_path)
        
        # è¼‰å…¥ç«™é»æ˜ å°„è³‡æ–™
        self.etag_data = self._load_etag_mapping()
        self.station_mapping = self._build_station_mapping()
        
        # è¼‰å…¥ä¼‘æ¯ç«™å’Œäº¤æµé“è³‡è¨Š
        self.rest_areas = self._load_rest_areas()
        self.interchange_info = self._load_interchange_info()
        
        self.json_analysis_data = self._load_json_analysis_data()

        logger.info("å¢å¼·CSVè™•ç†å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def _load_etag_mapping(self) -> pd.DataFrame:
        """è¼‰å…¥ Etag ç«™é»æ˜ å°„è³‡æ–™"""
        try:
            base_path = self.config_manager.resolve_path(self.data_config['input_data_path'])
            etag_file = os.path.join(base_path, '../Taiwan/Etag.csv')
            
            if os.path.exists(etag_file):
                etag_df = pd.read_csv(etag_file, encoding='utf-8')
                logger.info(f"è¼‰å…¥ Etag æ˜ å°„è³‡æ–™: {len(etag_df)} ç­†è¨˜éŒ„")
                return etag_df
            else:
                logger.warning(f"Etag æª”æ¡ˆä¸å­˜åœ¨: {etag_file}")
                return pd.DataFrame()
                
        except Exception as e:
            logger.error(f"è¼‰å…¥ Etag æ˜ å°„è³‡æ–™å¤±æ•—: {e}")
            return pd.DataFrame()
    
    def _build_station_mapping(self) -> Dict[str, Dict[str, Any]]:
        """å»ºç«‹ç«™é»ä»£è™Ÿåˆ°å‹å–„åç¨±çš„æ˜ å°„ï¼ˆä½¿ç”¨å°ˆæ¡ˆç¾æœ‰é‚è¼¯ï¼‰"""
        station_mapping = {}
        
        if self.etag_data.empty:
            return station_mapping
        
        for _, row in self.etag_data.iterrows():
            try:
                # æå–ç«™é»ç·¨è™Ÿï¼ˆå»é™¤ç‰ˆæœ¬è™Ÿï¼‰- ä½¿ç”¨èˆ‡ propagation_system.py ç›¸åŒé‚è¼¯
                station_code = row['ç·¨è™Ÿ']
                if pd.isna(station_code):
                    continue
                    
                # å°‡ç«™é»ç·¨è™Ÿè½‰æ›ç‚ºè³‡æ–™ä¸­çš„æ ¼å¼
                # ä¾‹å¦‚ï¼š01F-034.0N -> 01F0340N
                clean_code = station_code.replace('-', '').replace('.', '')
                
                # å»ºç«‹æ˜ å°„ï¼ˆä½¿ç”¨å°ˆæ¡ˆç¾æœ‰çµæ§‹ï¼‰
                station_mapping[clean_code] = {
                    'id': row['ID'],
                    'direction': row['æ–¹å‘'], 
                    'original_code': station_code,
                    'start_ic': row['äº¤æµé“(èµ·)'],
                    'end_ic': row['äº¤æµé“(è¿„)'],
                    'friendly_name': f"{row['äº¤æµé“(èµ·)']} è‡³ {row['äº¤æµé“(è¿„)']}",
                    'highway': station_code[:3] if len(station_code) >= 3 else "",
                    'mileage': self._extract_mileage(station_code),
                    'latitude': row['ç·¯åº¦(åŒ—ç·¯)'] if pd.notna(row['ç·¯åº¦(åŒ—ç·¯)']) else None,
                    'longitude': row['ç¶“åº¦(æ±ç¶“)'] if pd.notna(row['ç¶“åº¦(æ±ç¶“)']) else None
                }
                
            except Exception as e:
                logger.warning(f"è™•ç†ç«™é»æ˜ å°„å¤±æ•— {row.get('ç·¨è™Ÿ', 'Unknown')}: {e}")
                continue
        
        logger.info(f"å»ºç«‹ç«™é»æ˜ å°„: {len(station_mapping)} å€‹ç«™é»")
        return station_mapping
    
    def _extract_mileage(self, station_code: str) -> float:
        """å¾ç«™é»ç·¨è™Ÿä¸­æå–é‡Œç¨‹æ•¸"""
        try:
            # 01F-034.0N -> 34.0
            if '-' in station_code and '.' in station_code:
                parts = station_code.split('-')[1]  # 034.0N
                mileage_str = parts.replace('N', '').replace('S', '')  # 034.0
                return float(mileage_str)
            return 0.0
        except:
            return 0.0
    
    def _load_rest_areas(self) -> Dict[str, List[Dict[str, Any]]]:
        """è¼‰å…¥ä¼‘æ¯ç«™è³‡è¨Š"""
        # åœ‹é“ä¼‘æ¯ç«™è³‡è¨Šï¼ˆå¯ä»¥å¾å¤–éƒ¨æª”æ¡ˆè¼‰å…¥ï¼‰
        rest_areas = {
            '01F': [  # åœ‹é“ä¸€è™Ÿ
                {'name': 'ä¸­å£¢æœå‹™å€', 'mileage': 53.2, 'direction': 'both', 'facilities': ['åŠ æ²¹ç«™', 'é¤å»³', 'ä¾¿åˆ©åº—', 'åœè»Šå ´']},
                {'name': 'æ¹–å£æœå‹™å€', 'mileage': 62.5, 'direction': 'both', 'facilities': ['åŠ æ²¹ç«™', 'é¤å»³', 'ä¾¿åˆ©åº—', 'åœè»Šå ´']},
                {'name': 'è¥¿èºæœå‹™å€', 'mileage': 232.0, 'direction': 'both', 'facilities': ['åŠ æ²¹ç«™', 'é¤å»³', 'ä¾¿åˆ©åº—', 'åœè»Šå ´']},
                {'name': 'æ³°å®‰æœå‹™å€', 'mileage': 264.5, 'direction': 'both', 'facilities': ['åŠ æ²¹ç«™', 'é¤å»³', 'ä¾¿åˆ©åº—', 'åœè»Šå ´', 'ä¼‘æ¯å€']},
            ],
            '03F': [  # åœ‹é“ä¸‰è™Ÿ
                {'name': 'é—œè¥¿æœå‹™å€', 'mileage': 79.0, 'direction': 'both', 'facilities': ['åŠ æ²¹ç«™', 'é¤å»³', 'ä¾¿åˆ©åº—', 'åœè»Šå ´']},
                {'name': 'è¥¿æ¹–æœå‹™å€', 'mileage': 132.5, 'direction': 'both', 'facilities': ['åŠ æ²¹ç«™', 'é¤å»³', 'ä¾¿åˆ©åº—', 'åœè»Šå ´']},
                {'name': 'å—æŠ•æœå‹™å€', 'mileage': 214.0, 'direction': 'both', 'facilities': ['åŠ æ²¹ç«™', 'é¤å»³', 'ä¾¿åˆ©åº—', 'åœè»Šå ´']},
                {'name': 'å¤å‘æœå‹™å€', 'mileage': 254.5, 'direction': 'both', 'facilities': ['åŠ æ²¹ç«™', 'é¤å»³', 'ä¾¿åˆ©åº—', 'åœè»Šå ´']},
            ]
        }
        
        logger.info(f"è¼‰å…¥ä¼‘æ¯ç«™è³‡è¨Š: åœ‹é“1è™Ÿ {len(rest_areas['01F'])} å€‹ï¼Œåœ‹é“3è™Ÿ {len(rest_areas['03F'])} å€‹")
        return rest_areas
    
    def _load_interchange_info(self) -> Dict[str, Dict[str, Any]]:
        """è¼‰å…¥äº¤æµé“æ›¿ä»£è·¯ç·šè³‡è¨Š"""
        # ä¸»è¦äº¤æµé“çš„æ›¿ä»£è·¯ç·šè³‡è¨Š
        interchange_info = {
            # åŒ—éƒ¨åœ°å€
            'äº”è‚¡': {
                'alternatives': ['å°64ç·š', 'å°1ç·š', 'å°15ç·š'],
                'description': 'å¯ä½¿ç”¨å°64ç·šå¿«é€Ÿé“è·¯æˆ–å°1ç·šçœé“ä½œç‚ºæ›¿ä»£é“è·¯',
                'peak_hours': ['07:00-09:00', '17:00-19:00']
            },
            'æ—å£': {
                'alternatives': ['å°61ç·š', 'å°1ç·š'],
                'description': 'å¯ç¶“ç”±å°61ç·šè¥¿æ¿±å¿«é€Ÿé“è·¯ç¹è¡Œ',
                'peak_hours': ['07:30-09:30', '17:30-19:30']
            },
            'æ¡ƒåœ’': {
                'alternatives': ['å°4ç·š', 'å°1ç·š', 'ç¸£é“113ç·š'],
                'description': 'å¯ä½¿ç”¨å°4ç·šæˆ–ç¸£é“113ç·šé€²å…¥æ¡ƒåœ’å¸‚å€',
                'peak_hours': ['07:00-09:00', '17:00-19:00']
            },
            'ä¸­å£¢': {
                'alternatives': ['å°1ç·š', 'ç¸£é“114ç·š', 'å°66ç·š'],
                'description': 'å¯ä½¿ç”¨å°66ç·šæ±è¥¿å‘å¿«é€Ÿé“è·¯æˆ–å°1ç·š',
                'peak_hours': ['07:30-09:30', '17:30-19:30']
            },
            
            # ä¸­éƒ¨åœ°å€
            'å°ä¸­': {
                'alternatives': ['å°74ç·š', 'å°1ç·š', 'å°3ç·š'],
                'description': 'å¯ä½¿ç”¨å°74ç·šå¿«é€Ÿé“è·¯æˆ–å°1ç·šçœé“',
                'peak_hours': ['07:00-09:00', '17:00-19:00']
            },
            'å½°åŒ–': {
                'alternatives': ['å°1ç·š', 'å°19ç·š', 'ç¸£é“139ç·š'],
                'description': 'å¯ç¶“ç”±å°1ç·šæˆ–å°19ç·šç¹è¡Œå½°åŒ–å¸‚å€',
                'peak_hours': ['07:30-09:30', '17:30-19:30']
            },
            
            # å—éƒ¨åœ°å€
            'å°å—': {
                'alternatives': ['å°86ç·š', 'å°1ç·š', 'å°17ç·š'],
                'description': 'å¯ä½¿ç”¨å°86ç·šå¿«é€Ÿé“è·¯æˆ–å°17ç·šæ¿±æµ·å…¬è·¯',
                'peak_hours': ['07:00-09:00', '17:00-19:00']
            },
            'é«˜é›„': {
                'alternatives': ['å°88ç·š', 'å°1ç·š', 'å°17ç·š'],
                'description': 'å¯ä½¿ç”¨å°88ç·šå¿«é€Ÿé“è·¯é€²å…¥é«˜é›„',
                'peak_hours': ['07:30-09:30', '17:30-19:30']
            }
        }
        
        logger.info(f"è¼‰å…¥äº¤æµé“æ›¿ä»£è·¯ç·šè³‡è¨Š: {len(interchange_info)} å€‹äº¤æµé“")
        return interchange_info
    
    def resolve_station_code(self, station_code: str) -> str:
        """è§£æç«™é»ä»£è™Ÿç‚ºå‹å–„åç¨±ï¼ˆä½¿ç”¨å°ˆæ¡ˆç¾æœ‰é‚è¼¯ï¼‰"""
        
        # é¦–å…ˆå˜—è©¦å¾ CSV æ ¼å¼è§£æï¼šN0010_SB,034K+000
        if ',' in station_code and 'K+' in station_code:
            try:
                parts = station_code.split(',')
                direction_code = parts[0].strip()
                mileage_code = parts[1].strip()
                
                # è§£æåœ‹é“å’Œæ–¹å‘
                if 'N0010' in direction_code:
                    highway = 'åœ‹é“1è™Ÿ'
                    direction = 'åŒ—å‘' if 'NB' in direction_code else 'å—å‘'
                    highway_code = '01F'
                elif 'N0030' in direction_code:
                    highway = 'åœ‹é“3è™Ÿ' 
                    direction = 'åŒ—å‘' if 'NB' in direction_code else 'å—å‘'
                    highway_code = '03F'
                else:
                    return station_code  # ç„¡æ³•è­˜åˆ¥çš„æ ¼å¼
                
                # è§£æé‡Œç¨‹
                if 'K+' in mileage_code:
                    km_str = mileage_code.split('K+')[0]
                    km = float(km_str)
                    
                    # æ§‹å»ºå°æ‡‰çš„ Etag æ ¼å¼é€²è¡ŒæŸ¥æ‰¾
                    # ä¾‹å¦‚ï¼š034K+000 -> 01F-034.0N
                    etag_format = f"{highway_code}-{km_str:0>3}.0{'N' if 'åŒ—å‘' in direction else 'S'}"
                    clean_etag = etag_format.replace('-', '').replace('.', '')
                    
                    # å˜—è©¦å¾æ˜ å°„ä¸­æŸ¥æ‰¾
                    if clean_etag in self.station_mapping:
                        mapping = self.station_mapping[clean_etag]
                        return mapping['friendly_name']
                    
                    # å¦‚æœæ²’æ‰¾åˆ°æ˜ å°„ï¼Œè¿”å›åŸºæœ¬æè¿°
                    return f"{highway}{direction} {km}å…¬é‡Œè™•"
                    
            except Exception as e:
                logger.warning(f"è§£æCSVæ ¼å¼ä»£è™Ÿå¤±æ•— {station_code}: {e}")
        
        # å˜—è©¦ç›´æ¥å¾ Etag æ ¼å¼æ˜ å°„æŸ¥æ‰¾
        if station_code in self.station_mapping:
            return self.station_mapping[station_code]['friendly_name']
        
        # å˜—è©¦æ¸…ç†æ ¼å¼å¾ŒæŸ¥æ‰¾
        clean_code = station_code.replace('-', '').replace('.', '').replace('_', '')
        if clean_code in self.station_mapping:
            return self.station_mapping[clean_code]['friendly_name']
        
        return station_code  # å¦‚æœç„¡æ³•è§£æï¼Œè¿”å›åŸä»£è™Ÿ
    
    def find_nearby_rest_areas(self, highway: str, mileage: float, direction: str = 'both') -> List[Dict[str, Any]]:
        """å°‹æ‰¾é™„è¿‘çš„ä¼‘æ¯ç«™"""
        if highway not in self.rest_areas:
            return []
        
        nearby_areas = []
        for area in self.rest_areas[highway]:
            distance = abs(area['mileage'] - mileage)
            
            # åªè€ƒæ…®50å…¬é‡Œå…§çš„ä¼‘æ¯ç«™
            if distance <= 50:
                area_info = area.copy()
                area_info['distance_km'] = distance
                area_info['is_ahead'] = area['mileage'] > mileage
                nearby_areas.append(area_info)
        
        # æŒ‰è·é›¢æ’åº
        nearby_areas.sort(key=lambda x: x['distance_km'])
        return nearby_areas
    
    def get_alternative_routes(self, start_ic: str, end_ic: str) -> List[str]:
        """ç²å–æ›¿ä»£è·¯ç·šå»ºè­°"""
        alternatives = []
        
        # æª¢æŸ¥èµ·é»äº¤æµé“çš„æ›¿ä»£è·¯ç·š
        for ic_name, info in self.interchange_info.items():
            if ic_name in start_ic or ic_name in end_ic:
                alternatives.extend(info['alternatives'])
        
        # å»é‡ä¸¦è¿”å›
        return list(set(alternatives))
    
    def _load_json_analysis_data(self) -> List[Dict[str, Any]]:
        """è¼‰å…¥ JSON åˆ†æè³‡æ–™"""
        json_files = [
            'geometric_statistical_N01.json',
            'geometric_statistical_N03.json'
        ]
        
        all_json_data = []
        
        for json_file in json_files:
            try:
                base_path = self.config_manager.resolve_path(self.data_config['input_data_path'])
                json_file_path = os.path.join(base_path, '../Taiwan', json_file)
                
                if os.path.exists(json_file_path):
                    with open(json_file_path, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                    
                    # ç¢ºä¿æ˜¯åˆ—è¡¨æ ¼å¼
                    if isinstance(data, dict):
                        data = [data]
                    elif not isinstance(data, list):
                        logger.warning(f"JSON æª”æ¡ˆæ ¼å¼ä¸æ­£ç¢º: {json_file}")
                        continue
                    
                    # æ·»åŠ ä¾†æºæ¨™è¨˜
                    highway_type = "åœ‹é“1è™Ÿ" if "N01" in json_file else "åœ‹é“3è™Ÿ"
                    for item in data:
                        if isinstance(item, dict):
                            item['_source_file'] = json_file
                            item['_highway_type'] = highway_type
                    
                    all_json_data.extend(data)
                    logger.info(f"è¼‰å…¥ JSON åˆ†æè³‡æ–™: {json_file} - {len(data)} å€‹é …ç›®")
                    
                else:
                    logger.warning(f"JSON æª”æ¡ˆä¸å­˜åœ¨: {json_file_path}")
                    
            except Exception as e:
                logger.error(f"è¼‰å…¥ JSON æª”æ¡ˆå¤±æ•— {json_file}: {e}")
        
        logger.info(f"ç¸½å…±è¼‰å…¥ {len(all_json_data)} å€‹ JSON åˆ†æé …ç›®")
        return all_json_data

    def _convert_json_to_text_chunks(self, json_data: List[Dict[str, Any]]) -> List[str]:
        """å°‡ JSON åˆ†æè³‡æ–™è½‰æ›ç‚ºæ–‡æœ¬å¡Š"""
        text_chunks = []
        
        for item in json_data:
            try:
                # æ§‹å»ºæ–‡æœ¬å…§å®¹
                text_parts = []
                
                highway_type = item.get('_highway_type', 'åœ‹é“åˆ†æ')
                text_parts.append(f"=== {highway_type}äº¤é€šåˆ†æå ±å‘Š ===")
                
                # è™•ç†æ¨™æº– JSON çµæ§‹ (title + content)
                if 'title' in item and 'content' in item:
                    text_parts.append(f"\nğŸ“Š {item['title']}")
                    text_parts.append(f"\n{item['content']}")
                    
                    if 'category' in item:
                        text_parts.append(f"\né¡åˆ¥ï¼š{item['category']}")
                    
                    if 'tags' in item:
                        text_parts.append(f"\næ¨™ç±¤ï¼š{', '.join(item['tags'])}")
                
                # è™•ç†å…¶ä»–æ ¼å¼ï¼ˆç›´æ¥åºåˆ—åŒ–é‡è¦æ¬„ä½ï¼‰
                else:
                    # éæ¿¾æ‰å…§éƒ¨æ¬„ä½
                    filtered_item = {k: v for k, v in item.items() 
                                   if not k.startswith('_') and k not in ['id', 'timestamp']}
                    
                    if filtered_item:
                        text_parts.append(f"\nğŸ“‹ è³‡æ–™å…§å®¹ï¼š")
                        for key, value in filtered_item.items():
                            if isinstance(value, (dict, list)):
                                text_parts.append(f"â€¢ {key}: {json.dumps(value, ensure_ascii=False)}")
                            else:
                                text_parts.append(f"â€¢ {key}: {value}")
                
                # æ·»åŠ æœå°‹é—œéµè©ä»¥æé«˜æª¢ç´¢æ•ˆæœ
                full_text = ''.join(text_parts)
                
                # æ ¹æ“šå…§å®¹æ·»åŠ ç›¸é—œé—œéµè©
                keywords = []
                content_lower = full_text.lower()
                
                if 'äº”è‚¡' in content_lower and 'æ—å£' in content_lower:
                    keywords.append("äº”è‚¡-æ—å£æ®µäº¤é€šç“¶é ¸åˆ†æ")
                
                if 'å¤§å‹è»Š' in content_lower:
                    keywords.append("å¤§å‹è»Šæ¯”ä¾‹ è»Šæµå½±éŸ¿ æ™‚æ®µç®¡åˆ¶")
                
                if 'é€Ÿé™' in content_lower or 'é€Ÿåº¦' in content_lower:
                    keywords.append("å‹•æ…‹é€Ÿé™èª¿æ•´ è¡Œè»Šé€Ÿåº¦æ§åˆ¶")
                
                if any(word in content_lower for word in ['çŸ©é™£', 'å„ªå…ˆ', 'è©•ä¼°']):
                    keywords.append("å„ªå…ˆç´šçŸ©é™£ äº¤é€šç®¡ç†ç­–ç•¥")
                
                if 'çµ±è¨ˆ' in content_lower or 'rÂ²' in content_lower:
                    keywords.append("çµ±è¨ˆåˆ†æ è¿´æ­¸æ¨¡å‹ ç›¸é—œä¿‚æ•¸")
                
                if keywords:
                    full_text += f"\n\nğŸ” ç›¸é—œé—œéµè©ï¼š{' '.join(keywords)}"
                
                text_chunks.append(full_text)
                
            except Exception as e:
                logger.warning(f"è½‰æ› JSON é …ç›®å¤±æ•—: {e}")
                # å‚™ç”¨è½‰æ›æ–¹å¼
                backup_text = f"=== {item.get('_highway_type', 'åœ‹é“')}åˆ†æè³‡æ–™ ===\n"
                backup_text += f"åŸå§‹è³‡æ–™ï¼š{json.dumps(item, ensure_ascii=False, indent=2)}"
                text_chunks.append(backup_text)
        
        logger.info(f"JSON è½‰æ›å®Œæˆï¼š{len(text_chunks)} å€‹æ–‡æœ¬å¡Š")
        return text_chunks

    def generate_enhanced_text_descriptions(self, df: pd.DataFrame) -> List[str]:
        """ç”Ÿæˆå¢å¼·çš„æ–‡å­—æè¿° - åŒ…å«å‹å–„åç¨±å’Œé§•é§›å»ºè­°"""
        descriptions = []
        
        for idx, row in df.iterrows():
            try:
                # è§£æåŸºæœ¬è³‡è¨Š
                direction_code = str(row['åœ‹é“ç·¨è™Ÿæ–¹å‘'])
                mileage_str = str(row['æ¨è™Ÿ'])
                mileage_num = float(row['é‡Œç¨‹']) / 1000  # è½‰æ›ç‚ºå…¬é‡Œ
                
                # è§£æåœ‹é“å’Œæ–¹å‘
                if 'N0010' in direction_code:
                    highway = 'åœ‹é“1è™Ÿ'
                    highway_code = '01F'
                    direction = 'åŒ—å‘' if 'NB' in direction_code else 'å—å‘'
                elif 'N0030' in direction_code:
                    highway = 'åœ‹é“3è™Ÿ'
                    highway_code = '03F'
                    direction = 'åŒ—å‘' if 'NB' in direction_code else 'å—å‘'
                else:
                    highway = 'åœ‹é“'
                    highway_code = '01F'
                    direction = ''
                
                # åŸºæœ¬è·¯æ®µæè¿°
                desc = f"""=== {highway}{direction} {mileage_num:.1f}å…¬é‡Œè™•è·¯æ®µè³‡è¨Š ===

ğŸ“ ä½ç½®è³‡è¨Šï¼š
â€¢ åœ‹é“ï¼š{highway}
â€¢ æ–¹å‘ï¼š{direction}
â€¢ é‡Œç¨‹ï¼š{mileage_num:.1f}å…¬é‡Œ ({mileage_str})
â€¢ èª¿æŸ¥æ—¥æœŸï¼š{row['èª¿æŸ¥æ—¥æœŸ']}
â€¢ åº§æ¨™ï¼šåŒ—ç·¯ {row['ç¶“ç·¯åº¦åæ¨™Lat']:.6f}ï¼Œæ±ç¶“ {row['ç¶“ç·¯åº¦åæ¨™Lon']:.6f}

ğŸ›£ï¸ é“è·¯è¦æ ¼ï¼š
â€¢ é‹ªé¢é¡å‹ï¼š{row['é‹ªé¢ç¨®é¡']}
â€¢ è·¯å¹…å¯¬åº¦ï¼š{row['è·¯å¹…å¯¬']}å…¬å°ºï¼ˆå…¨è·¯å¹…ï¼š{row['å…¨è·¯å¹…å¯¬']}å…¬å°ºï¼‰
â€¢ ä¸»ç·šè»Šé“æ•¸ï¼š{int(row['è»Šé“æ•¸'])}è»Šé“"""

                # è»Šé“å¯¬åº¦è©³ç´°è³‡è¨Š
                lane_info = []
                for i in range(1, 7):
                    lane_width = row.get(f'è»Šé“{i}å¯¬', 0)
                    if pd.notna(lane_width) and float(lane_width) > 0:
                        lane_info.append(f"ç¬¬{i}è»Šé“ {lane_width}å…¬å°º")
                
                if lane_info:
                    desc += f"\nâ€¢ è»Šé“å¯¬åº¦ï¼š{' | '.join(lane_info)}"
                
                # è·¯è‚©è³‡è¨Š
                shoulder_info = []
                if row.get('å…§è·¯è‚©') == 'æœ‰' and pd.notna(row.get('å…§è·¯è‚©å¯¬')) and float(row.get('å…§è·¯è‚©å¯¬', 0)) > 0:
                    shoulder_info.append(f"å…§è·¯è‚© {row['å…§è·¯è‚©å¯¬']}å…¬å°º")
                if row.get('å¤–è·¯è‚©') == 'ç„¡' and pd.notna(row.get('å¤–è·¯è‚©å¯¬')) and float(row.get('å¤–è·¯è‚©å¯¬', 0)) > 0:
                    shoulder_info.append(f"å¤–è·¯è‚© {row['å¤–è·¯è‚©å¯¬']}å…¬å°º")
                
                if shoulder_info:
                    desc += f"\nâ€¢ è·¯è‚©è¨­æ–½ï¼š{' | '.join(shoulder_info)}"
                
                # è¼”åŠ©è»Šé“è³‡è¨Š
                aux_lanes = []
                for i in range(1, 4):
                    aux_lane = row.get(f'è¼”åŠ©è»Šé“{i}')
                    aux_width = row.get(f'è¼”åŠ©è»Šé“{i}å¯¬', 0)
                    if pd.notna(aux_lane) and aux_lane != 'ç„¡' and pd.notna(aux_width) and float(aux_width) > 0:
                        aux_lanes.append(f"è¼”åŠ©è»Šé“{i} {aux_width}å…¬å°º")
                
                if aux_lanes:
                    desc += f"\nâ€¢ è¼”åŠ©è»Šé“ï¼š{' | '.join(aux_lanes)}"
                
                # å¹¾ä½•è¨­è¨ˆç‰¹æ€§
                desc += f"\n\nğŸ”„ å¹¾ä½•è¨­è¨ˆï¼š"
                if pd.notna(row.get('æ›²ç‡åŠå¾‘')) and float(row.get('æ›²ç‡åŠå¾‘', 0)) > 0:
                    curvature = float(row['æ›²ç‡åŠå¾‘'])
                    if curvature < 500:
                        curve_desc = "æ€¥å½è·¯æ®µ"
                    elif curvature < 1000:
                        curve_desc = "å½é“è·¯æ®µ"
                    else:
                        curve_desc = "ç·©å½è·¯æ®µ"
                    desc += f"\nâ€¢ æ›²ç‡åŠå¾‘ï¼š{curvature}å…¬å°º ({curve_desc})"
                
                if pd.notna(row.get('ç¸±å‘å¡åº¦')):
                    slope = float(row['ç¸±å‘å¡åº¦'])
                    if abs(slope) > 0.03:
                        slope_desc = "é™¡å¡è·¯æ®µ" if abs(slope) > 0.05 else "ç·©å¡è·¯æ®µ"
                    else:
                        slope_desc = "å¹³å¦è·¯æ®µ"
                    desc += f"\nâ€¢ ç¸±å‘å¡åº¦ï¼š{slope:.3f} ({slope_desc})"
                
                if pd.notna(row.get('æ©«å‘å¡åº¦')):
                    desc += f"\nâ€¢ æ©«å‘å¡åº¦ï¼š{float(row['æ©«å‘å¡åº¦']):.3f}"
                
                # å°‹æ‰¾é™„è¿‘ä¼‘æ¯ç«™
                nearby_rest_areas = self.find_nearby_rest_areas(highway_code, mileage_num)
                if nearby_rest_areas:
                    desc += f"\n\nğŸ¨ é™„è¿‘ä¼‘æ¯ç«™ï¼š"
                    for area in nearby_rest_areas[:3]:  # æœ€å¤šé¡¯ç¤º3å€‹æœ€è¿‘çš„
                        direction_desc = "å‰æ–¹" if area['is_ahead'] else "å¾Œæ–¹"
                        desc += f"\nâ€¢ {area['name']}ï¼š{direction_desc}{area['distance_km']:.1f}å…¬é‡Œ"
                        desc += f" (è¨­æ–½ï¼š{', '.join(area['facilities'])})"
                
                # ç‰¹æ®Šè·¯æ®µè­¦ç¤º
                warnings = []
                if pd.notna(row.get('æ›²ç‡åŠå¾‘')) and float(row['æ›²ç‡åŠå¾‘']) < 500:
                    warnings.append("æ³¨æ„æ€¥å½ï¼Œå»ºè­°æ¸›é€Ÿæ…¢è¡Œ")
                if pd.notna(row.get('ç¸±å‘å¡åº¦')) and abs(float(row['ç¸±å‘å¡åº¦'])) > 0.05:
                    warnings.append("æ³¨æ„å¡åº¦è®ŠåŒ–ï¼Œä¿æŒå®‰å…¨è»Šè·")
                if row.get('é¿è»Šå½') != 'ç„¡':
                    warnings.append("è·¯æ®µè¨­æœ‰é¿è»Šå½ï¼Œæ³¨æ„å®‰å…¨")
                
                if warnings:
                    desc += f"\n\nâš ï¸ é§•é§›æé†’ï¼š\nâ€¢ " + "\nâ€¢ ".join(warnings)
                
                # é§•é§›å»ºè­°
                desc += f"\n\nğŸ’¡ é§•é§›å»ºè­°ï¼š"
                desc += f"\nâ€¢ å»ºè­°è»Šé€Ÿï¼šä¾è·¯æ³èª¿æ•´ï¼Œæ³¨æ„é€Ÿé™æ¨™ç¤º"
                desc += f"\nâ€¢ è»Šé“é¸æ“‡ï¼šå»ºè­°ä½¿ç”¨ä¸­é–“è»Šé“è¡Œé§›"
                if aux_lanes:
                    desc += f"\nâ€¢ åŒ¯å…¥åŒ¯å‡ºï¼šæ³¨æ„è¼”åŠ©è»Šé“è»Šè¼›å‹•æ…‹"
                
                descriptions.append(desc.strip())
                
            except Exception as e:
                logger.warning(f"ç”Ÿæˆè·¯æ®µ {idx} çš„å¢å¼·æè¿°å¤±æ•—: {e}")
                # ä½¿ç”¨åŸºæœ¬æè¿°ä½œç‚ºå‚™ç”¨
                basic_desc = f"è·¯æ®µè³‡è¨Šï¼š{row.get('åœ‹é“ç·¨è™Ÿæ–¹å‘', '')} {row.get('æ¨è™Ÿ', '')} - åŸºæœ¬é“è·¯è³‡æ–™"
                descriptions.append(basic_desc)
        
        logger.info(f"ç”Ÿæˆå¢å¼·æ–‡å­—æè¿°: {len(descriptions)} å€‹")
        return descriptions
    
    def process_all_data_enhanced(self) -> List[Dict[str, Any]]:
        """è™•ç†æ‰€æœ‰è³‡æ–™ä¸¦ç”Ÿæˆå¢å¼·çš„è¨“ç·´æ ¼å¼"""
        # è¼‰å…¥è³‡æ–™
        highway1_df, highway3_df = self.load_highway_data()
        
        # æ¸…ç†è³‡æ–™
        highway1_clean = self.clean_and_normalize_data(highway1_df)
        highway3_clean = self.clean_and_normalize_data(highway3_df)
        
        # ç”Ÿæˆå¢å¼·çš„æ–‡å­—æè¿°
        highway1_texts = self.generate_enhanced_text_descriptions(highway1_clean)
        highway3_texts = self.generate_enhanced_text_descriptions(highway3_clean)
        
        # è™•ç† JSON åˆ†æè³‡æ–™
        json_texts = self._convert_json_to_text_chunks(self.json_analysis_data)
        # åˆä½µæ‰€æœ‰æ–‡æœ¬
        all_texts = highway1_texts + highway3_texts + json_texts

        # åˆ†å‰²æ–‡æœ¬ä¸¦æ·»åŠ å…ƒæ•¸æ“š
        processed_data = []
        # è™•ç† CSV æ–‡æœ¬å¡Š
        csv_text_count = len(highway1_texts) + len(highway3_texts)
        
        for i, text in enumerate(all_texts):
            chunks = self.chunk_text(text)
            
            # ç¢ºå®šä¾†æºå’ŒåŸºæœ¬è³‡è¨Š
            if i < len(highway1_texts):
                source = 'highway1_csv'
                row = highway1_clean.iloc[i]
                data_type = 'csv'
            elif i < csv_text_count:
                source = 'highway3_csv'
                row = highway3_clean.iloc[i - len(highway1_texts)]
                data_type = 'csv'
            else:
                # JSON è³‡æ–™
                source = 'json_analysis'
                row = None
                data_type = 'json'
            
            # è™•ç†æ¯å€‹æ–‡æœ¬å¡Š
            for j, chunk in enumerate(chunks):
                doc_item = {
                    'id': f'{source}_{i}_{j}',
                    'text': chunk,
                    'source': source,
                    'data_type': data_type,
                    'chunk_index': j,
                    'original_index': i
                }
                
                # ç‚º CSV è³‡æ–™æ·»åŠ è©³ç´°å…ƒæ•¸æ“š
                if data_type == 'csv' and row is not None:
                    # è§£æä½ç½®è³‡è¨Š
                    direction_code = str(row['åœ‹é“ç·¨è™Ÿæ–¹å‘'])
                    mileage_num = float(row['é‡Œç¨‹']) / 1000
                    
                    doc_item.update({
                        'highway': 'åœ‹é“1è™Ÿ' if 'N0010' in direction_code else 'åœ‹é“3è™Ÿ',
                        'direction': 'åŒ—å‘' if 'NB' in direction_code else 'å—å‘',
                        'mileage': mileage_num,
                        'station_code': f"{row['åœ‹é“ç·¨è™Ÿæ–¹å‘']},{row['æ¨è™Ÿ']}",
                        'friendly_location': self.resolve_station_code(f"{row['åœ‹é“ç·¨è™Ÿæ–¹å‘']},{row['æ¨è™Ÿ']}"),
                        'coordinates': {
                            'lat': float(row['ç¶“ç·¯åº¦åæ¨™Lat']) if pd.notna(row['ç¶“ç·¯åº¦åæ¨™Lat']) else None,
                            'lng': float(row['ç¶“ç·¯åº¦åæ¨™Lon']) if pd.notna(row['ç¶“ç·¯åº¦åæ¨™Lon']) else None
                        }
                    })
                
                # ç‚º JSON è³‡æ–™æ·»åŠ å…ƒæ•¸æ“š
                elif data_type == 'json':
                    json_index = i - csv_text_count
                    if json_index < len(self.json_analysis_data):
                        json_item = self.json_analysis_data[json_index]
                        doc_item.update({
                            'highway': json_item.get('_highway_type', 'åœ‹é“åˆ†æ'),
                            'analysis_type': json_item.get('category', 'general'),
                            'source_file': json_item.get('_source_file', 'unknown')
                        })
                
                processed_data.append(doc_item)
        
        logger.info(f"å¢å¼·è™•ç†å®Œæˆï¼Œç¸½å…±ç”Ÿæˆ {len(processed_data)} å€‹æ–‡æœ¬å¡Š")
        
        # çµ±è¨ˆå¢å¼·è³‡æ–™
        highway_stats = {}
        for item in processed_data:
            highway = item.get('highway', 'unknown')
            highway_stats[highway] = highway_stats.get(highway, 0) + 1
        
        logger.info(f"å¢å¼·è³‡æ–™åˆ†å¸ƒ: {highway_stats}")
        
        return processed_data

    def chunk_text(self, text: str) -> List[str]:
        """ä¿®å¾©çš„æ–‡æœ¬åˆ†å¡Šæ–¹æ³•"""
        if not text or len(text.strip()) == 0:
            return []
        
        text = text.strip()
        chunk_size = self.data_config.get('chunk_size', 1500)
        chunk_overlap = self.data_config.get('chunk_overlap', 200)
        
        # å¦‚æœæ–‡æœ¬å°æ–¼å¡Šå¤§å°ï¼Œç›´æ¥è¿”å›
        if len(text) <= chunk_size:
            return [text]
        
        chunks = []
        start = 0
        
        while start < len(text):
            end = start + chunk_size
            
            if end >= len(text):
                # æœ€å¾Œä¸€å€‹å¡Š
                chunk = text[start:].strip()
                if chunk:
                    chunks.append(chunk)
                break
            
            # å°‹æ‰¾é©ç•¶çš„åˆ†å‰²é»
            chunk_text = text[start:end]
            
            # åœ¨åˆé©çš„åœ°æ–¹åˆ†å‰²
            best_split = end
            for sep in ['\n\n', '\n', 'ã€‚', 'ï¼›', 'ï¼Œ']:
                last_sep = chunk_text.rfind(sep)
                if last_sep > chunk_size * 0.7:
                    best_split = start + last_sep + len(sep)
                    break
            
            chunk = text[start:best_split].strip()
            if chunk:
                chunks.append(chunk)
            
            # ä¸‹ä¸€å€‹å¡Šçš„é–‹å§‹ä½ç½®ï¼ˆè€ƒæ…®é‡ç–Šï¼‰
            start = best_split - chunk_overlap
            if start < 0:
                start = best_split
        
        return chunks

def main():
    """æ¸¬è©¦å¢å¼·è™•ç†å™¨"""
    try:
        processor = EnhancedHighwayCSVProcessor()
        processed_data = processor.process_all_data_enhanced()
        
        # å„²å­˜å¢å¼·è³‡æ–™
        output_file = "enhanced_highway_data.json"
        output_path = processor.save_processed_data(processed_data, output_file)
        
        print(f"å¢å¼·è™•ç†å®Œæˆï¼ç”Ÿæˆäº† {len(processed_data)} å€‹æ–‡æœ¬å¡Š")
        print(f"è¼¸å‡ºæª”æ¡ˆï¼š{output_path}")
        
        # é¡¯ç¤ºç¯„ä¾‹
        print("\n=== ç¯„ä¾‹å¢å¼·æè¿° ===")
        for i in range(min(2, len(processed_data))):
            print(f"\n--- å¡Š {i+1} ---")
            print(f"ä½ç½®ï¼š{processed_data[i]['friendly_location']}")
            print(f"æ–‡æœ¬é è¦½ï¼š{processed_data[i]['text'][:300]}...")
            
        return processed_data
        
    except Exception as e:
        logger.error(f"å¢å¼·è™•ç†å¤±æ•—: {e}")
        return None

if __name__ == "__main__":
    main()