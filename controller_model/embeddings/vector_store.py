"""
å‘é‡å„²å­˜å’Œæª¢ç´¢æ¨¡çµ„
æ”¯æ´ ChromaDB å’Œ FAISS å‘é‡è³‡æ–™åº«
"""

import os
import json
import numpy as np
import gc
from typing import List, Dict, Any, Optional, Tuple
from loguru import logger
from sentence_transformers import SentenceTransformer
import chromadb
from chromadb.config import Settings
try:
    from chromadb.errors import InvalidCollectionException
except ImportError:
    # ChromaDB è¼ƒæ–°ç‰ˆæœ¬ä¸­çš„ç•°å¸¸é¡å
    try:
        from chromadb.api.types import CollectionNotExistError as InvalidCollectionException
    except ImportError:
        # å¦‚æœéƒ½æ‰¾ä¸åˆ°ï¼Œä½¿ç”¨é€šç”¨ç•°å¸¸
        InvalidCollectionException = ValueError

# å°å…¥é…ç½®ç®¡ç†å™¨
import sys
from pathlib import Path
sys.path.append(str(Path(__file__).parent.parent))
from utils.config_manager import get_config_manager

class VectorStore:
    """å‘é‡å„²å­˜å’Œæª¢ç´¢ç³»çµ±"""
    
    def __init__(self, config_path: str = None):
        """åˆå§‹åŒ–å‘é‡å„²å­˜ç³»çµ±"""
        # ä½¿ç”¨é…ç½®ç®¡ç†å™¨
        if config_path:
            os.environ['RAG_CONFIG_PATH'] = config_path
        
        self.config_manager = get_config_manager()
        self.config = self.config_manager.get_config()
        
        self.embedding_config = self.config['embeddings']
        self.vector_db_config = self.config['vector_db']
        self.retrieval_config = self.config['retrieval']
        
        # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
        self.embedding_model = SentenceTransformer(
            self.embedding_config['model_name'],
            device=self.embedding_config['device']
        )
        
        # åˆå§‹åŒ–å‘é‡è³‡æ–™åº«
        self.vector_db = None
        self.collection = None
        self._initialize_vector_db()
        
        logger.info("å‘é‡å„²å­˜ç³»çµ±åˆå§‹åŒ–å®Œæˆ")
    
    def _initialize_vector_db(self):
        """åˆå§‹åŒ–å‘é‡è³‡æ–™åº«"""
        if self.vector_db_config['type'] == 'chroma':
            self._initialize_chromadb()
        elif self.vector_db_config['type'] == 'faiss':
            self._initialize_faiss()
        else:
            raise ValueError(f"ä¸æ”¯æ´çš„å‘é‡è³‡æ–™åº«é¡å‹: {self.vector_db_config['type']}")
    
    def _initialize_chromadb(self):
        """åˆå§‹åŒ– ChromaDB"""
        persist_dir = self.vector_db_config['persist_directory']
        os.makedirs(persist_dir, exist_ok=True)
        
        # å‰µå»º ChromaDB å®¢æˆ¶ç«¯
        self.vector_db = chromadb.PersistentClient(
            path=persist_dir,
            settings=Settings(anonymized_telemetry=False)
        )
        
        # ç²å–æˆ–å‰µå»ºé›†åˆ
        collection_name = self.vector_db_config['collection_name']
        self.collection = None

        try:
            self.collection = self.vector_db.get_collection(collection_name)
            count = self.collection.count()
            logger.info(f"è¼‰å…¥ç¾æœ‰é›†åˆ: {collection_name}ï¼Œæ–‡æª”æ•¸é‡: {count}")
            
            # å¦‚æœè¼‰å…¥çš„é›†åˆæ˜¯ç©ºçš„ï¼Œå˜—è©¦è¼‰å…¥æ­£ç¢ºçš„é›†åˆ
            if count == 0:
                logger.warning(f"é›†åˆ {collection_name} ç‚ºç©ºï¼Œæª¢æŸ¥æ˜¯å¦æœ‰å…¶ä»–é›†åˆ")
                all_collections = self.vector_db.list_collections()
                for coll in all_collections:
                    coll_count = coll.count()
                    if coll_count > 0:
                        logger.info(f"æ‰¾åˆ°æœ‰è³‡æ–™çš„é›†åˆ: {coll.name}ï¼Œæ–‡æª”æ•¸é‡: {coll_count}")
                        self.collection = coll
                        break
                        
        except (InvalidCollectionException, ValueError, Exception) as e:
            logger.info(f"é›†åˆä¸å­˜åœ¨ï¼Œå‰µå»ºæ–°é›†åˆ: {collection_name}")
            self.collection = self.vector_db.create_collection(
                name=collection_name,
                metadata={"description": "Highway traffic data embeddings"}
            )


    def _find_populated_collection(self):
        """æœç´¢æœ‰è³‡æ–™çš„é›†åˆï¼ŒåŒ…æ‹¬æœç´¢å…¶ä»–å¯èƒ½çš„è·¯å¾‘"""
        logger.info("æœç´¢æœ‰è³‡æ–™çš„é›†åˆ...")
        
        # 1. é¦–å…ˆåœ¨ç•¶å‰è³‡æ–™åº«ä¸­å°‹æ‰¾
        try:
            all_collections = self.vector_db.list_collections()
            for coll in all_collections:
                count = coll.count()
                logger.info(f"æª¢æŸ¥é›†åˆ: {coll.name} -> {count} å€‹æ–‡æª”")
                if count > 0:
                    logger.info(f"âœ… æ‰¾åˆ°æœ‰è³‡æ–™çš„é›†åˆ: {coll.name}ï¼Œæ–‡æª”æ•¸é‡: {count}")
                    return coll
        except Exception as e:
            logger.error(f"æœç´¢ç•¶å‰è³‡æ–™åº«é›†åˆå¤±æ•—: {e}")
        
        # 2. æœç´¢å…¶ä»–å¯èƒ½çš„è³‡æ–™åº«è·¯å¾‘
        logger.info("æœç´¢å…¶ä»–å¯èƒ½çš„è³‡æ–™åº«è·¯å¾‘...")
        current_dir = Path.cwd()
        
        # å¯èƒ½çš„è·¯å¾‘
        search_paths = [
            current_dir / 'vector_db',
            current_dir / 'train_model' / 'vector_db',
            current_dir.parent / 'vector_db',
            current_dir / '.chroma',
            current_dir / 'chroma_db'
        ]
        
        # æ·»åŠ é…ç½®æ–‡ä»¶ä¸­çš„ç›¸å°è·¯å¾‘è®Šé«”
        config_dir = self.vector_db_config['persist_directory']
        if not os.path.isabs(config_dir):
            search_paths.extend([
                current_dir / config_dir,
                current_dir.parent / config_dir,
                current_dir / 'train_model' / config_dir
            ])
        
        best_collection = None
        best_count = 0
        best_client = None
        best_path = None
        
        for path in search_paths:
            if path.exists() and path != Path(self.vector_db_config['persist_directory']):
                try:
                    logger.info(f"æª¢æŸ¥è·¯å¾‘: {path}")
                    client = chromadb.PersistentClient(
                        path=str(path),
                        settings=Settings(anonymized_telemetry=False)
                    )
                    collections = client.list_collections()
                    
                    for coll in collections:
                        count = coll.count()
                        logger.info(f"  é›†åˆ: {coll.name} -> {count} å€‹æ–‡æª”")
                        
                        if count > best_count:
                            best_collection = coll
                            best_count = count
                            best_client = client
                            best_path = str(path)
                            logger.info(f"  ğŸ¯ ç™¼ç¾æ›´å¥½çš„é›†åˆ: {coll.name} ({count} å€‹æ–‡æª”)")
                            
                except Exception as e:
                    logger.debug(f"ç„¡æ³•è¨ªå•è·¯å¾‘ {path}: {e}")
                    continue
        
        # å¦‚æœæ‰¾åˆ°æ›´å¥½çš„é›†åˆï¼Œåˆ‡æ›åˆ°è©²è³‡æ–™åº«
        if best_collection and best_count > 0:
            logger.info(f"ğŸ”„ åˆ‡æ›åˆ°æ›´å¥½çš„è³‡æ–™åº«:")
            logger.info(f"   è·¯å¾‘: {best_path}")
            logger.info(f"   é›†åˆ: {best_collection.name}")
            logger.info(f"   æ–‡æª”æ•¸: {best_count}")
            
            # æ›´æ–°å…§éƒ¨é…ç½®å’Œå®¢æˆ¶ç«¯
            self.vector_db = best_client
            self.vector_db_config['persist_directory'] = best_path
            self.vector_db_config['collection_name'] = best_collection.name
            
            return best_collection
        
        logger.warning("æœªæ‰¾åˆ°ä»»ä½•æœ‰è³‡æ–™çš„é›†åˆ")
        return None

    def _initialize_faiss(self):
        """åˆå§‹åŒ– FAISS (æš«æ™‚ä½”ä½ï¼Œå¾ŒçºŒå¯æ“´å±•)"""
        # TODO: å¯¦ä½œ FAISS æ”¯æ´
        raise NotImplementedError("FAISS æ”¯æ´å°šæœªå¯¦ä½œ")
    
    def encode_texts(self, texts: List[str]) -> np.ndarray:
        """å°æ–‡æœ¬é€²è¡Œå‘é‡åŒ–ç·¨ç¢¼"""
        batch_size = self.embedding_config['batch_size']
        max_length = self.embedding_config['max_length']
        
        # åˆ†æ‰¹è™•ç†ä»¥é¿å…è¨˜æ†¶é«”å•é¡Œ
        all_embeddings = []
        try:
            for i in range(0, len(texts), batch_size):
                batch_texts = texts[i:i + batch_size]
                
                # ç·¨ç¢¼
                embeddings = self.embedding_model.encode(
                    batch_texts,
                    batch_size=batch_size,
                    max_length=max_length,
                    show_progress_bar=True,
                    convert_to_numpy=True,
                    normalize_embeddings=True,  # æ­£è¦åŒ–åµŒå…¥å‘é‡
                    truncate=True
                )
                all_embeddings.append(embeddings)
                
                # å®šæœŸæ¸…ç†è¨˜æ†¶é«”
                if (i // batch_size + 1) % 10 == 0:
                    gc.collect()
                    logger.debug(f"å·²è™•ç† {i // batch_size + 1} å€‹æ‰¹æ¬¡ï¼ŒåŸ·è¡Œè¨˜æ†¶é«”æ¸…ç†")
            
            # åˆä½µæ‰€æœ‰åµŒå…¥
            all_embeddings = np.vstack(all_embeddings)
            logger.info(f"å®Œæˆ {len(texts)} å€‹æ–‡æœ¬çš„å‘é‡åŒ–ç·¨ç¢¼")
            return all_embeddings
        
        except Exception as e:
            logger.error(f"å‘é‡åŒ–ç·¨ç¢¼éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")
            # æ¸…ç†è¨˜æ†¶é«”
            gc.collect()
            raise
    
    def add_documents(self, documents: List[Dict[str, Any]]):
        """æ·»åŠ æ–‡æª”åˆ°å‘é‡è³‡æ–™åº«"""
        if self.vector_db_config['type'] == 'chroma':
            self._add_documents_chromadb(documents)
        else:
            raise NotImplementedError(f"æœªå¯¦ä½œçš„å‘é‡è³‡æ–™åº«é¡å‹: {self.vector_db_config['type']}")
    
    def _add_documents_chromadb(self, documents: List[Dict[str, Any]]):
        """æ·»åŠ æ–‡æª”åˆ° ChromaDB"""
        texts = [doc['text'] for doc in documents]
        ids = [doc['id'] for doc in documents]
        
        # æº–å‚™å…ƒæ•¸æ“š
        metadatas = []
        for doc in documents:
            metadata = {
                'source': doc.get('source', ''),
                'chunk_index': doc.get('chunk_index', 0),
                'original_index': doc.get('original_index', 0)
            }
            metadatas.append(metadata)
        
        # ç”ŸæˆåµŒå…¥
        logger.info("æ­£åœ¨ç”Ÿæˆæ–‡æª”åµŒå…¥...")
        try:
            embeddings = self.encode_texts(texts)
            
            # æ·»åŠ åˆ°é›†åˆ
            logger.info("æ­£åœ¨æ·»åŠ æ–‡æª”åˆ°å‘é‡è³‡æ–™åº«...")
            self.collection.add(
                documents=texts,
                embeddings=embeddings.tolist(),
                metadatas=metadatas,
                ids=ids
            )
            
            logger.info(f"æˆåŠŸæ·»åŠ  {len(documents)} å€‹æ–‡æª”åˆ°å‘é‡è³‡æ–™åº«")
            
        except Exception as e:
            logger.error(f"æ·»åŠ æ–‡æª”åˆ°å‘é‡è³‡æ–™åº«æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            raise
        finally:
            # æ¸…ç†è¨˜æ†¶é«”
            if 'embeddings' in locals():
                del embeddings
            gc.collect()
    
    def search(self, query: str, top_k: Optional[int] = None) -> List[Dict[str, Any]]:
        """æœç´¢ç›¸é—œæ–‡æª”"""
        if top_k is None:
            top_k = self.retrieval_config['top_k']
        
        if self.vector_db_config['type'] == 'chroma':
            return self._search_chromadb(query, top_k)
        else:
            raise NotImplementedError(f"æœªå¯¦ä½œçš„å‘é‡è³‡æ–™åº«é¡å‹: {self.vector_db_config['type']}")
    
    def _search_chromadb(self, query: str, top_k: int) -> List[Dict[str, Any]]:
        """åœ¨ ChromaDB ä¸­æœç´¢"""
        try:
            # ç”ŸæˆæŸ¥è©¢åµŒå…¥
            query_embedding = self.embedding_model.encode(
                [query], 
                normalize_embeddings=True
            )
            
            # åŸ·è¡Œæœç´¢
            results = self.collection.query(
                query_embeddings=query_embedding.tolist(),
                n_results=min(top_k, 100),  # é™åˆ¶æœ€å¤§çµæœæ•¸
                include=["documents", "metadatas", "distances"]
            )
            
            # æ ¼å¼åŒ–çµæœ
            formatted_results = []
            if results.get('documents') and len(results['documents']) > 0:
                for i in range(len(results['documents'][0])):
                    distance = results['distances'][0][i]
                    # å°‡è·é›¢è½‰æ›ç‚ºç›¸ä¼¼åº¦ (cosineè·é›¢: score = 1 - distance)
                    score = 1 / (1 + distance)
                    
                    result = {
                        'id': results['ids'][0][i],
                        'text': results['documents'][0][i],
                        'score': score,
                        'metadata': results.get('metadatas', [{}])[0][i] if results.get('metadatas') else {}
                    }
                    
                    # éæ¿¾ä½æ–¼é–¾å€¼çš„çµæœ
                    if score >= self.retrieval_config['score_threshold']:
                        formatted_results.append(result)
            
            logger.info(f"æœç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(formatted_results)} å€‹ç›¸é—œæ–‡æª”")
            return formatted_results
            
        except Exception as e:
            logger.error(f"æœç´¢éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")
            return []
        finally:
            # æ¸…ç†è¨˜æ†¶é«”
            if 'query_embedding' in locals():
                del query_embedding
            gc.collect()
    
    def get_collection_stats(self) -> Dict[str, Any]:
        """ç²å–é›†åˆçµ±è¨ˆè³‡è¨Š"""
        try:
            if self.vector_db_config['type'] == 'chroma':
                count = self.collection.count()
                return {
                    'document_count': count,
                    'collection_name': self.vector_db_config['collection_name'],
                    'embedding_dimension': self.embedding_model.get_sentence_embedding_dimension(),
                    'vector_db_type': 'chroma',
                    'persist_directory': self.vector_db_config['persist_directory']
                }
            else:
                return {'vector_db_type': self.vector_db_config['type']}
        except Exception as e:
            logger.error(f"ç²å–çµ±è¨ˆè³‡è¨Šæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return {'error': str(e)}
    
    def delete_collection(self):
        """åˆªé™¤æ•´å€‹é›†åˆ (è¬¹æ…ä½¿ç”¨)"""
        try:
            if self.vector_db_config['type'] == 'chroma':
                collection_name = self.vector_db_config['collection_name']
                self.vector_db.delete_collection(collection_name)
                logger.warning(f"å·²åˆªé™¤é›†åˆ: {collection_name}")
                # é‡ç½®é›†åˆå¼•ç”¨
                self.collection = None
        except Exception as e:
            logger.error(f"åˆªé™¤é›†åˆæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            raise

class RAGRetriever:
    """RAG æª¢ç´¢å™¨"""
    
    def __init__(self, vector_store: VectorStore):
        """åˆå§‹åŒ–æª¢ç´¢å™¨"""
        self.vector_store = vector_store
        self.config = vector_store.config
        
    def retrieve_context(self, query: str, max_context_length: int = 2000) -> str:
        """æª¢ç´¢ç›¸é—œä¸Šä¸‹æ–‡"""
        # æœç´¢ç›¸é—œæ–‡æª”
        results = self.vector_store.search(query)
        
        # çµ„åˆä¸Šä¸‹æ–‡
        context_parts = []
        current_length = 0
        
        for result in results:
            text = result['text']
            if current_length + len(text) <= max_context_length:
                context_parts.append(text)
                current_length += len(text)
            else:
                # å¦‚æœè¶…å‡ºé•·åº¦é™åˆ¶ï¼Œæˆªæ–·æœ€å¾Œä¸€å€‹æ–‡æœ¬
                remaining_length = max_context_length - current_length
                if remaining_length > 100:  # è‡³å°‘ä¿ç•™100å€‹å­—ç¬¦
                    context_parts.append(text[:remaining_length])
                break
        
        context = "\n\n".join(context_parts)
        logger.info(f"æª¢ç´¢åˆ° {len(context_parts)} å€‹æ–‡æª”ç‰‡æ®µï¼Œç¸½é•·åº¦: {len(context)} å­—ç¬¦")
        return context

if __name__ == "__main__":
    # æ¸¬è©¦å‘é‡å„²å­˜ç³»çµ±
    vector_store = VectorStore()
    print("å‘é‡å„²å­˜ç³»çµ±æ¸¬è©¦:")
    print(f"é›†åˆçµ±è¨ˆ: {vector_store.get_collection_stats()}")
    
    # æ¸¬è©¦æœç´¢
    query = "åœ‹é“ä¸€è™Ÿè»Šé“å¯¬åº¦"
    results = vector_store.search(query, top_k=3)
    print(f"\næœç´¢æŸ¥è©¢: {query}")
    for i, result in enumerate(results, 1):
        print(f"{i}. ç›¸ä¼¼åº¦: {result['score']:.3f}")
        print(f"   æ–‡æœ¬: {result['text'][:100]}...")
        print(f"   ä¾†æº: {result['metadata']}")