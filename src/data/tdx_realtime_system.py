#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import time
import requests
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import json
import logging
import signal
import sys
import glob
from pathlib import Path
from math import exp
from dotenv import load_dotenv

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
load_dotenv()

class TDXRealtimeSystem:
    """
    TDX ETag å³æ™‚äº¤é€šç›£æ§ç³»çµ± (ä¿®å¾©ä¸­æ–·å•é¡Œç‰ˆæœ¬)
    
    ç‰¹é»ï¼š
    1. ä½¿ç”¨ TDX API å–å¾—å³æ™‚ ETag è³‡æ–™
    2. OAuth2 èªè­‰æ©Ÿåˆ¶
    3. è‡ªå‹•è³‡æ–™è™•ç†å’Œä¿å­˜
    4. å®Œæ•´æ—¥èªŒè¨˜éŒ„
    5. éŒ¯èª¤æ¢å¾©æ©Ÿåˆ¶
    6. èˆ‡ç¾æœ‰ç³»çµ±ç›¸å®¹çš„è³‡æ–™æ ¼å¼
    7. ä¿®å¾© Ctrl+C ä¸­æ–·å•é¡Œ
    """

    def __init__(self, base_dir="../data"):
        """åˆå§‹åŒ– TDX ç³»çµ±"""
        
        # åŸºæœ¬è¨­å®š
        self.base_dir = base_dir
        self.realtime_dir = os.path.join(self.base_dir, "realtime_data")
        self.log_dir = os.path.join(self.base_dir, "logs")
        
        # å»ºç«‹ç›®éŒ„
        os.makedirs(self.realtime_dir, exist_ok=True)
        os.makedirs(self.log_dir, exist_ok=True)
        
        # è¨­å®šæ—¥èªŒ
        self._setup_logging()
        
        # TDX API è¨­å®š - å¾ç’°å¢ƒè®Šæ•¸è®€å–
        self.client_id = os.getenv('TDX_CLIENT_ID')
        self.client_secret = os.getenv('TDX_CLIENT_SECRET')
        
        if not self.client_id or not self.client_secret:
            raise ValueError("è«‹åœ¨ .env æª”æ¡ˆä¸­è¨­å®š TDX_CLIENT_ID å’Œ TDX_CLIENT_SECRET")
        self.base_url = "https://tdx.transportdata.tw/api/basic"
        self.auth_url = "https://tdx.transportdata.tw/auth/realms/TDXConnect/protocol/openid-connect/token"
        
        # èªè­‰ç›¸é—œ
        self.access_token = None
        self.token_expires_at = None
        
        # ç›£æ§é…ç½® - ä¿®æ”¹ç‚ºæ›´åˆç†çš„é–“éš”
        self.collection_interval = 1    # 1åˆ†é˜é–“éš” (æ›´å³æ™‚)
        self.cleanup_frequency = 12     # æ¯12æ¬¡æ”¶é›†å¾Œæ¸…ç†ä¸€æ¬¡
        self.max_file_age_hours = 24    # ä¿ç•™24å°æ™‚çš„æª”æ¡ˆ
        
        # ç³»çµ±ç‹€æ…‹
        self.is_running = False
        self.collection_count = 0
        self.last_successful_collection = None
        self.consecutive_failures = 0
        self.max_consecutive_failures = 5
        
        # ä¸­æ–·è™•ç†æ¨™èªŒ
        self.interrupt_requested = False
        
        # è»Šç¨®å°æ‡‰è¡¨ (TDX ä½¿ç”¨æ•¸å­—ä»£ç¢¼)
        self.vehicle_types = {
            1: 'å°å®¢è»Š',
            2: 'å°è²¨è»Š', 
            3: 'å¤§å®¢è»Š',
            4: 'å¤§è²¨è»Š',
            5: 'è¯çµè»Š'
        }
        
        # ç›®æ¨™è·¯æ®µç¯©é¸
        self.target_highways = ['1', '2', '3', '4', '5', '6']
        self.target_directions = ['0', '1']
        
        # è¼‰å…¥ç›®æ¨™ç«™é»æ¸…å–®
        self.target_stations = self._load_target_stations()
        
        # è¨»å†Šä¿¡è™Ÿè™•ç†å™¨ - ä¿®æ”¹è™•ç†æ–¹å¼
        signal.signal(signal.SIGINT, self._signal_handler)
        signal.signal(signal.SIGTERM, self._signal_handler)
        
        self.logger.info("ğŸš€ TDX å³æ™‚ç›£æ§ç³»çµ±åˆå§‹åŒ–å®Œæˆ")
        self.logger.info(f"ğŸ”‘ Client ID: {self.client_id}")
        self.logger.info(f"ğŸ’¾ è³‡æ–™ç›®éŒ„: {self.realtime_dir}")
        self.logger.info(f"â±ï¸ æ”¶é›†é–“éš”: {self.collection_interval} åˆ†é˜")

    def _setup_logging(self):
        """è¨­å®šæ—¥èªŒç³»çµ±"""
        log_file = os.path.join(self.log_dir, f"tdx_system_{datetime.now().strftime('%Y%m%d')}.log")
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file, encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)

    def _signal_handler(self, signum, frame):
        """ä¿¡è™Ÿè™•ç†å™¨ - ç«‹å³éŸ¿æ‡‰ä¸­æ–·"""
        self.logger.info(f"\nğŸ›‘ æ”¶åˆ°ä¸­æ–·ä¿¡è™Ÿ {signum}ï¼Œæ­£åœ¨åœæ­¢ç³»çµ±...")
        self.interrupt_requested = True
        self.is_running = False
        
        # å¼·åˆ¶é€€å‡ºé¸é …
        if hasattr(self, '_interrupt_count'):
            self._interrupt_count += 1
        else:
            self._interrupt_count = 1
            
        if self._interrupt_count >= 2:
            self.logger.info("ğŸš¨ æ”¶åˆ°å¤šæ¬¡ä¸­æ–·ä¿¡è™Ÿï¼Œå¼·åˆ¶é€€å‡º")
            sys.exit(1)
        else:
            self.logger.info("ğŸ’¡ å†æŒ‰ä¸€æ¬¡ Ctrl+C å¼·åˆ¶é€€å‡º")

    def _load_target_stations(self):
        """è¼‰å…¥ç›®æ¨™ç«™é»æ¸…å–®"""
        target_stations = set()
        etag_file = os.path.join(os.path.dirname(self.base_dir), 'data', 'Taiwan', 'Etag.csv')
        
        try:
            import pandas as pd
            df = pd.read_csv(etag_file, encoding='utf-8')
            
            for station_code in df['ç·¨è™Ÿ'].values:
                if pd.notna(station_code):
                    converted = station_code.replace('-', '').replace('.', '')
                    target_stations.add(converted)
            
            self.logger.info(f"âœ… è¼‰å…¥ {len(target_stations)} å€‹ç›®æ¨™ç«™é»")
            return target_stations
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ç„¡æ³•è¼‰å…¥ç›®æ¨™ç«™é»æ¸…å–®: {e}")
            self.logger.info("ğŸ”„ å°‡ä½¿ç”¨æ‰€æœ‰å¯ç”¨ç«™é»")
            return set()

    def get_access_token(self):
        """å–å¾— OAuth2 å­˜å–æ¬Šæ–"""
        if self.access_token and self.token_expires_at and datetime.now() < self.token_expires_at:
            return self.access_token
        
        self.logger.info("ğŸ”‘ å–å¾—æ–°çš„å­˜å–æ¬Šæ–...")
        
        auth_data = {
            'grant_type': 'client_credentials',
            'client_id': self.client_id,
            'client_secret': self.client_secret
        }
        
        try:
            response = requests.post(
                self.auth_url,
                data=auth_data,
                headers={'Content-Type': 'application/x-www-form-urlencoded'},
                timeout=10
            )
            response.raise_for_status()
            
            token_data = response.json()
            self.access_token = token_data['access_token']
            expires_in = token_data.get('expires_in', 3600)
            self.token_expires_at = datetime.now() + timedelta(seconds=expires_in - 60)
            
            self.logger.info("âœ… å­˜å–æ¬Šæ–å–å¾—æˆåŠŸ")
            return self.access_token
            
        except Exception as e:
            self.logger.error(f"âŒ å–å¾—å­˜å–æ¬Šæ–å¤±æ•—: {e}")
            raise

    def make_api_request(self, endpoint, params=None, retries=3):
        """ç™¼é€ API è«‹æ±‚"""
        for attempt in range(retries):
            try:
                token = self.get_access_token()
                
                headers = {
                    'Authorization': f'Bearer {token}',
                    'Content-Type': 'application/json',
                    'User-Agent': 'HighwayTrafficSystem/1.0'
                }
                
                url = f"{self.base_url}{endpoint}"
                
                response = requests.get(url, headers=headers, params=params, timeout=30)
                response.raise_for_status()
                
                return response.json()
                
            except requests.exceptions.Timeout:
                self.logger.warning(f"API è«‹æ±‚è¶…æ™‚ (å˜—è©¦ {attempt + 1}/{retries}): {endpoint}")
                if attempt < retries - 1:
                    time.sleep(2 ** attempt)
                    continue
                raise
                
            except requests.exceptions.HTTPError as e:
                if e.response.status_code == 401:
                    self.logger.info("Token å¯èƒ½éæœŸï¼Œé‡æ–°å–å¾—...")
                    self.access_token = None
                    if attempt < retries - 1:
                        continue
                self.logger.error(f"HTTP éŒ¯èª¤: {e.response.status_code} - {endpoint}")
                raise
                
            except Exception as e:
                self.logger.error(f"API è«‹æ±‚å¤±æ•— (å˜—è©¦ {attempt + 1}/{retries}): {endpoint} - {e}")
                if attempt < retries - 1:
                    time.sleep(2 ** attempt)
                    continue
                raise

    def get_live_etag_data(self):
        """å–å¾—å³æ™‚ ETag è³‡æ–™"""
        self.logger.info("ğŸ“¡ å–å¾—å³æ™‚ ETag è·¯æ³è³‡æ–™...")
        
        try:
            params = {
                '$format': 'JSON'
            }
            
            data = self.make_api_request("/v2/Road/Traffic/Live/ETag/Freeway", params)
            
            if isinstance(data, dict) and 'ETagPairLives' in data:
                live_data = data['ETagPairLives']
                self.logger.info(f"ğŸ“‹ å¾ ETagPairLives å–å¾— {len(live_data)} ç­†åŸå§‹è³‡æ–™")
            elif isinstance(data, list):
                live_data = data
                self.logger.info(f"ğŸ“‹ ç›´æ¥å–å¾— {len(live_data)} ç­†åŸå§‹è³‡æ–™")
            else:
                self.logger.warning("âŒ ç„¡æ³•è§£æè³‡æ–™çµæ§‹")
                return []
            
            current_time = datetime.now()
            valid_data = []
            highway_stats = {}
            
            for record in live_data:
                if not isinstance(record, dict):
                    continue
                
                pair_id = record.get('ETagPairID', '')
                highway_id = ''
                
                if pair_id.startswith('01F'):
                    highway_id = '1'
                elif pair_id.startswith('03F'):
                    highway_id = '3'
                elif pair_id.startswith('02F'):
                    highway_id = '2'
                elif pair_id.startswith('04F'):
                    highway_id = '4'
                elif pair_id.startswith('05F'):
                    highway_id = '5'
                elif pair_id.startswith('06F'):
                    highway_id = '6'
                
                highway_stats[highway_id] = highway_stats.get(highway_id, 0) + 1
                
                data_time_str = record.get('DataCollectTime', '')
                data_is_fresh = True
                
                if data_time_str:
                    try:
                        if '+' in data_time_str or 'Z' in data_time_str:
                            data_time = datetime.fromisoformat(data_time_str.replace('Z', '+00:00'))
                            data_time = data_time.replace(tzinfo=None)
                        else:
                            data_time = datetime.fromisoformat(data_time_str)
                        
                        time_diff = (current_time - data_time).total_seconds() / 60
                        
                        if time_diff > 60:
                            data_is_fresh = False
                            continue
                    except Exception as e:
                        pass
                
                record['ParsedHighwayID'] = highway_id
                valid_data.append(record)
            
            self.logger.info(f"ğŸ“Š åœ‹é“è³‡æ–™åˆ†å¸ƒ: {highway_stats}")
            self.logger.info(f"âœ… ç¯©é¸å‡º {len(valid_data)} ç­†æœ‰æ•ˆå³æ™‚è³‡æ–™")
            return valid_data
            
        except Exception as e:
            self.logger.error(f"âŒ å–å¾—å³æ™‚è³‡æ–™å¤±æ•—: {e}")
            return []

    def process_live_data(self, live_data):
        """è™•ç†å³æ™‚è³‡æ–™"""
        if not live_data:
            return pd.DataFrame()
        
        self.logger.info(f"ğŸ”„ è™•ç† {len(live_data)} ç­†å³æ™‚è³‡æ–™...")
        
        processed_records = []
        current_time = datetime.now()
        
        for record in live_data:
            try:
                pair_id = record.get('ETagPairID', '')
                if not pair_id:
                    continue
                
                highway_id = record.get('ParsedHighwayID', '')
                direction = 'N' if pair_id.endswith('N') else 'S' if pair_id.endswith('S') else ''
                
                if highway_id not in self.target_highways:
                    continue
                
                flows = record.get('Flows', [])
                if not flows:
                    travel_time = record.get('TravelTime', 0)
                    speed = record.get('SpaceMeanSpeed', 0)
                    volume = record.get('Volume', 0) or record.get('VehicleCount', 0)
                    
                    if volume > 0:
                        flows = [{
                            'VehicleType': 1,
                            'TravelTime': travel_time,
                            'SpaceMeanSpeed': speed,
                            'VehicleCount': volume
                        }]
                
                for flow in flows:
                    vehicle_type = flow.get('VehicleType', 1)
                    travel_time = flow.get('TravelTime', 0)
                    speed = flow.get('SpaceMeanSpeed', 0) or flow.get('Speed', 0)
                    volume = flow.get('VehicleCount', 0) or flow.get('Volume', 0)
                    
                    if volume <= 0:
                        continue
                    
                    equivalent = self._calculate_vehicle_equivalent(vehicle_type, speed)
                    station_id = self._generate_station_id(pair_id, highway_id, direction)
                    
                    processed_record = {
                        'station': station_id,
                        'date': current_time.strftime('%Y/%m/%d'),
                        'hour': current_time.hour,
                        'minute': (current_time.minute // 5) * 5,
                        'flow': volume * equivalent,
                        'median_speed': speed,
                        'avg_travel_time': travel_time,
                        'pair_id': pair_id,
                        'highway_id': highway_id,
                        'direction': direction,
                        'vehicle_type': vehicle_type,
                        'raw_volume': volume,
                        'data_collect_time': record.get('DataCollectTime', current_time.isoformat())
                    }
                    
                    processed_records.append(processed_record)
                    
            except Exception as e:
                self.logger.warning(f"è™•ç†å–®ç­†è³‡æ–™æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                continue
        
        if processed_records:
            df = pd.DataFrame(processed_records)
            
            aggregated_df = df.groupby(['station', 'date', 'hour', 'minute']).agg({
                'flow': 'sum',
                'median_speed': lambda x: np.average(x, weights=df.loc[x.index, 'raw_volume']) if len(x) > 0 else 0,
                'avg_travel_time': lambda x: np.average(x, weights=df.loc[x.index, 'raw_volume']) if len(x) > 0 else 0,
                'pair_id': 'first',
                'highway_id': 'first', 
                'direction': 'first'
            }).reset_index()
            
            self.logger.info(f"âœ… è™•ç†å®Œæˆ: {len(aggregated_df)} å€‹ç«™é»çš„èšåˆè³‡æ–™")
            return aggregated_df
        
        return pd.DataFrame()

    def _generate_station_id(self, pair_id, highway_id, direction):
        """ç”Ÿæˆèˆ‡åŸç³»çµ±ç›¸å®¹çš„ç«™é»ID"""
        import re
        
        if '-' in pair_id:
            end_station = pair_id.split('-')[1]
            return end_station
        else:
            direction_suffix = 'S' if direction == '0' else 'N'
            highway_prefix = f"{highway_id.zfill(2)}F"
            
            numbers = re.findall(r'\d+', pair_id)
            if numbers:
                number_part = numbers[0].zfill(4)
            else:
                number_part = str(abs(hash(pair_id)) % 9999).zfill(4)
            
            return f"{highway_prefix}{number_part}{direction_suffix}"

    def _calculate_vehicle_equivalent(self, vehicle_type, speed):
        """è¨ˆç®—è»Šç¨®ç•¶é‡"""
        if vehicle_type in [1, 2]:
            return 1.0
        elif vehicle_type == 3:
            if speed < 70:
                return 1.13 + 1.66 * exp(-speed / 34.93)
            elif 70 <= speed <= 87:
                return 2.79 - 0.0206 * speed
            else:
                return 1.0
        elif vehicle_type == 4:
            if speed <= 105:
                return 1.9 - 0.00857 * speed
            else:
                return 1.0
        elif vehicle_type == 5:
            if speed <= 108:
                return 2.7 - 0.0157 * speed
            else:
                return 1.0
        else:
            return 1.0

    def save_data(self, processed_data):
        """ä¿å­˜è™•ç†å¾Œçš„è³‡æ–™"""
        if processed_data.empty:
            return None
        
        current_time = datetime.now()
        
        date_str = current_time.strftime('%Y%m%d')
        time_str = current_time.strftime('%H%M')
        
        output_file = os.path.join(self.realtime_dir, f"realtime_shock_data_{date_str}_{time_str}.csv")
        
        output_columns = ['station', 'date', 'hour', 'minute', 'flow', 'median_speed', 'avg_travel_time']
        compatible_data = processed_data[output_columns].copy()
        
        if self.target_stations:
            before_filter = len(compatible_data)
            compatible_data = compatible_data[compatible_data['station'].isin(self.target_stations)]
            after_filter = len(compatible_data)
            self.logger.info(f"ğŸ¯ ç«™é»éæ¿¾: {before_filter} â†’ {after_filter} ç­†è¨˜éŒ„")
        
        compatible_data.to_csv(output_file, index=False, encoding='utf-8')
        
        self.logger.info(f"ğŸ’¾ è³‡æ–™å·²ä¿å­˜: {output_file}")
        self.logger.info(f"ğŸ“Š è¨˜éŒ„æ•¸: {len(compatible_data)}, ç«™é»æ•¸: {compatible_data['station'].nunique()}")
        
        return output_file

    def cleanup_old_files(self):
        """æ¸…ç†èˆŠæª”æ¡ˆ"""
        self.logger.info("ğŸ§¹ åŸ·è¡Œæª”æ¡ˆæ¸…ç†...")
        
        cutoff_time = datetime.now() - timedelta(hours=self.max_file_age_hours)
        csv_pattern = os.path.join(self.realtime_dir, "realtime_shock_data_*.csv")
        
        deleted_count = 0
        for file_path in glob.glob(csv_pattern):
            try:
                file_time = datetime.fromtimestamp(os.path.getmtime(file_path))
                if file_time < cutoff_time:
                    os.remove(file_path)
                    deleted_count += 1
            except:
                pass
        
        if deleted_count > 0:
            self.logger.info(f"âœ… æ¸…ç†å®Œæˆ: åˆªé™¤ {deleted_count} å€‹æª”æ¡ˆ")

    def single_collection(self):
        """åŸ·è¡Œå–®æ¬¡è³‡æ–™æ”¶é›†"""
        try:
            start_time = datetime.now()
            self.logger.info(f"ğŸ“Š é–‹å§‹ TDX å³æ™‚è³‡æ–™æ”¶é›† - {start_time.strftime('%H:%M:%S')}")
            
            live_data = self.get_live_etag_data()
            processed_data = self.process_live_data(live_data)
            output_file = self.save_data(processed_data)
            
            self.last_successful_collection = datetime.now()
            self.consecutive_failures = 0
            
            duration = (datetime.now() - start_time).total_seconds()
            self.logger.info(f"âœ… æ”¶é›†å®Œæˆï¼Œè€—æ™‚ {duration:.1f} ç§’")
            
            return processed_data, output_file
            
        except Exception as e:
            self.logger.error(f"âŒ è³‡æ–™æ”¶é›†å¤±æ•—: {e}")
            self.consecutive_failures += 1
            return pd.DataFrame(), None

    def interruptible_sleep(self, seconds):
        """å¯ä¸­æ–·çš„ä¼‘çœ å‡½æ•¸"""
        end_time = time.time() + seconds
        
        while time.time() < end_time and not self.interrupt_requested:
            # æ¯ç§’æª¢æŸ¥ä¸€æ¬¡ä¸­æ–·ä¿¡è™Ÿ
            remaining = min(1.0, end_time - time.time())
            if remaining <= 0:
                break
            time.sleep(remaining)
        
        return self.interrupt_requested

    def start_continuous_monitoring(self):
        """å•Ÿå‹•æŒçºŒç›£æ§ - ä¿®å¾©ä¸­æ–·å•é¡Œ"""
        self.logger.info("ğŸš€ å•Ÿå‹• TDX æŒçºŒç›£æ§æ¨¡å¼")
        self.logger.info(f"â±ï¸ æ”¶é›†é–“éš”: {self.collection_interval} åˆ†é˜")
        self.logger.info(f"ğŸ§¹ æ¸…ç†é »ç‡: æ¯ {self.cleanup_frequency} æ¬¡æ”¶é›†")
        self.logger.info("ğŸ’¡ æŒ‰ Ctrl+C å¯éš¨æ™‚åœæ­¢")
        
        self.is_running = True
        self.interrupt_requested = False
        
        try:
            while self.is_running and not self.interrupt_requested:
                self.collection_count += 1
                
                self.logger.info(f"=== ç¬¬ {self.collection_count} æ¬¡æ”¶é›† ===")
                
                # æª¢æŸ¥é€£çºŒå¤±æ•—æ¬¡æ•¸
                if self.consecutive_failures >= self.max_consecutive_failures:
                    self.logger.error(f"é€£çºŒå¤±æ•— {self.consecutive_failures} æ¬¡ï¼Œæš«åœ10åˆ†é˜")
                    if self.interruptible_sleep(600):  # 10åˆ†é˜ï¼Œä½†å¯ä¸­æ–·
                        break
                    self.consecutive_failures = 0
                
                # åŸ·è¡Œè³‡æ–™æ”¶é›†
                processed_data, output_file = self.single_collection()
                
                # å®šæœŸæ¸…ç†
                if self.collection_count % self.cleanup_frequency == 0:
                    self.cleanup_old_files()
                
                # çµæœå ±å‘Š
                if not processed_data.empty:
                    unique_stations = processed_data['station'].nunique() if 'station' in processed_data.columns else 0
                    self.logger.info(f"âœ… æ”¶é›†æˆåŠŸ: {len(processed_data)} ç­†è¨˜éŒ„, {unique_stations} å€‹ç«™é»")
                else:
                    self.logger.warning("âš ï¸ æœ¬æ¬¡æ”¶é›†ç„¡æœ‰æ•ˆè³‡æ–™")
                
                # å¯ä¸­æ–·çš„ç­‰å¾…
                if self.is_running and not self.interrupt_requested:
                    self.logger.info(f"â³ ç­‰å¾… {self.collection_interval} åˆ†é˜...")
                    if self.interruptible_sleep(self.collection_interval * 60):
                        break
                    
        except KeyboardInterrupt:
            self.logger.info("ğŸ›‘ æ”¶åˆ°éµç›¤ä¸­æ–·")
        except Exception as e:
            self.logger.error(f"âŒ ç›£æ§éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")
        finally:
            self.logger.info("ğŸ ç›£æ§å·²åœæ­¢")
            self.is_running = False

    def test_api_connection(self):
        """æ¸¬è©¦ API é€£æ¥"""
        self.logger.info("ğŸ” æ¸¬è©¦ TDX API é€£æ¥...")
        
        try:
            token = self.get_access_token()
            self.logger.info("âœ… OAuth2 èªè­‰æˆåŠŸ")
            
            live_data = self.get_live_etag_data()
            self.logger.info(f"âœ… å³æ™‚è³‡æ–™: {len(live_data)} ç­†")
            
            if live_data:
                processed = self.process_live_data(live_data[:5])
                self.logger.info(f"âœ… è³‡æ–™è™•ç†: ç”Ÿæˆ {len(processed)} ç­†è™•ç†å¾Œè³‡æ–™")
            
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ API æ¸¬è©¦å¤±æ•—: {e}")
            return False


def run_tdx_system():
    """åŸ·è¡Œ TDX ç³»çµ±"""
    
    print("=" * 60)
    print("ğŸš€ TDX ETag å³æ™‚äº¤é€šç›£æ§ç³»çµ± (ä¿®å¾©ä¸­æ–·å•é¡Œç‰ˆ)")
    print("=" * 60)
    
    system = TDXRealtimeSystem()
    
    print("\né¸æ“‡é‹è¡Œæ¨¡å¼:")
    print("1. API é€£æ¥æ¸¬è©¦")
    print("2. å–®æ¬¡è³‡æ–™æ”¶é›†æ¸¬è©¦")
    print("3. æŒçºŒç›£æ§æ¨¡å¼ (å¯æ­£å¸¸ä¸­æ–·)")
    print("4. è‡ªå®šç¾©é–“éš”ç›£æ§ (å¯æ­£å¸¸ä¸­æ–·)")
    
    try:
        choice = input("\nè«‹é¸æ“‡ (1/2/3/4): ").strip()
        
        if choice == "1":
            print("\nğŸ” åŸ·è¡Œ API é€£æ¥æ¸¬è©¦...")
            if system.test_api_connection():
                print("âœ… API é€£æ¥æ¸¬è©¦æˆåŠŸ!")
            else:
                print("âŒ API é€£æ¥æ¸¬è©¦å¤±æ•—ï¼Œè«‹æª¢æŸ¥æ†‘è­‰è¨­å®š")
        
        elif choice == "2":
            print("\nğŸ§ª åŸ·è¡Œå–®æ¬¡æ”¶é›†æ¸¬è©¦...")
            processed_data, output_file = system.single_collection()
            
            if not processed_data.empty:
                print(f"âœ… æ¸¬è©¦æˆåŠŸ!")
                print(f"ğŸ“Š æ”¶é›†åˆ° {len(processed_data)} ç­†è¨˜éŒ„")
                if 'station' in processed_data.columns:
                    print(f"ğŸ“ æ¶µè“‹ {processed_data['station'].nunique()} å€‹ç«™é»")
                print(f"ğŸ’¾ æª”æ¡ˆ: {output_file}")
                
                print("\nå‰5ç­†è³‡æ–™é è¦½:")
                display_columns = ['station', 'hour', 'minute', 'flow', 'median_speed', 'avg_travel_time']
                available_columns = [col for col in display_columns if col in processed_data.columns]
                print(processed_data[available_columns].head().to_string())
            else:
                print("âŒ æ¸¬è©¦å¤±æ•—ï¼Œè«‹æª¢æŸ¥ API è¨­å®š")
        
        elif choice == "3":
            print(f"\nğŸš€ å•Ÿå‹•æŒçºŒç›£æ§æ¨¡å¼ (æ¯{system.collection_interval}åˆ†é˜æ”¶é›†ä¸€æ¬¡)...")
            print("ğŸ’¡ ç¾åœ¨å¯ä»¥éš¨æ™‚æŒ‰ Ctrl+C åœæ­¢ï¼")
            system.start_continuous_monitoring()
        
        elif choice == "4":
            interval = int(input("è«‹è¼¸å…¥æ”¶é›†é–“éš”(åˆ†é˜): "))
            system.collection_interval = interval
            print(f"\nğŸš€ å•Ÿå‹•è‡ªå®šç¾©ç›£æ§ (æ¯{interval}åˆ†é˜æ”¶é›†ä¸€æ¬¡)...")
            print("ğŸ’¡ ç¾åœ¨å¯ä»¥éš¨æ™‚æŒ‰ Ctrl+C åœæ­¢ï¼")
            system.start_continuous_monitoring()
        
        else:
            print("ç„¡æ•ˆé¸æ“‡")
    
    except KeyboardInterrupt:
        print("\nğŸ‘‹ ç³»çµ±å·²æ­£å¸¸åœæ­¢")
    except Exception as e:
        print(f"\nâŒ ç³»çµ±éŒ¯èª¤: {e}")


if __name__ == "__main__":
    run_tdx_system()