#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import time
import requests
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import json
import logging
import signal
import sys
import glob
from pathlib import Path
from math import exp
from dotenv import load_dotenv
from io import StringIO
import threading
from collections import deque, defaultdict

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
load_dotenv()

class OptimizedIntegratedDataCollectionSystem:
    """
    å„ªåŒ–çš„æ•´åˆå¼äº¤é€šè³‡æ–™æ”¶é›†ç³»çµ± - å«è³‡æ–™ç·©å­˜æ©Ÿåˆ¶
    
    ç‰¹é»ï¼š
    1. åˆå§‹å•Ÿå‹•æ™‚è¼‰å…¥60åˆ†é˜æ­·å²è³‡æ–™
    2. ä¹‹å¾Œæ¯åˆ†é˜åªæ·»åŠ å³æ™‚è³‡æ–™
    3. æ™ºæ…§è³‡æ–™ç·©å­˜èˆ‡æ»‘å‹•è¦–çª—
    4. é«˜æ•ˆç‡è³‡æ–™èåˆ
    5. èˆ‡è¡æ“Šæ³¢ç³»çµ±ç›¸å®¹çš„è¼¸å‡ºæ ¼å¼
    """

    def __init__(self, base_dir="data"):
        """åˆå§‹åŒ–å„ªåŒ–çš„æ•´åˆè³‡æ–™æ”¶é›†ç³»çµ±"""
        
        # æ™ºæ…§è·¯å¾‘åµæ¸¬
        current_dir = os.getcwd()
        if current_dir.endswith('/src/data'):
            self.base_dir = os.path.join('..', '..', base_dir)
        elif current_dir.endswith('/src'):
            self.base_dir = os.path.join('..', base_dir)
        else:
            self.base_dir = base_dir
            
        self.realtime_dir = os.path.join(self.base_dir, "realtime_data")
        self.log_dir = os.path.join(self.base_dir, "logs")
        
        # å»ºç«‹ç›®éŒ„
        for directory in [self.realtime_dir, self.log_dir]:
            os.makedirs(directory, exist_ok=True)
        
        # è¨­å®šæ—¥èªŒ
        self._setup_logging()
        
        # TDX API è¨­å®š
        self.tdx_client_id = os.getenv('TDX_CLIENT_ID')
        self.tdx_client_secret = os.getenv('TDX_CLIENT_SECRET')
        
        if not self.tdx_client_id or not self.tdx_client_secret:
            self.logger.warning("âš ï¸ TDX æ†‘è­‰æœªè¨­å®šï¼Œå°‡åƒ…ä½¿ç”¨ TISC è³‡æ–™")
            self.tdx_available = False
        else:
            self.tdx_available = True
            
        self.tdx_base_url = "https://tdx.transportdata.tw/api/basic"
        self.tdx_auth_url = "https://tdx.transportdata.tw/auth/realms/TDXConnect/protocol/openid-connect/token"
        self.tdx_access_token = None
        self.tdx_token_expires_at = None
        
        # TISC è¨­å®š
        self.tisc_codes = ["M04A", "M05A"]
        self.tisc_base_url = "https://tisvcloud.freeway.gov.tw"
        self.tisc_headers = {'User-Agent': 'Mozilla/5.0'}
        
        # ğŸ“Š è³‡æ–™ç·©å­˜è¨­å®š - æ ¸å¿ƒæ”¹é€²
        self.data_cache = defaultdict(lambda: deque(maxlen=120))  # æ¯ç«™é»ä¿æŒ120å€‹æ™‚é–“é»ï¼ˆ2å°æ™‚ï¼‰
        self.cache_window_minutes = 60                           # ç·©å­˜è¦–çª—60åˆ†é˜
        self.historical_loaded = False                           # æ­·å²è³‡æ–™è¼‰å…¥ç‹€æ…‹
        self.cache_lock = threading.Lock()                      # ç·©å­˜ç·šç¨‹å®‰å…¨
        
        # ç›£æ§åƒæ•¸
        self.collection_interval = 1        # 1åˆ†é˜é–“éš”
        self.cleanup_frequency = 12         # æ¯12æ¬¡æ”¶é›†å¾Œæ¸…ç†ä¸€æ¬¡
        self.max_file_age_hours = 24        # ä¿ç•™24å°æ™‚çš„æª”æ¡ˆ
        
        # ç³»çµ±ç‹€æ…‹
        self.is_running = False
        self.collection_count = 0
        self.last_successful_collection = None
        self.data_source_stats = {
            'tdx_success': 0,
            'tdx_failure': 0,
            'tisc_success': 0, 
            'tisc_failure': 0,
            'fusion_success': 0,
            'cache_hits': 0
        }
        
        # éŒ¯èª¤è™•ç†
        self.consecutive_failures = 0
        self.max_consecutive_failures = 5
        self.failover_mode = False
        
        # ç›®æ¨™ç«™é»
        self.target_stations = self._load_target_stations()
        
        # è»Šç¨®å°æ‡‰è¡¨
        self.vehicle_types = {
            1: 'å°å®¢è»Š', 2: 'å°è²¨è»Š', 3: 'å¤§å®¢è»Š', 4: 'å¤§è²¨è»Š', 5: 'è¯çµè»Š',
            31: 'å°å®¢è»Š', 32: 'å°è²¨è»Š', 41: 'å¤§å®¢è»Š', 42: 'å–®é«”è²¨è»Š'
        }
        
        # ä¿¡è™Ÿè™•ç†
        self.interrupt_requested = False
        signal.signal(signal.SIGINT, self._signal_handler)
        signal.signal(signal.SIGTERM, self._signal_handler)
        
        self.logger.info("ğŸš€ å„ªåŒ–æ•´åˆå¼è³‡æ–™æ”¶é›†ç³»çµ±åˆå§‹åŒ–å®Œæˆ")
        self.logger.info(f"ğŸ“¡ TDX å¯ç”¨: {self.tdx_available}")
        self.logger.info(f"ğŸ“Š TISC ä»£ç¢¼: {self.tisc_codes}")
        self.logger.info(f"ğŸ¯ ç›®æ¨™ç«™é»: {len(self.target_stations)}")
        self.logger.info(f"ğŸ’¾ ç·©å­˜è¦–çª—: {self.cache_window_minutes} åˆ†é˜")
        self.logger.info(f"â±ï¸ æ”¶é›†é–“éš”: {self.collection_interval} åˆ†é˜")

    def _setup_logging(self):
        """è¨­å®šæ—¥èªŒç³»çµ±"""
        log_file = os.path.join(self.log_dir, f"optimized_data_{datetime.now().strftime('%Y%m%d')}.log")
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file, encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)

    def _signal_handler(self, signum, frame):
        """ä¿¡è™Ÿè™•ç†å™¨"""
        self.logger.info(f"\nğŸ›‘ æ”¶åˆ°ä¸­æ–·ä¿¡è™Ÿ {signum}ï¼Œæ­£åœ¨åœæ­¢ç³»çµ±...")
        self.interrupt_requested = True
        self.is_running = False
        
        if hasattr(self, '_interrupt_count'):
            self._interrupt_count += 1
        else:
            self._interrupt_count = 1
            
        if self._interrupt_count >= 2:
            self.logger.info("ğŸš¨ æ”¶åˆ°å¤šæ¬¡ä¸­æ–·ä¿¡è™Ÿï¼Œå¼·åˆ¶é€€å‡º")
            import threading
            if threading.current_thread() is threading.main_thread():
                os._exit(0)
            else:
                sys.exit(1)
        else:
            self.logger.info("ğŸ’¡ å†æŒ‰ä¸€æ¬¡ Ctrl+C å¼·åˆ¶é€€å‡º")

    def _load_target_stations(self):
        """è¼‰å…¥ç›®æ¨™ç«™é»æ¸…å–®"""
        target_stations = set()
        
        # å¾ Etag.csv è¼‰å…¥
        etag_file = os.path.join(self.base_dir, 'Taiwan', 'Etag.csv')
        try:
            df = pd.read_csv(etag_file, encoding='utf-8')
            for station_code in df['ç·¨è™Ÿ'].values:
                if pd.notna(station_code):
                    converted = station_code.replace('-', '').replace('.', '')
                    target_stations.add(converted)
            self.logger.info(f"âœ… å¾ Etag.csv è¼‰å…¥ {len(target_stations)} å€‹ç«™é»")
        except Exception as e:
            self.logger.warning(f"âš ï¸ ç„¡æ³•è¼‰å…¥ Etag.csv: {e}")
        
        # æ·»åŠ é‡è¦ç«™é»
        important_stations = [
            # åœ‹é“1è™ŸåŒ—å‘
            '01F0340N', '01F0376N', '01F0413N', '01F0467N', '01F0492N',
            '01F0511N', '01F0532N', '01F0557N', '01F0584N', '01F0633N',
            '01F0664N', '01F0681N', '01F0699N', '01F0750N', '01F0880N',
            '01F0928N', '01F0956N', '01F0980N', '01F1045N',
            # åœ‹é“1è™Ÿå—å‘  
            '01F0339S', '01F0376S', '01F0413S', '01F0467S', '01F0492S',
            '01F0511S', '01F0532S', '01F0557S', '01F0578S', '01F0633S',
            '01F0664S', '01F0681S', '01F0699S', '01F0750S', '01F0880S',
            '01F0928S', '01F0950S', '01F0980S', '01F1045S',
            # åœ‹é“3è™ŸåŒ—å‘
            '03F0447N', '03F0498N', '03F0525N', '03F0559N', '03F0648N',
            '03F0698N', '03F0746N', '03F0783N', '03F0846N', '03F0961N',
            '03F0996N', '03F1022N',
            # åœ‹é“3è™Ÿå—å‘
            '03F0447S', '03F0498S', '03F0525S', '03F0559S', '03F0648S', 
            '03F0698S', '03F0746S', '03F0783S', '03F0846S', '03F0961S',
            '03F0996S', '03F1022S'
        ]
        
        target_stations.update(important_stations)
        self.logger.info(f"âœ… ç¸½è¨ˆ {len(target_stations)} å€‹ç›®æ¨™ç«™é»")
        
        return target_stations

    # ==================== ğŸ†• è³‡æ–™ç·©å­˜æ ¸å¿ƒæ–¹æ³• ====================
    
    def load_initial_historical_data(self):
        """ğŸ”„ åˆå§‹è¼‰å…¥æ­·å²è³‡æ–™ - åªåœ¨ç³»çµ±å•Ÿå‹•æ™‚åŸ·è¡Œä¸€æ¬¡"""
        if self.historical_loaded:
            self.logger.info("ğŸ“‹ æ­·å²è³‡æ–™å·²è¼‰å…¥ï¼Œè·³éé‡è¤‡è¼‰å…¥")
            return
        
        self.logger.info("ğŸ“¥ é–‹å§‹è¼‰å…¥åˆå§‹æ­·å²è³‡æ–™ï¼ˆ60åˆ†é˜ï¼‰...")
        start_time = datetime.now()
        
        try:
            # å–å¾—60åˆ†é˜çš„æ­·å²è³‡æ–™
            historical_data = self.fetch_tisc_historical_data(window_minutes=60)
            processed_data = self.process_tisc_data(historical_data)
            
            if not processed_data.empty:
                # å°‡æ­·å²è³‡æ–™è¼‰å…¥ç·©å­˜
                self._add_to_cache(processed_data, is_historical=True)
                self.historical_loaded = True
                
                duration = (datetime.now() - start_time).total_seconds()
                self.logger.info(f"âœ… æ­·å²è³‡æ–™è¼‰å…¥å®Œæˆ")
                self.logger.info(f"   ğŸ“Š è¼‰å…¥ {len(processed_data)} ç­†è¨˜éŒ„")
                self.logger.info(f"   ğŸ“ æ¶µè“‹ {processed_data['station'].nunique()} å€‹ç«™é»")
                self.logger.info(f"   â±ï¸ è¼‰å…¥æ™‚é–“: {duration:.1f} ç§’")
                self.logger.info(f"   ğŸ’¾ ç·©å­˜ç«™é»æ•¸: {len(self.data_cache)}")
            else:
                self.logger.warning("âš ï¸ æ­·å²è³‡æ–™è¼‰å…¥å¤±æ•—ï¼Œå°‡å¾å³æ™‚è³‡æ–™é–‹å§‹")
                
        except Exception as e:
            self.logger.error(f"âŒ æ­·å²è³‡æ–™è¼‰å…¥éŒ¯èª¤: {e}")

    def _add_to_cache(self, new_data, is_historical=False):
        """ğŸ“ å°‡è³‡æ–™åŠ å…¥ç·©å­˜"""
        if new_data.empty:
            return
        
        with self.cache_lock:
            current_time = datetime.now()
            
            # æŒ‰ç«™é»åˆ†çµ„åŠ å…¥ç·©å­˜
            for station, station_data in new_data.groupby('station'):
                if station not in self.target_stations:
                    continue
                
                for _, row in station_data.iterrows():
                    # å»ºç«‹æ™‚é–“æˆ³è¨˜
                    if 'timestamp' in row and pd.notna(row['timestamp']):
                        timestamp = pd.to_datetime(row['timestamp'])
                    else:
                        # ä½¿ç”¨hour, minuteå»ºç«‹æ™‚é–“æˆ³è¨˜
                        timestamp = current_time.replace(
                            hour=int(row['hour']), 
                            minute=int(row['minute']), 
                            second=0, 
                            microsecond=0
                        )
                    
                    # åŠ å…¥ç·©å­˜ï¼ˆdequeè‡ªå‹•ç®¡ç†å¤§å°ï¼‰
                    cache_record = {
                        'timestamp': timestamp,
                        'station': station,
                        'flow': row['flow'],
                        'median_speed': row['median_speed'],
                        'avg_travel_time': row['avg_travel_time'],
                        'data_source': row.get('data_source', 'UNKNOWN'),
                        'hour': int(row['hour']),
                        'minute': int(row['minute']),
                        'date': row['date']
                    }
                    
                    self.data_cache[station].append(cache_record)
            
            self.data_source_stats['cache_hits'] += 1
            
            if is_historical:
                self.logger.info(f"ğŸ’¾ æ­·å²è³‡æ–™å·²åŠ å…¥ç·©å­˜: {new_data['station'].nunique()} å€‹ç«™é»")
            else:
                self.logger.info(f"â• å³æ™‚è³‡æ–™å·²åŠ å…¥ç·©å­˜: {new_data['station'].nunique()} å€‹ç«™é»")

    def get_cached_data_for_output(self, time_window_minutes=60):
        """ğŸ“¤ å¾ç·©å­˜å–å¾—è¼¸å‡ºè³‡æ–™"""
        with self.cache_lock:
            if not self.data_cache:
                return pd.DataFrame()
            
            current_time = datetime.now()
            cutoff_time = current_time - timedelta(minutes=time_window_minutes)
            
            output_records = []
            
            for station, cache_deque in self.data_cache.items():
                if station not in self.target_stations:
                    continue
                
                # å–å¾—æ™‚é–“çª—å£å…§çš„è³‡æ–™
                for record in cache_deque:
                    if record['timestamp'] >= cutoff_time:
                        output_records.append(record)
            
            if output_records:
                df = pd.DataFrame(output_records)
                # æŒ‰æ™‚é–“æ’åº
                df = df.sort_values(['station', 'timestamp'])
                self.logger.info(f"ğŸ“‹ ç·©å­˜è³‡æ–™æ“·å–: {len(df)} ç­†è¨˜éŒ„ï¼Œ{df['station'].nunique()} å€‹ç«™é»")
                return df
            
            return pd.DataFrame()

    def cleanup_cache(self):
        """ğŸ§¹ æ¸…ç†éèˆŠçš„ç·©å­˜è³‡æ–™"""
        with self.cache_lock:
            current_time = datetime.now()
            cutoff_time = current_time - timedelta(minutes=self.cache_window_minutes * 2)
            
            cleaned_count = 0
            for station, cache_deque in self.data_cache.items():
                original_length = len(cache_deque)
                
                # ç§»é™¤éèˆŠçš„è¨˜éŒ„
                while cache_deque and cache_deque[0]['timestamp'] < cutoff_time:
                    cache_deque.popleft()
                    cleaned_count += 1
            
            if cleaned_count > 0:
                self.logger.info(f"ğŸ§¹ ç·©å­˜æ¸…ç†: ç§»é™¤ {cleaned_count} ç­†éèˆŠè¨˜éŒ„")

    # ==================== TDX ç›¸é—œæ–¹æ³• ====================
    
    def get_tdx_access_token(self):
        """å–å¾— TDX OAuth2 æ¬Šæ–"""
        if not self.tdx_available:
            return None
            
        if self.tdx_access_token and self.tdx_token_expires_at and datetime.now() < self.tdx_token_expires_at:
            return self.tdx_access_token
        
        auth_data = {
            'grant_type': 'client_credentials',
            'client_id': self.tdx_client_id,
            'client_secret': self.tdx_client_secret
        }
        
        try:
            response = requests.post(
                self.tdx_auth_url,
                data=auth_data,
                headers={'Content-Type': 'application/x-www-form-urlencoded'},
                timeout=10
            )
            response.raise_for_status()
            
            token_data = response.json()
            self.tdx_access_token = token_data['access_token']
            expires_in = token_data.get('expires_in', 3600)
            self.tdx_token_expires_at = datetime.now() + timedelta(seconds=expires_in - 60)
            
            return self.tdx_access_token
            
        except Exception as e:
            self.logger.error(f"âŒ TDX æ¬Šæ–å–å¾—å¤±æ•—: {e}")
            return None

    def fetch_tdx_realtime_data(self):
        """å–å¾— TDX å³æ™‚è³‡æ–™ - åƒ…æœ€æ–°è³‡æ–™"""
        if not self.tdx_available:
            return []
        
        try:
            token = self.get_tdx_access_token()
            if not token:
                return []
            
            headers = {
                'Authorization': f'Bearer {token}',
                'Content-Type': 'application/json'
            }
            
            url = f"{self.tdx_base_url}/v2/Road/Traffic/Live/ETag/Freeway"
            params = {'$format': 'JSON'}
            
            response = requests.get(url, headers=headers, params=params, timeout=30)
            response.raise_for_status()
            
            data = response.json()
            
            if isinstance(data, dict) and 'ETagPairLives' in data:
                live_data = data['ETagPairLives']
            elif isinstance(data, list):
                live_data = data
            else:
                return []
            
            self.data_source_stats['tdx_success'] += 1
            self.logger.info(f"ğŸ“¡ TDX å³æ™‚è³‡æ–™: {len(live_data)} ç­†")
            return live_data
            
        except Exception as e:
            self.data_source_stats['tdx_failure'] += 1
            self.logger.warning(f"âš ï¸ TDX å³æ™‚è³‡æ–™å¤±æ•—: {e}")
            return []

    def process_tdx_data(self, raw_data):
        """è™•ç† TDX è³‡æ–™"""
        if not raw_data:
            return pd.DataFrame()
        
        processed_records = []
        current_time = datetime.now()
        
        for record in raw_data:
            try:
                pair_id = record.get('ETagPairID', '')
                if not pair_id:
                    continue
                
                # è§£æåœ‹é“ç·¨è™Ÿ
                highway_id = ''
                if pair_id.startswith('01F'): highway_id = '1'
                elif pair_id.startswith('02F'): highway_id = '2'
                elif pair_id.startswith('03F'): highway_id = '3'
                elif pair_id.startswith('04F'): highway_id = '4'
                elif pair_id.startswith('05F'): highway_id = '5'
                elif pair_id.startswith('06F'): highway_id = '6'
                
                direction = 'N' if pair_id.endswith('N') else 'S'
                
                flows = record.get('Flows', [])
                if not flows:
                    travel_time = record.get('TravelTime', 0)
                    speed = record.get('SpaceMeanSpeed', 0)
                    volume = record.get('Volume', 0) or record.get('VehicleCount', 0)
                    
                    if volume > 0:
                        flows = [{
                            'VehicleType': 1,
                            'TravelTime': travel_time,
                            'SpaceMeanSpeed': speed,
                            'VehicleCount': volume
                        }]
                
                for flow in flows:
                    vehicle_type = flow.get('VehicleType', 1)
                    travel_time = flow.get('TravelTime', 0)
                    speed = flow.get('SpaceMeanSpeed', 0) or flow.get('Speed', 0)
                    volume = flow.get('VehicleCount', 0) or flow.get('Volume', 0)
                    
                    if volume <= 0:
                        continue
                    
                    equivalent = self._calculate_vehicle_equivalent(vehicle_type, speed)
                    station_id = self._generate_station_id(pair_id, highway_id, direction)
                    
                    processed_record = {
                        'station': station_id,
                        'timestamp': current_time,
                        'date': current_time.strftime('%Y/%m/%d'),
                        'hour': current_time.hour,
                        'minute': current_time.minute,
                        'flow': volume * equivalent,
                        'median_speed': speed,
                        'avg_travel_time': travel_time,
                        'data_source': 'TDX_REALTIME',
                        'vehicle_type': vehicle_type,
                        'raw_volume': volume
                    }
                    
                    processed_records.append(processed_record)
                    
            except Exception as e:
                continue
        
        if processed_records:
            df = pd.DataFrame(processed_records)
            return df
        
        return pd.DataFrame()

    def _generate_station_id(self, pair_id, highway_id, direction):
        """ç”Ÿæˆç«™é»ID"""
        import re
        
        if '-' in pair_id:
            return pair_id.split('-')[1]
        else:
            direction_suffix = 'S' if direction == '0' else 'N'
            highway_prefix = f"{highway_id.zfill(2)}F"
            
            numbers = re.findall(r'\d+', pair_id)
            if numbers:
                number_part = numbers[0].zfill(4)
            else:
                number_part = str(abs(hash(pair_id)) % 9999).zfill(4)
            
            return f"{highway_prefix}{number_part}{direction_suffix}"

    # ==================== TISC ç›¸é—œæ–¹æ³• ====================
    
    def download_tisc_csv(self, url, retries=2):
        """ä¸‹è¼‰ TISC CSV è³‡æ–™"""
        for i in range(retries):
            try:
                response = requests.get(url, headers=self.tisc_headers, timeout=20)
                response.raise_for_status()
                
                csv_content = response.text
                if csv_content.startswith('\ufeff'):
                    csv_content = csv_content[1:]
                
                df = pd.read_csv(StringIO(csv_content), encoding='utf-8')
                
                if len(df.columns) >= 6:
                    if 'M05A' in url:
                        expected_cols = ['TimeStamp', 'GantryFrom', 'GantryTo', 'VehicleType', 'Speed', 'Volume']
                    else:
                        expected_cols = ['TimeStamp', 'GantryFrom', 'GantryTo', 'VehicleType', 'TravelTime', 'VehicleCount']
                    
                    df.columns = expected_cols[:len(df.columns)]
                
                return df
                
            except Exception as e:
                if i < retries - 1:
                    time.sleep(1)
                else:
                    self.logger.debug(f"TISC ä¸‹è¼‰å¤±æ•—: {url}")
        
        return pd.DataFrame()

    def get_tisc_latest_time(self):
        """å–å¾— TISC æœ€æ–°å¯ç”¨æ™‚é–“"""
        current = datetime.now()
        
        for minutes_back in range(0, 121, 5):
            test_time = current - timedelta(minutes=minutes_back)
            test_time = test_time.replace(minute=(test_time.minute // 5) * 5, second=0, microsecond=0)
            
            date_str = test_time.strftime('%Y%m%d')
            hour_str = test_time.strftime('%H')
            minute_str = test_time.strftime('%M')
            ts = f"{hour_str}{minute_str}00"
            
            test_url = f"{self.tisc_base_url}/history/TDCS/M05A/{date_str}/{hour_str}/TDCS_M05A_{date_str}_{ts}.csv"
            
            try:
                response = requests.head(test_url, headers=self.tisc_headers, timeout=5)
                if response.status_code == 200:
                    return test_time
            except:
                continue
        
        return current - timedelta(hours=2)

    def fetch_tisc_historical_data(self, target_time=None, window_minutes=30):
        """å–å¾— TISC æ­·å²è³‡æ–™ï¼ˆåƒ…ç”¨æ–¼åˆå§‹è¼‰å…¥ï¼‰"""
        if target_time is None:
            target_time = self.get_tisc_latest_time()
        
        all_results = {}
        time_points_needed = window_minutes // 5
        
        for code in self.tisc_codes:
            code_data = []
            
            for i in range(time_points_needed):
                point_time = target_time - timedelta(minutes=i*5)
                point_data = self._fetch_tisc_single_timepoint(code, point_time)
                
                if not point_data.empty:
                    code_data.append(point_data)
                
                if len(code_data) >= 6:  # è‡³å°‘6å€‹æ™‚é–“é»ï¼ˆ30åˆ†é˜ï¼‰
                    break
            
            if code_data:
                all_results[code] = pd.concat(code_data, ignore_index=True)
        
        self.data_source_stats['tisc_success'] += 1
        return all_results

    def fetch_tisc_current_data(self):
        """å–å¾— TISC ç•¶å‰æ™‚é–“é»è³‡æ–™ - ç”¨æ–¼æŒçºŒæ›´æ–°"""
        current_time = self.get_tisc_latest_time()
        
        all_results = {}
        for code in self.tisc_codes:
            point_data = self._fetch_tisc_single_timepoint(code, current_time)
            if not point_data.empty:
                all_results[code] = point_data
        
        return all_results

    def _fetch_tisc_single_timepoint(self, code, point_time):
        """å–å¾— TISC å–®ä¸€æ™‚é–“é»è³‡æ–™"""
        date_str = point_time.strftime('%Y%m%d')
        hour_str = point_time.strftime('%H')
        minute_str = point_time.strftime('%M')
        
        minute_int = (int(minute_str) // 5) * 5
        ts = f"{hour_str}{minute_int:02d}00"
        
        url = f"{self.tisc_base_url}/history/TDCS/{code}/{date_str}/{hour_str}/TDCS_{code}_{date_str}_{ts}.csv"
        
        df = self.download_tisc_csv(url)
        if not df.empty:
            df['download_time'] = point_time.replace(minute=minute_int, second=0, microsecond=0)
            df['data_hour'] = int(hour_str)
            df['data_minute'] = minute_int
        
        return df

    def process_tisc_data(self, raw_data):
        """è™•ç† TISC è³‡æ–™"""
        if not raw_data:
            return pd.DataFrame()
        
        processed_records = []
        
        m05a_data = raw_data.get('M05A', pd.DataFrame())
        m04a_data = raw_data.get('M04A', pd.DataFrame())
        
        # è™•ç† M05A (é€Ÿåº¦/æµé‡)
        if not m05a_data.empty:
            target_data = m05a_data[
                m05a_data['GantryFrom'].isin(self.target_stations) | 
                m05a_data['GantryTo'].isin(self.target_stations)
            ].copy()
            
            target_data['station'] = target_data.apply(
                lambda row: row['GantryFrom'] if row['GantryFrom'] in self.target_stations else row['GantryTo'], 
                axis=1
            )
            
            for (gantry, hour, minute), group in target_data.groupby(['station', 'data_hour', 'data_minute']):
                total_weighted_flow = 0
                speeds = []
                
                for _, row in group.iterrows():
                    if row['VehicleType'] in self.vehicle_types and row['Speed'] > 0 and row['Volume'] > 0:
                        equivalent = self._calculate_vehicle_equivalent(row['VehicleType'], row['Speed'])
                        total_weighted_flow += row['Volume'] * equivalent
                        speeds.extend([row['Speed']] * int(row['Volume']))
                
                if total_weighted_flow > 0:
                    historical_time = group.iloc[0]['download_time']
                    
                    record = {
                        'station': gantry,
                        'timestamp': historical_time,
                        'date': historical_time.strftime('%Y/%m/%d'),
                        'hour': hour,
                        'minute': minute,
                        'flow': total_weighted_flow,
                        'median_speed': np.median(speeds) if speeds else 0,
                        'avg_travel_time': 0,
                        'data_source': 'TISC_M05A'
                    }
                    processed_records.append(record)
        
        # è™•ç† M04A (æ—…è¡Œæ™‚é–“)
        if not m04a_data.empty:
            target_data = m04a_data[
                m04a_data['GantryFrom'].isin(self.target_stations) | 
                m04a_data['GantryTo'].isin(self.target_stations)
            ].copy()
            
            target_data['station'] = target_data.apply(
                lambda row: row['GantryFrom'] if row['GantryFrom'] in self.target_stations else row['GantryTo'], 
                axis=1
            )
            
            travel_time_dict = {}
            
            for (gantry, hour, minute), group in target_data.groupby(['station', 'data_hour', 'data_minute']):
                valid_data = group[
                    (group['VehicleType'].isin(self.vehicle_types)) &
                    (group['TravelTime'] > 0) & 
                    (group['VehicleCount'] > 0)
                ]
                
                if not valid_data.empty:
                    total_travel_time = (valid_data['TravelTime'] * valid_data['VehicleCount']).sum()
                    total_count = valid_data['VehicleCount'].sum()
                    avg_travel_time = total_travel_time / total_count if total_count > 0 else 0
                    
                    key = (gantry, hour, minute)
                    travel_time_dict[key] = avg_travel_time
            
            # æ›´æ–°å·²æœ‰è¨˜éŒ„çš„æ—…è¡Œæ™‚é–“
            for record in processed_records:
                if record['data_source'] == 'TISC_M05A':
                    key = (record['station'], record['hour'], record['minute'])
                    if key in travel_time_dict:
                        record['avg_travel_time'] = travel_time_dict[key]
            
            # æ·»åŠ åªæœ‰æ—…è¡Œæ™‚é–“çš„è¨˜éŒ„
            for (gantry, hour, minute), avg_travel_time in travel_time_dict.items():
                existing = any(
                    r['station'] == gantry and r['hour'] == hour and r['minute'] == minute 
                    for r in processed_records
                )
                
                if not existing:
                    matching_data = target_data[
                        (target_data['station'] == gantry) &
                        (target_data['data_hour'] == hour) & 
                        (target_data['data_minute'] == minute)
                    ]
                    
                    if not matching_data.empty:
                        historical_time = matching_data.iloc[0]['download_time']
                        
                        record = {
                            'station': gantry,
                            'timestamp': historical_time,
                            'date': historical_time.strftime('%Y/%m/%d'),
                            'hour': hour,
                            'minute': minute,
                            'flow': 0,
                            'median_speed': 0,
                            'avg_travel_time': avg_travel_time,
                            'data_source': 'TISC_M04A'
                        }
                        processed_records.append(record)
        
        if processed_records:
            df = pd.DataFrame(processed_records)
            df = df.sort_values(['station', 'timestamp'])
            return df
        
        return pd.DataFrame()

    def _calculate_vehicle_equivalent(self, vehicle_type, speed):
        """è¨ˆç®—è»Šç¨®ç•¶é‡"""
        if vehicle_type in [1, 2, 31, 32]:  # å°å®¢è»Š/å°è²¨è»Š
            return 1.0
        elif vehicle_type in [3, 41]:  # å¤§å®¢è»Š
            if speed < 70:
                return 1.13 + 1.66 * exp(-speed / 34.93)
            elif 70 <= speed <= 87:
                return 2.79 - 0.0206 * speed
            else:
                return 1.0
        elif vehicle_type in [4, 42]:  # å¤§è²¨è»Š
            if speed <= 105:
                return 1.9 - 0.00857 * speed
            else:
                return 1.0
        elif vehicle_type == 5:  # è¯çµè»Š
            if speed <= 108:
                return 2.7 - 0.0157 * speed
            else:
                return 1.0
        else:
            return 1.0

    # ==================== ğŸ†• å„ªåŒ–çš„ä¸»è¦è³‡æ–™æ”¶é›†æ–¹æ³• ====================
    
    def single_optimized_collection(self):
        """åŸ·è¡Œå–®æ¬¡å„ªåŒ–è³‡æ–™æ”¶é›† - æ ¸å¿ƒæ”¹é€²"""
        try:
            start_time = datetime.now()
            self.logger.info(f"ğŸ“Š é–‹å§‹å„ªåŒ–è³‡æ–™æ”¶é›† - {start_time.strftime('%H:%M:%S')}")
            
            # ğŸ”„ å¦‚æœæ˜¯ç¬¬ä¸€æ¬¡æ”¶é›†ï¼Œè¼‰å…¥æ­·å²è³‡æ–™
            if not self.historical_loaded:
                self.load_initial_historical_data()
            
            # ğŸ“¡ æ”¶é›†å³æ™‚è³‡æ–™ï¼ˆTDX + TISCæœ€æ–°ï¼‰
            new_data_records = []
            
            # TDX å³æ™‚è³‡æ–™
            if self.tdx_available and not self.failover_mode:
                try:
                    tdx_raw = self.fetch_tdx_realtime_data()
                    tdx_data = self.process_tdx_data(tdx_raw)
                    if not tdx_data.empty:
                        new_data_records.append(tdx_data)
                        self.logger.info(f"ğŸ“¡ TDX å³æ™‚è³‡æ–™: {len(tdx_data)} ç­†")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ TDX æ”¶é›†å¤±æ•—: {e}")
                    self.failover_mode = True
            
            # TISC æœ€æ–°è³‡æ–™é»ï¼ˆä¸å†ä¸‹è¼‰60åˆ†é˜ï¼‰
            try:
                tisc_current = self.fetch_tisc_current_data()
                tisc_data = self.process_tisc_data(tisc_current)
                if not tisc_data.empty:
                    new_data_records.append(tisc_data)
                    self.logger.info(f"ğŸ“Š TISC å³æ™‚è³‡æ–™: {len(tisc_data)} ç­†")
            except Exception as e:
                self.logger.warning(f"âš ï¸ TISC æ”¶é›†å¤±æ•—: {e}")
                self.data_source_stats['tisc_failure'] += 1
            
            # ğŸ’¾ å°‡æ–°è³‡æ–™åŠ å…¥ç·©å­˜
            if new_data_records:
                combined_new_data = pd.concat(new_data_records, ignore_index=True)
                self._add_to_cache(combined_new_data, is_historical=False)
            
            # ğŸ“¤ å¾ç·©å­˜å–å¾—å®Œæ•´è¼¸å‡ºè³‡æ–™
            output_data = self.get_cached_data_for_output(time_window_minutes=60)
            
            # ğŸ’¾ ä¿å­˜è³‡æ–™
            output_file = self.save_cached_data(output_data)
            
            # ğŸ§¹ å®šæœŸæ¸…ç†ç·©å­˜
            if self.collection_count % 10 == 0:
                self.cleanup_cache()
            
            # âœ… æ›´æ–°ç‹€æ…‹
            if not output_data.empty:
                self.failover_mode = False
                self.consecutive_failures = 0
                self.last_successful_collection = datetime.now()
            
            duration = (datetime.now() - start_time).total_seconds()
            self.logger.info(f"âœ… å„ªåŒ–æ”¶é›†å®Œæˆï¼Œè€—æ™‚ {duration:.1f} ç§’")
            
            return output_data, output_file
            
        except Exception as e:
            self.logger.error(f"âŒ å„ªåŒ–è³‡æ–™æ”¶é›†å¤±æ•—: {e}")
            self.consecutive_failures += 1
            return pd.DataFrame(), None

    def save_cached_data(self, cached_data):
        """ä¿å­˜ç·©å­˜è³‡æ–™ - è¼¸å‡ºè¡æ“Šæ³¢ç³»çµ±ç›¸å®¹æ ¼å¼"""
        if cached_data.empty:
            return None
        
        current_time = datetime.now()
        date_str = current_time.strftime('%Y%m%d')
        time_str = current_time.strftime('%H%M')
        
        # ä¸»è¦è¼¸å‡ºæª”æ¡ˆ
        output_file = os.path.join(self.realtime_dir, f"realtime_shock_data_{date_str}_{time_str}.csv")
        
        # æº–å‚™æ¨™æº–è¼¸å‡ºæ ¼å¼
        output_columns = ['station', 'date', 'hour', 'minute', 'flow', 'median_speed', 'avg_travel_time']
        output_data = cached_data[output_columns].copy()
        
        # åªä¿ç•™ç›®æ¨™ç«™é»
        if self.target_stations:
            before_filter = len(output_data)
            output_data = output_data[output_data['station'].isin(self.target_stations)]
            after_filter = len(output_data)
            
            if before_filter != after_filter:
                self.logger.info(f"ğŸ¯ ç«™é»éæ¿¾: {before_filter} â†’ {after_filter} ç­†è¨˜éŒ„")
        
        # ç§»é™¤é‡è¤‡è¨˜éŒ„ï¼ˆåŒç«™é»åŒæ™‚é–“ï¼‰
        output_data = output_data.drop_duplicates(subset=['station', 'hour', 'minute'])
        
        # ä¿å­˜ä¸»è¦æª”æ¡ˆ
        output_data.to_csv(output_file, index=False, encoding='utf-8')
        
        # ä¿å­˜è©³ç´°ç‰ˆæœ¬æª”æ¡ˆ
        if 'data_source' in cached_data.columns and 'timestamp' in cached_data.columns:
            detail_columns = ['station', 'timestamp', 'date', 'hour', 'minute', 'flow', 'median_speed', 'avg_travel_time', 'data_source']
            detail_data = cached_data[detail_columns].copy()
            if self.target_stations:
                detail_data = detail_data[detail_data['station'].isin(self.target_stations)]
            detail_data = detail_data.drop_duplicates(subset=['station', 'hour', 'minute'])
            detail_file = os.path.join(self.realtime_dir, f"detailed_cached_data_{date_str}_{time_str}.csv")
            detail_data.to_csv(detail_file, index=False, encoding='utf-8')
        
        # å ±å‘Šè³‡æ–™æºçµ±è¨ˆ
        if 'data_source' in cached_data.columns:
            source_stats = cached_data['data_source'].value_counts().to_dict()
            self.logger.info(f"ğŸ“Š ç·©å­˜è³‡æ–™æºåˆ†å¸ƒ: {source_stats}")
        
        self.logger.info(f"ğŸ’¾ ç·©å­˜è³‡æ–™å·²ä¿å­˜: {output_file}")
        self.logger.info(f"ğŸ“Š è¨˜éŒ„æ•¸: {len(output_data)}, ç«™é»æ•¸: {output_data['station'].nunique()}")
        
        return output_file

    def cleanup_old_files(self):
        """æ¸…ç†èˆŠæª”æ¡ˆ"""
        self.logger.info("ğŸ§¹ åŸ·è¡Œæª”æ¡ˆæ¸…ç†...")
        
        cutoff_time = datetime.now() - timedelta(hours=self.max_file_age_hours)
        deleted_count = 0
        
        patterns = ["realtime_shock_data_*.csv", "detailed_cached_data_*.csv"]
        for pattern in patterns:
            for file_path in glob.glob(os.path.join(self.realtime_dir, pattern)):
                try:
                    file_time = datetime.fromtimestamp(os.path.getmtime(file_path))
                    if file_time < cutoff_time:
                        os.remove(file_path)
                        deleted_count += 1
                except:
                    pass
        
        if deleted_count > 0:
            self.logger.info(f"âœ… æª”æ¡ˆæ¸…ç†å®Œæˆ: åˆªé™¤ {deleted_count} å€‹æª”æ¡ˆ")

    def interruptible_sleep(self, seconds):
        """å¯ä¸­æ–·çš„ä¼‘çœ å‡½æ•¸"""
        sleep_interval = 0.5
        elapsed = 0
        
        while elapsed < seconds and not self.interrupt_requested:
            remaining = seconds - elapsed
            current_sleep = min(sleep_interval, remaining)
            
            time.sleep(current_sleep)
            elapsed += current_sleep
            
            if self.interrupt_requested:
                self.logger.info("ğŸ’¡ æª¢æ¸¬åˆ°ä¸­æ–·ä¿¡è™Ÿï¼Œåœæ­¢ç­‰å¾…")
                return True
        
        return self.interrupt_requested

    def start_optimized_monitoring(self):
        """ğŸš€ å•Ÿå‹•å„ªåŒ–çš„æŒçºŒç›£æ§"""
        self.logger.info("ğŸš€ å•Ÿå‹•å„ªåŒ–æ•´åˆå¼è³‡æ–™æ”¶é›†ç›£æ§")
        self.logger.info(f"â±ï¸ æ”¶é›†é–“éš”: {self.collection_interval} åˆ†é˜")
        self.logger.info(f"ğŸ’¾ ç·©å­˜è¦–çª—: {self.cache_window_minutes} åˆ†é˜")
        self.logger.info(f"ğŸ¯ è¼¸å‡ºæ ¼å¼: è¡æ“Šæ³¢ç³»çµ±ç›¸å®¹")
        self.logger.info("ğŸ’¡ æŒ‰ Ctrl+C å¯éš¨æ™‚åœæ­¢")
        
        self.is_running = True
        self.interrupt_requested = False
        
        try:
            while self.is_running and not self.interrupt_requested:
                self.collection_count += 1
                
                self.logger.info(f"=== ç¬¬ {self.collection_count} æ¬¡å„ªåŒ–æ”¶é›† ===")
                
                # æª¢æŸ¥é€£çºŒå¤±æ•—æ¬¡æ•¸
                if self.consecutive_failures >= self.max_consecutive_failures:
                    self.logger.error(f"é€£çºŒå¤±æ•— {self.consecutive_failures} æ¬¡ï¼Œæš«åœ10åˆ†é˜")
                    if self.interruptible_sleep(600):
                        break
                    self.consecutive_failures = 0
                
                # åŸ·è¡Œå„ªåŒ–è³‡æ–™æ”¶é›†
                output_data, output_file = self.single_optimized_collection()
                
                # å®šæœŸæ¸…ç†
                if self.collection_count % self.cleanup_frequency == 0:
                    self.cleanup_old_files()
                
                # ç³»çµ±ç‹€æ…‹å ±å‘Š
                if self.collection_count % 10 == 0:
                    self._report_optimized_status()
                
                # çµæœå ±å‘Š
                if not output_data.empty:
                    unique_stations = output_data['station'].nunique()
                    total_cache_records = sum(len(cache) for cache in self.data_cache.values())
                    
                    self.logger.info(f"âœ… æ”¶é›†æˆåŠŸ: {len(output_data)} ç­†è¨˜éŒ„, {unique_stations} å€‹ç«™é»")
                    self.logger.info(f"ğŸ’¾ ç·©å­˜ç‹€æ…‹: {len(self.data_cache)} å€‹ç«™é», {total_cache_records} ç­†è¨˜éŒ„")
                    
                    if 'data_source' in output_data.columns:
                        source_stats = output_data['data_source'].value_counts().to_dict()
                        self.logger.info(f"ğŸ“Š è³‡æ–™æºåˆ†å¸ƒ: {source_stats}")
                else:
                    self.logger.warning("âš ï¸ æœ¬æ¬¡æ”¶é›†ç„¡æœ‰æ•ˆè³‡æ–™")
                
                # å¯ä¸­æ–·çš„ç­‰å¾…
                if self.is_running and not self.interrupt_requested:
                    self.logger.info(f"â³ ç­‰å¾… {self.collection_interval} åˆ†é˜...")
                    if self.interruptible_sleep(self.collection_interval * 60):
                        break
                    
        except KeyboardInterrupt:
            self.logger.info("ğŸ›‘ æ”¶åˆ°éµç›¤ä¸­æ–·")
            self.interrupt_requested = True
            self.is_running = False
        except Exception as e:
            self.logger.error(f"âŒ ç›£æ§éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")
        finally:
            self.logger.info("ğŸ å„ªåŒ–ç›£æ§å·²åœæ­¢")
            self.is_running = False

    def _report_optimized_status(self):
        """å ±å‘Šå„ªåŒ–ç³»çµ±ç‹€æ…‹"""
        success_rate_tdx = 0
        success_rate_tisc = 0
        
        if self.data_source_stats['tdx_success'] + self.data_source_stats['tdx_failure'] > 0:
            success_rate_tdx = (self.data_source_stats['tdx_success'] / 
                               (self.data_source_stats['tdx_success'] + self.data_source_stats['tdx_failure'])) * 100
        
        if self.data_source_stats['tisc_success'] + self.data_source_stats['tisc_failure'] > 0:
            success_rate_tisc = (self.data_source_stats['tisc_success'] / 
                                (self.data_source_stats['tisc_success'] + self.data_source_stats['tisc_failure'])) * 100
        
        total_cache_records = sum(len(cache) for cache in self.data_cache.values())
        
        self.logger.info("=" * 50)
        self.logger.info("ğŸ“ˆ å„ªåŒ–ç³»çµ±ç‹€æ…‹å ±å‘Š")
        self.logger.info(f"ğŸ“Š è³‡æ–™æºçµ±è¨ˆ:")
        self.logger.info(f"   TDX: æˆåŠŸ {self.data_source_stats['tdx_success']}, å¤±æ•— {self.data_source_stats['tdx_failure']} (æˆåŠŸç‡: {success_rate_tdx:.1f}%)")
        self.logger.info(f"   TISC: æˆåŠŸ {self.data_source_stats['tisc_success']}, å¤±æ•— {self.data_source_stats['tisc_failure']} (æˆåŠŸç‡: {success_rate_tisc:.1f}%)")
        self.logger.info(f"ğŸ’¾ ç·©å­˜çµ±è¨ˆ:")
        self.logger.info(f"   ç«™é»æ•¸: {len(self.data_cache)}")
        self.logger.info(f"   ç¸½è¨˜éŒ„æ•¸: {total_cache_records}")
        self.logger.info(f"   ç·©å­˜å‘½ä¸­: {self.data_source_stats['cache_hits']} æ¬¡")
        self.logger.info(f"   æ­·å²è¼‰å…¥: {'âœ…' if self.historical_loaded else 'âŒ'}")
        self.logger.info(f"ğŸ”„ æ•…éšœè½‰ç§»æ¨¡å¼: {'å•Ÿç”¨' if self.failover_mode else 'åœç”¨'}")
        
        if self.last_successful_collection:
            time_since_success = (datetime.now() - self.last_successful_collection).total_seconds() / 60
            self.logger.info(f"ğŸ•’ ä¸Šæ¬¡æˆåŠŸæ”¶é›†: {time_since_success:.1f} åˆ†é˜å‰")
        
        self.logger.info("=" * 50)

    def test_optimized_system(self):
        """æ¸¬è©¦å„ªåŒ–ç³»çµ±"""
        self.logger.info("ğŸ” æ¸¬è©¦å„ªåŒ–æ•´åˆè³‡æ–™æ”¶é›†ç³»çµ±...")
        
        success_count = 0
        total_tests = 0
        
        # æ¸¬è©¦ TDX é€£æ¥
        if self.tdx_available:
            total_tests += 1
            try:
                token = self.get_tdx_access_token()
                if token:
                    self.logger.info("âœ… TDX èªè­‰æˆåŠŸ")
                    success_count += 1
                else:
                    self.logger.warning("âš ï¸ TDX èªè­‰å¤±æ•—")
            except Exception as e:
                self.logger.error(f"âŒ TDX æ¸¬è©¦å¤±æ•—: {e}")
        else:
            self.logger.info("â„¹ï¸ TDX æœªé…ç½®ï¼Œå°‡åƒ…ä½¿ç”¨ TISC è³‡æ–™")
        
        # æ¸¬è©¦ TISC é€£æ¥
        total_tests += 1
        try:
            latest_time = self.get_tisc_latest_time()
            delay_minutes = (datetime.now() - latest_time).total_seconds() / 60
            self.logger.info(f"âœ… TISC æœ€æ–°æ™‚é–“: {latest_time.strftime('%Y-%m-%d %H:%M')} (å»¶é² {delay_minutes:.0f} åˆ†é˜)")
            success_count += 1
        except Exception as e:
            self.logger.error(f"âŒ TISC æ¸¬è©¦å¤±æ•—: {e}")
        
        # æ¸¬è©¦æ­·å²è³‡æ–™è¼‰å…¥
        total_tests += 1
        try:
            self.load_initial_historical_data()
            if self.historical_loaded:
                total_cache_records = sum(len(cache) for cache in self.data_cache.values())
                self.logger.info(f"âœ… æ­·å²è³‡æ–™è¼‰å…¥æˆåŠŸ:")
                self.logger.info(f"   ğŸ’¾ ç·©å­˜ç«™é»: {len(self.data_cache)}")
                self.logger.info(f"   ğŸ“Š ç¸½è¨˜éŒ„æ•¸: {total_cache_records}")
                success_count += 1
            else:
                self.logger.warning("âš ï¸ æ­·å²è³‡æ–™è¼‰å…¥å¤±æ•—")
        except Exception as e:
            self.logger.error(f"âŒ æ­·å²è³‡æ–™è¼‰å…¥æ¸¬è©¦å¤±æ•—: {e}")
        
        # æ¸¬è©¦å„ªåŒ–è³‡æ–™æ”¶é›†
        total_tests += 1
        try:
            output_data, output_file = self.single_optimized_collection()
            if not output_data.empty:
                self.logger.info(f"âœ… å„ªåŒ–æ”¶é›†æ¸¬è©¦æˆåŠŸ:")
                self.logger.info(f"   ğŸ“Š è¨˜éŒ„æ•¸: {len(output_data)}")
                self.logger.info(f"   ğŸ“ ç«™é»æ•¸: {output_data['station'].nunique()}")
                
                if 'data_source' in output_data.columns:
                    source_dist = output_data['data_source'].value_counts()
                    self.logger.info(f"   ğŸ“Š è³‡æ–™æº: {dict(source_dist)}")
                
                self.logger.info(f"   ğŸ’¾ è¼¸å‡ºæª”æ¡ˆ: {output_file}")
                success_count += 1
            else:
                self.logger.warning("âš ï¸ å„ªåŒ–æ”¶é›†æ¸¬è©¦ç„¡è³‡æ–™è¿”å›")
        except Exception as e:
            self.logger.error(f"âŒ å„ªåŒ–æ”¶é›†æ¸¬è©¦å¤±æ•—: {e}")
        
        success_rate = (success_count / total_tests) * 100 if total_tests > 0 else 0
        self.logger.info(f"ğŸ“Š æ¸¬è©¦å®Œæˆ: {success_count}/{total_tests} æˆåŠŸ ({success_rate:.1f}%)")
        
        return success_rate >= 50

    def get_latest_data_for_shockwave(self):
        """ç‚ºè¡æ“Šæ³¢ç³»çµ±æä¾›æœ€æ–°è³‡æ–™"""
        try:
            output_data = self.get_cached_data_for_output(time_window_minutes=60)
            
            if not output_data.empty:
                output_columns = ['station', 'date', 'hour', 'minute', 'flow', 'median_speed', 'avg_travel_time']
                return output_data[output_columns]
            
            return pd.DataFrame()
            
        except Exception as e:
            self.logger.error(f"âŒ ç‚ºè¡æ“Šæ³¢ç³»çµ±å–å¾—è³‡æ–™å¤±æ•—: {e}")
            return pd.DataFrame()


def run_optimized_integrated_system():
    """åŸ·è¡Œå„ªåŒ–çš„æ•´åˆè³‡æ–™æ”¶é›†ç³»çµ±"""
    
    print("=" * 80)
    print("ğŸš€ å„ªåŒ–çš„æ•´åˆå¼äº¤é€šè³‡æ–™æ”¶é›†ç³»çµ± v2.0")
    print("ğŸ’¾ è³‡æ–™ç·©å­˜æ©Ÿåˆ¶ - åªè¼‰å…¥ä¸€æ¬¡æ­·å²è³‡æ–™")
    print("âš¡ æŒçºŒç´¯ç©å³æ™‚è³‡æ–™ - é«˜æ•ˆèƒ½ç›£æ§")
    print("ğŸŒŠ å°ˆç‚ºè¡æ“Šæ³¢æª¢æ¸¬ç³»çµ±è¨­è¨ˆ")
    print("=" * 80)
    
    system = OptimizedIntegratedDataCollectionSystem(base_dir="data")
    
    print("\nğŸ’¡ æ ¸å¿ƒæ”¹é€²:")
    print("âœ¨ åˆå§‹å•Ÿå‹•æ™‚è¼‰å…¥60åˆ†é˜æ­·å²è³‡æ–™")
    print("âš¡ ä¹‹å¾Œæ¯åˆ†é˜åªä¸‹è¼‰å³æ™‚è³‡æ–™")
    print("ğŸ’¾ æ™ºæ…§ç·©å­˜ç®¡ç†ï¼Œæ»‘å‹•è¦–çª—æ©Ÿåˆ¶")
    print("ğŸ”„ è‡ªå‹•è³‡æ–™èåˆèˆ‡å“è³ªæ§åˆ¶")
    
    print("\nğŸ“ è³‡æ–™å°‡å„²å­˜è‡³: data/realtime_data/")
    
    print("\né¸æ“‡é‹è¡Œæ¨¡å¼:")
    print("1. ç³»çµ±å®Œæ•´æ¸¬è©¦ (æ¸¬è©¦æ‰€æœ‰åŠŸèƒ½)")
    print("2. å„ªåŒ–æŒçºŒç›£æ§ (æ¯1åˆ†é˜æ›´æ–°ï¼Œæ¨è–¦)")
    print("3. è‡ªå®šç¾©é–“éš”ç›£æ§")
    print("4. å–®æ¬¡å„ªåŒ–æ”¶é›†æ¸¬è©¦")
    print("5. æŸ¥çœ‹ç·©å­˜ç‹€æ…‹")
    
    try:
        choice = input("\nè«‹é¸æ“‡ (1/2/3/4/5): ").strip()
        
        if choice == "1":
            print("\nğŸ” åŸ·è¡Œç³»çµ±å®Œæ•´æ¸¬è©¦...")
            if system.test_optimized_system():
                print("âœ… å„ªåŒ–ç³»çµ±æ¸¬è©¦é€šé!")
                print("ğŸ¯ ç³»çµ±å·²æº–å‚™å¥½ç‚ºè¡æ“Šæ³¢æª¢æ¸¬æä¾›é«˜æ•ˆè³‡æ–™æµ")
            else:
                print("âŒ ç³»çµ±æ¸¬è©¦å¤±æ•—ï¼Œè«‹æª¢æŸ¥è¨­å®š")
        
        elif choice == "2":
            print("\nğŸš€ å•Ÿå‹•å„ªåŒ–æŒçºŒç›£æ§æ¨¡å¼...")
            print("ğŸ’¾ åˆå§‹è¼‰å…¥60åˆ†é˜æ­·å²è³‡æ–™ï¼ˆåƒ…é¦–æ¬¡ï¼‰")
            print("âš¡ æ¯1åˆ†é˜æ·»åŠ å³æ™‚è³‡æ–™åˆ°ç·©å­˜")
            print("ğŸ“¤ æŒçºŒè¼¸å‡ºå®Œæ•´æ™‚é–“åºåˆ—è³‡æ–™")
            print("ğŸ’¡ æŒ‰ Ctrl+C å¯éš¨æ™‚åœæ­¢ (å¯èƒ½éœ€è¦æŒ‰å…©æ¬¡)")
            
            try:
                system.start_optimized_monitoring()
            except KeyboardInterrupt:
                print("\nğŸ›‘ æ¥æ”¶åˆ°ä¸­æ–·ä¿¡è™Ÿï¼Œæ­£åœ¨åœæ­¢...")
                system.interrupt_requested = True
                system.is_running = False
                time.sleep(1)
                print("âœ… å„ªåŒ–ç³»çµ±å·²å®‰å…¨åœæ­¢")
        
        elif choice == "3":
            interval = int(input("è«‹è¼¸å…¥æ”¶é›†é–“éš”(åˆ†é˜ï¼Œå»ºè­°1-3åˆ†é˜): "))
            if interval < 1:
                print("âš ï¸ é–“éš”éçŸ­ï¼Œèª¿æ•´ç‚º1åˆ†é˜")
                interval = 1
            elif interval > 5:
                print("âš ï¸ é–“éš”éé•·å¯èƒ½å½±éŸ¿å³æ™‚æ€§")
            
            system.collection_interval = interval
            print(f"\nğŸš€ å•Ÿå‹•è‡ªå®šç¾©ç›£æ§ (æ¯{interval}åˆ†é˜æ”¶é›†ä¸€æ¬¡)...")
            print("ğŸ’¡ æŒ‰ Ctrl+C å¯éš¨æ™‚åœæ­¢")
            
            try:
                system.start_optimized_monitoring()
            except KeyboardInterrupt:
                print("\nğŸ›‘ æ¥æ”¶åˆ°ä¸­æ–·ä¿¡è™Ÿï¼Œæ­£åœ¨åœæ­¢...")
                system.interrupt_requested = True
                system.is_running = False
                time.sleep(1)
                print("âœ… å„ªåŒ–ç³»çµ±å·²å®‰å…¨åœæ­¢")
        
        elif choice == "4":
            print("\nğŸ§ª åŸ·è¡Œå–®æ¬¡å„ªåŒ–æ”¶é›†æ¸¬è©¦...")
            
            # å…ˆè¼‰å…¥æ­·å²è³‡æ–™
            if not system.historical_loaded:
                print("ğŸ“¥ é¦–æ¬¡åŸ·è¡Œï¼Œè¼‰å…¥æ­·å²è³‡æ–™...")
                system.load_initial_historical_data()
            
            output_data, output_file = system.single_optimized_collection()
            
            if not output_data.empty:
                print(f"âœ… å„ªåŒ–æ”¶é›†æ¸¬è©¦æˆåŠŸ!")
                print(f"ğŸ“Š è¼¸å‡ºè¨˜éŒ„æ•¸: {len(output_data)}")
                print(f"ğŸ“ æ¶µè“‹ç«™é»æ•¸: {output_data['station'].nunique()}")
                
                total_cache_records = sum(len(cache) for cache in system.data_cache.values())
                print(f"ğŸ’¾ ç·©å­˜ç‹€æ…‹: {len(system.data_cache)} å€‹ç«™é», {total_cache_records} ç­†è¨˜éŒ„")
                
                if 'data_source' in output_data.columns:
                    source_dist = output_data['data_source'].value_counts()
                    print(f"ğŸ“Š è³‡æ–™æºåˆ†å¸ƒ:")
                    for source, count in source_dist.items():
                        print(f"   {source}: {count} ç­†")
                
                print(f"ğŸ’¾ è¼¸å‡ºæª”æ¡ˆ: {output_file}")
                
                # é¡¯ç¤ºå‰5ç­†è³‡æ–™é è¦½
                print("\nğŸ“‹ å‰5ç­†è³‡æ–™é è¦½:")
                display_cols = ['station', 'hour', 'minute', 'flow', 'median_speed', 'avg_travel_time']
                available_cols = [col for col in display_cols if col in output_data.columns]
                print(output_data[available_cols].head().to_string(index=False))
            else:
                print("âŒ æ¸¬è©¦å¤±æ•—ï¼Œè«‹æª¢æŸ¥ç¶²è·¯é€£æ¥")
        
        elif choice == "5":
            print("\nğŸ“‹ æŸ¥çœ‹ç·©å­˜ç‹€æ…‹...")
            
            if not system.historical_loaded:
                print("âš ï¸ æ­·å²è³‡æ–™å°šæœªè¼‰å…¥")
                load_choice = input("æ˜¯å¦è¼‰å…¥æ­·å²è³‡æ–™? (y/n): ").strip().lower()
                if load_choice == 'y':
                    system.load_initial_historical_data()
            
            if system.data_cache:
                total_records = sum(len(cache) for cache in system.data_cache.values())
                print(f"ğŸ’¾ ç·©å­˜çµ±è¨ˆ:")
                print(f"   ç«™é»æ•¸: {len(system.data_cache)}")
                print(f"   ç¸½è¨˜éŒ„æ•¸: {total_records}")
                print(f"   æ­·å²è¼‰å…¥ç‹€æ…‹: {'âœ…' if system.historical_loaded else 'âŒ'}")
                
                # é¡¯ç¤ºå‰10å€‹ç«™é»çš„è¨˜éŒ„æ•¸
                print(f"\nğŸ“Š å‰10å€‹ç«™é»è¨˜éŒ„æ•¸:")
                station_counts = [(station, len(cache)) for station, cache in system.data_cache.items()]
                station_counts.sort(key=lambda x: x[1], reverse=True)
                for station, count in station_counts[:10]:
                    print(f"   {station}: {count} ç­†")
                
                # å–å¾—æœ€æ–°è³‡æ–™æ¨£æœ¬
                latest_data = system.get_cached_data_for_output(time_window_minutes=30)
                if not latest_data.empty:
                    print(f"\nğŸ“‹ æœ€è¿‘30åˆ†é˜è³‡æ–™é è¦½:")
                    display_cols = ['station', 'hour', 'minute', 'flow', 'median_speed']
                    available_cols = [col for col in display_cols if col in latest_data.columns]
                    print(latest_data[available_cols].tail().to_string(index=False))
            else:
                print("ğŸ“­ ç·©å­˜ç‚ºç©º")
        
        else:
            print("ç„¡æ•ˆé¸æ“‡")
    
    except KeyboardInterrupt:
        print("\nğŸ›‘ ç¨‹åºè¢«ä¸­æ–·")
        if hasattr(system, 'is_running'):
            system.is_running = False
            system.interrupt_requested = True
        print("âœ… å„ªåŒ–ç³»çµ±å·²åœæ­¢")
    except Exception as e:
        print(f"\nâŒ ç³»çµ±éŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()
    finally:
        print("\nğŸ‘‹ æ„Ÿè¬ä½¿ç”¨å„ªåŒ–çš„æ•´åˆå¼è³‡æ–™æ”¶é›†ç³»çµ±ï¼")


if __name__ == "__main__":
    run_optimized_integrated_system()