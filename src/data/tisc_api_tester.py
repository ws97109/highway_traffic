#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import time
import requests
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from io import StringIO
from math import exp
import glob
import threading
import logging
import signal
import sys
from pathlib import Path

class ProductionRealtimeSystem:
    """
    ç”Ÿç”¢ç‰ˆå³æ™‚äº¤é€šç›£æ§ç³»çµ±
    
    ç‰¹é»ï¼š
    1. æŒçºŒè‡ªå‹•ç›£æ§
    2. è‡ªå‹•æ¸…ç†æ©Ÿåˆ¶
    3. å®Œæ•´æ—¥èªŒè¨˜éŒ„
    4. éŒ¯èª¤æ¢å¾©æ©Ÿåˆ¶
    5. å„ªé›…é—œé–‰
    """

    def __init__(self, base_dir="../data"):
        """åˆå§‹åŒ–ç”Ÿç”¢ç‰ˆç³»çµ±"""
        
        # åŸºæœ¬è¨­å®š
        self.base_dir = base_dir
        self.realtime_dir = os.path.join(self.base_dir, "realtime_data")
        self.log_dir = os.path.join(self.base_dir, "logs")
        
        # å»ºç«‹ç›®éŒ„
        os.makedirs(self.realtime_dir, exist_ok=True)
        os.makedirs(self.log_dir, exist_ok=True)
        
        # è¨­å®šæ—¥èªŒ
        self._setup_logging()
        
        # ç³»çµ±åƒæ•¸
        self.codes = ["M04A", "M05A"]
        self.base_url = "https://tisvcloud.freeway.gov.tw"
        self.headers = {'User-Agent': 'Mozilla/5.0'}
        
        # ç›£æ§åƒæ•¸
        self.collection_interval = 5      # æ¯5åˆ†é˜æ”¶é›†ä¸€æ¬¡
        self.data_window_minutes = 30     # è³‡æ–™è¦–çª—30åˆ†é˜
        self.min_data_points = 6          # æœ€å°‘éœ€è¦6å€‹è³‡æ–™é»
        
        # æ¸…ç†åƒæ•¸
        self.cleanup_frequency = 12       # æ¯12æ¬¡æ”¶é›†å¾Œæ¸…ç†ä¸€æ¬¡ï¼ˆæ¯å°æ™‚ï¼‰
        self.max_file_age_hours = 24      # ä¿ç•™24å°æ™‚çš„æª”æ¡ˆ
        self.max_log_age_days = 1         # ä¿ç•™7å¤©çš„æ—¥èªŒ
        
        # è¨˜æ†¶é«”ç®¡ç†
        self.data_buffer = {}
        self.buffer_max_points = 24       # è¨˜æ†¶é«”ä¸­ä¿ç•™2å°æ™‚è³‡æ–™
        
        # ç³»çµ±ç‹€æ…‹
        self.is_running = False
        self.collection_count = 0
        self.last_successful_collection = None
        
        # éŒ¯èª¤è™•ç†
        self.consecutive_failures = 0
        self.max_consecutive_failures = 5
        
        # ç›®æ¨™é–€æ¶
        self.target_gantries = [
            # åœ‹é“1è™ŸåŒ—å‘ (19å€‹)
            '01F0340N', '01F0376N', '01F0413N', '01F0467N', '01F0492N',
            '01F0511N', '01F0532N', '01F0557N', '01F0584N', '01F0633N',
            '01F0664N', '01F0681N', '01F0699N', '01F0750N', '01F0880N',
            '01F0928N', '01F0956N', '01F0980N', '01F1045N',
            # åœ‹é“1è™Ÿå—å‘ (19å€‹)
            '01F0339S', '01F0376S', '01F0413S', '01F0467S', '01F0492S',
            '01F0511S', '01F0532S', '01F0557S', '01F0578S', '01F0633S',
            '01F0664S', '01F0681S', '01F0699S', '01F0750S', '01F0880S',
            '01F0928S', '01F0950S', '01F0980S', '01F1045S',
            # åœ‹é“3è™ŸåŒ—å‘ (12å€‹)
            '03F0447N', '03F0498N', '03F0525N', '03F0559N', '03F0648N',
            '03F0698N', '03F0746N', '03F0783N', '03F0846N', '03F0961N',
            '03F0996N', '03F1022N',
            # åœ‹é“3è™Ÿå—å‘ (12å€‹)
            '03F0447S', '03F0498S', '03F0525S', '03F0559S', '03F0648S',
            '03F0698S', '03F0746S', '03F0783S', '03F0846S', '03F0961S',
            '03F0996S', '03F1022S'
        ]
        
        # è»Šç¨®åˆ†é¡
        self.vehicle_types = {31: 'å°å®¢è»Š', 32: 'å°è²¨è»Š', 41: 'å¤§å®¢è»Š', 42: 'å–®é«”è²¨è»Š', 5: '5è»¸è¯çµè»Š'}
        
        # è¨»å†Šä¿¡è™Ÿè™•ç†å™¨ï¼ˆå„ªé›…é—œé–‰ï¼‰
        signal.signal(signal.SIGINT, self._signal_handler)
        signal.signal(signal.SIGTERM, self._signal_handler)
        
        self.logger.info("ğŸš€ ç”Ÿç”¢ç‰ˆå³æ™‚ç›£æ§ç³»çµ±åˆå§‹åŒ–å®Œæˆ")
        self.logger.info(f"ğŸ“¡ ç›£æ§ä»£ç¢¼: {self.codes}")
        self.logger.info(f"ğŸ“ ç›®æ¨™é–€æ¶: {len(self.target_gantries)} å€‹")
        self.logger.info(f"ğŸ’¾ è³‡æ–™ç›®éŒ„: {self.realtime_dir}")
        self.logger.info(f"â±ï¸ æ”¶é›†é–“éš”: {self.collection_interval} åˆ†é˜")

    def _setup_logging(self):
        """è¨­å®šæ—¥èªŒç³»çµ±"""
        log_file = os.path.join(self.log_dir, f"realtime_system_{datetime.now().strftime('%Y%m%d')}.log")
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file, encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)

    def _signal_handler(self, signum, frame):
        """ä¿¡è™Ÿè™•ç†å™¨ - å„ªé›…é—œé–‰"""
        self.logger.info(f"æ”¶åˆ°ä¿¡è™Ÿ {signum}ï¼Œæº–å‚™å„ªé›…é—œé–‰...")
        self.is_running = False

    def download_csv_data(self, url, retries=2, wait=1):
        """ä¸‹è¼‰CSVè³‡æ–™"""
        for i in range(1, retries + 1):
            try:
                response = requests.get(url, headers=self.headers, timeout=20)
                response.raise_for_status()
                
                csv_content = response.text
                if csv_content.startswith('\ufeff'):
                    csv_content = csv_content[1:]
                
                df = pd.read_csv(StringIO(csv_content), encoding='utf-8')
                
                if len(df.columns) >= 6:
                    if 'M05A' in url:
                        expected_cols = ['TimeStamp', 'GantryFrom', 'GantryTo', 'VehicleType', 'Speed', 'Volume']
                    else:
                        expected_cols = ['TimeStamp', 'GantryFrom', 'GantryTo', 'VehicleType', 'TravelTime', 'VehicleCount']
                    
                    df.columns = expected_cols[:len(df.columns)]
                
                return df
                
            except Exception as e:
                if i < retries:
                    time.sleep(wait)
                else:
                    self.logger.warning(f"ä¸‹è¼‰å¤±æ•—: {url} - {e}")
        
        return pd.DataFrame()

    def get_latest_available_time(self):
        """å‹•æ…‹å°‹æ‰¾æœ€æ–°å¯ç”¨çš„è³‡æ–™æ™‚é–“ - å¾æœ€è¿‘æ™‚é–“é–‹å§‹æœå°‹"""
        current = datetime.now()
        
        # é¦–å…ˆå˜—è©¦ç•¶å‰æ™‚é–“å’Œæœ€è¿‘çš„å¹¾å€‹5åˆ†é˜é–“éš”
        search_times = []
        
        # ç•¶å‰æ™‚é–“èª¿æ•´åˆ°5åˆ†é˜é–“éš”
        current_minute = (current.minute // 5) * 5
        current_adjusted = current.replace(minute=current_minute, second=0, microsecond=0)
        
        # ç”Ÿæˆæœå°‹æ™‚é–“åˆ—è¡¨ï¼šç•¶å‰æ™‚é–“å¾€å‰æ¯5åˆ†é˜ä¸€æ¬¡ï¼Œæœå°‹2å°æ™‚
        for minutes_back in range(0, 121, 5):
            search_time = current_adjusted - timedelta(minutes=minutes_back)
            search_times.append(search_time)
        
        self.logger.info(f"é–‹å§‹æœå°‹æœ€æ–°å¯ç”¨è³‡æ–™ï¼Œå¾ {current_adjusted.strftime('%H:%M')} é–‹å§‹å¾€å‰æ‰¾...")
        
        for i, test_time in enumerate(search_times):
            test_url = self._build_test_url(test_time)
            
            try:
                response = requests.head(test_url, headers=self.headers, timeout=5)
                if response.status_code == 200:
                    delay_minutes = (current - test_time).total_seconds() / 60
                    if delay_minutes < 10:
                        self.logger.info(f"âœ… ç™¼ç¾å³æ™‚è³‡æ–™: {test_time.strftime('%Y-%m-%d %H:%M')} (å»¶é² {delay_minutes:.0f} åˆ†é˜)")
                    else:
                        self.logger.info(f"âœ… ç™¼ç¾å¯ç”¨è³‡æ–™: {test_time.strftime('%Y-%m-%d %H:%M')} (å»¶é² {delay_minutes:.0f} åˆ†é˜)")
                    return test_time
            except Exception as e:
                if i < 5:  # åªåœ¨å‰5æ¬¡å¤±æ•—æ™‚è¨˜éŒ„è©³ç´°éŒ¯èª¤
                    self.logger.debug(f"æ¸¬è©¦ {test_time.strftime('%H:%M')} å¤±æ•—: {e}")
                continue
        
        self.logger.warning("âš ï¸ æœªæ‰¾åˆ°ä»»ä½•å¯ç”¨è³‡æ–™ï¼Œä½¿ç”¨é è¨­æ™‚é–“")
        return current - timedelta(hours=2)

    def _build_test_url(self, target_time):
        """å»ºç«‹æ¸¬è©¦URL - èª¿æ•´åˆ°5åˆ†é˜é–“éš”"""
        # å°‡æ™‚é–“èª¿æ•´åˆ°æœ€æ¥è¿‘çš„5åˆ†é˜é–“éš”
        minute = target_time.minute
        rounded_minute = (minute // 5) * 5  # å–5çš„å€æ•¸
        adjusted_time = target_time.replace(minute=rounded_minute, second=0, microsecond=0)
        
        date_str = adjusted_time.strftime('%Y%m%d')
        hour_str = adjusted_time.strftime('%H')
        minute_str = adjusted_time.strftime('%M')
        ts = f"{hour_str}{minute_str}00"
        
        return f"{self.base_url}/history/TDCS/M05A/{date_str}/{hour_str}/TDCS_M05A_{date_str}_{ts}.csv"

    def fetch_recent_data(self, target_time=None):
        """æ”¶é›†æœ€è¿‘è³‡æ–™"""
        if target_time is None:
            target_time = self.get_latest_available_time()
        
        self.logger.info(f"æ”¶é›† {target_time.strftime('%Y-%m-%d %H:xx')} æœ€è¿‘ {self.data_window_minutes} åˆ†é˜è³‡æ–™")
        
        all_results = {}
        time_points_needed = self.data_window_minutes // 5
        
        for code in self.codes:
            code_data = []
            
            for i in range(time_points_needed):
                point_time = target_time - timedelta(minutes=i*5)
                point_data = self._fetch_single_timepoint(code, point_time)
                
                if not point_data.empty:
                    point_data['data_sequence'] = i
                    code_data.append(point_data)
                
                if len(code_data) >= self.min_data_points:
                    break
            
            if code_data:
                all_results[code] = pd.concat(code_data, ignore_index=True)
                self.logger.info(f"{code} æ”¶é›†: {len(all_results[code])} ç­†")
        
        return all_results

    def _fetch_single_timepoint(self, code, point_time):
        """ç²å–å–®ä¸€æ™‚é–“é»è³‡æ–™"""
        date_str = point_time.strftime('%Y%m%d')
        hour_str = point_time.strftime('%H')
        minute_str = point_time.strftime('%M')
        
        minute_int = (int(minute_str) // 5) * 5
        ts = f"{hour_str}{minute_int:02d}00"
        
        url = f"{self.base_url}/history/TDCS/{code}/{date_str}/{hour_str}/TDCS_{code}_{date_str}_{ts}.csv"
        
        df = self.download_csv_data(url)
        if not df.empty:
            df['download_time'] = point_time.replace(minute=minute_int, second=0, microsecond=0)
            df['data_hour'] = int(hour_str)
            df['data_minute'] = minute_int
        
        return df

    def process_data(self, raw_data):
        """è™•ç†åŸå§‹è³‡æ–™"""
        if not raw_data:
            return pd.DataFrame()
        
        self.logger.info("è™•ç†åŸå§‹è³‡æ–™...")
        
        m05a_data = raw_data.get('M05A', pd.DataFrame())
        m04a_data = raw_data.get('M04A', pd.DataFrame())
        
        processed_records = []
        
        # è™•ç†M05Aå’ŒM04Aè³‡æ–™
        for data, data_type in [(m05a_data, 'M05A'), (m04a_data, 'M04A')]:
            if data.empty:
                continue
            
            # åªä¿ç•™ç›®æ¨™é–€æ¶
            target_data = data[
                data['GantryFrom'].isin(self.target_gantries) | 
                data['GantryTo'].isin(self.target_gantries)
            ]
            
            if target_data.empty:
                continue
            
            # æŒ‰é–€æ¶å’Œæ™‚é–“åˆ†çµ„è™•ç†
            for gantry_col in ['GantryFrom', 'GantryTo']:
                gantry_data = target_data[target_data[gantry_col].isin(self.target_gantries)]
                
                for (gantry, hour, minute), group in gantry_data.groupby([gantry_col, 'data_hour', 'data_minute']):
                    
                    if data_type == 'M05A':
                        total_weighted_flow = 0
                        speeds = []
                        
                        for _, row in group.iterrows():
                            if row['VehicleType'] in self.vehicle_types and row['Speed'] > 0 and row['Volume'] > 0:
                                equivalent = self._calculate_vehicle_equivalent(row['VehicleType'], row['Speed'])
                                total_weighted_flow += row['Volume'] * equivalent
                                speeds.extend([row['Speed']] * int(row['Volume']))
                        
                        median_speed = np.median(speeds) if speeds else 0
                        
                        record = {
                            'station': gantry,
                            'date': datetime.now().strftime('%Y/%m/%d'),
                            'hour': hour,
                            'minute': minute,
                            'flow': total_weighted_flow,
                            'median_speed': median_speed,
                            'avg_travel_time': 0
                        }
                        
                    else:  # M04A
                        valid_data = group[
                            (group['VehicleType'].isin(self.vehicle_types)) &
                            (group['TravelTime'] > 0) & 
                            (group['VehicleCount'] > 0)
                        ]
                        
                        if not valid_data.empty:
                            total_travel_time = (valid_data['TravelTime'] * valid_data['VehicleCount']).sum()
                            total_count = valid_data['VehicleCount'].sum()
                            avg_travel_time = total_travel_time / total_count if total_count > 0 else 0
                            
                            record = {
                                'station': gantry,
                                'date': datetime.now().strftime('%Y/%m/%d'),
                                'hour': hour,
                                'minute': minute,
                                'flow': 0,
                                'median_speed': 0,
                                'avg_travel_time': avg_travel_time
                            }
                        else:
                            continue
                    
                    processed_records.append(record)
        
        if processed_records:
            df = pd.DataFrame(processed_records)
            
            # åˆä½µåŒä¸€ç«™é»åŒä¸€æ™‚é–“çš„è³‡æ–™
            final_df = df.groupby(['station', 'date', 'hour', 'minute']).agg({
                'flow': 'max',
                'median_speed': 'max', 
                'avg_travel_time': 'max'
            }).reset_index()
            
            self.logger.info(f"è™•ç†å®Œæˆ: {len(final_df)} ç­†è¨˜éŒ„")
            return final_df
        
        return pd.DataFrame()

    def _calculate_vehicle_equivalent(self, vehicle_type, speed):
        """è¨ˆç®—è»Šç¨®ç•¶é‡"""
        if vehicle_type in [31, 32]:
            return 1.0
        elif vehicle_type == 41:
            if speed < 70:
                return 1.13 + 1.66 * exp(-speed / 34.93)
            elif 70 <= speed <= 87:
                return 2.79 - 0.0206 * speed
            else:
                return 1.0
        elif vehicle_type == 42:
            if speed <= 105:
                return 1.9 - 0.00857 * speed
            else:
                return 1.0
        elif vehicle_type == 5:
            if speed <= 108:
                return 2.7 - 0.0157 * speed
            else:
                return 1.0
        else:
            return 1.0

    def save_data(self, processed_data):
        """ä¿å­˜è™•ç†å¾Œçš„è³‡æ–™"""
        if processed_data.empty:
            return None
        
        # ä½¿ç”¨è³‡æ–™çš„å¯¦éš›æ™‚é–“ç¯„åœä½œç‚ºæª”æ¡ˆåç¨±
        if not processed_data.empty:
            min_hour = processed_data['hour'].min()
            max_hour = processed_data['hour'].max()
            min_minute = processed_data['minute'].min()
            max_minute = processed_data['minute'].max()
            data_date = processed_data['date'].iloc[0].replace('/', '')  # 2025/08/02 -> 20250802
            
            # å¦‚æœè·¨å°æ™‚ï¼Œä½¿ç”¨æ™‚é–“ç¯„åœï¼›å¦å‰‡ä½¿ç”¨å–®ä¸€æ™‚é–“
            if min_hour != max_hour:
                timestamp = f"{data_date}_{min_hour:02d}{min_minute:02d}-{max_hour:02d}{max_minute:02d}"
            else:
                timestamp = f"{data_date}_{min_hour:02d}{min_minute:02d}-{max_minute:02d}"
        else:
            # å‚™ç”¨æ–¹æ¡ˆï¼šä½¿ç”¨åŸ·è¡Œæ™‚é–“
            timestamp = datetime.now().strftime('%Y%m%d_%H%M')
            
        output_file = os.path.join(self.realtime_dir, f"realtime_shock_data_{timestamp}.csv")
        
        processed_data.to_csv(output_file, index=False, encoding='utf-8')
        
        self.logger.info(f"è³‡æ–™å·²ä¿å­˜: {output_file}")
        self.logger.info(f"è¨˜éŒ„æ•¸: {len(processed_data)}, ç«™é»æ•¸: {processed_data['station'].nunique()}")
        
        return output_file

    def update_buffer(self, new_data):
        """æ›´æ–°è¨˜æ†¶é«”ç·©è¡"""
        if new_data.empty:
            return
        
        current_time = datetime.now()
        
        for station in new_data['station'].unique():
            if station not in self.data_buffer:
                self.data_buffer[station] = []
            
            station_new_data = new_data[new_data['station'] == station]
            for _, row in station_new_data.iterrows():
                self.data_buffer[station].append({
                    'timestamp': current_time,
                    'data': row.to_dict()
                })
            
            # é™åˆ¶ç·©è¡å¤§å°
            if len(self.data_buffer[station]) > self.buffer_max_points:
                self.data_buffer[station] = self.data_buffer[station][-self.buffer_max_points:]

    def cleanup_old_files(self):
        """æ¸…ç†èˆŠæª”æ¡ˆ"""
        self.logger.info("åŸ·è¡Œæª”æ¡ˆæ¸…ç†...")
        
        # æ¸…ç†CSVæª”æ¡ˆ
        cutoff_time = datetime.now() - timedelta(hours=self.max_file_age_hours)
        csv_pattern = os.path.join(self.realtime_dir, "realtime_shock_data_*.csv")
        
        deleted_csv = 0
        for file_path in glob.glob(csv_pattern):
            try:
                filename = os.path.basename(file_path)
                timestamp_str = filename.replace('realtime_shock_data_', '').replace('.csv', '')
                file_time = datetime.strptime(timestamp_str, '%Y%m%d_%H%M')
                
                if file_time < cutoff_time:
                    os.remove(file_path)
                    deleted_csv += 1
            except:
                pass
        
        # æ¸…ç†æ—¥èªŒæª”æ¡ˆ
        log_cutoff = datetime.now() - timedelta(days=self.max_log_age_days)
        log_pattern = os.path.join(self.log_dir, "realtime_system_*.log")
        
        deleted_logs = 0
        for file_path in glob.glob(log_pattern):
            try:
                filename = os.path.basename(file_path)
                date_str = filename.replace('realtime_system_', '').replace('.log', '')
                file_date = datetime.strptime(date_str, '%Y%m%d')
                
                if file_date < log_cutoff:
                    os.remove(file_path)
                    deleted_logs += 1
            except:
                pass
        
        if deleted_csv > 0 or deleted_logs > 0:
            self.logger.info(f"æ¸…ç†å®Œæˆ: åˆªé™¤ {deleted_csv} å€‹CSVæª”æ¡ˆ, {deleted_logs} å€‹æ—¥èªŒæª”æ¡ˆ")

    def single_collection(self):
        """åŸ·è¡Œå–®æ¬¡è³‡æ–™æ”¶é›†"""
        try:
            # 1. å°‹æ‰¾ä¸¦æ”¶é›†æœ€æ–°è³‡æ–™
            latest_time = self.get_latest_available_time()
            current_time = datetime.now()
            delay_minutes = (current_time - latest_time).total_seconds() / 60
            
            self.logger.info(f"ğŸ“Š æº–å‚™æ”¶é›†è³‡æ–™ - æœ€æ–°å¯ç”¨æ™‚é–“: {latest_time.strftime('%H:%M')}, å»¶é²: {delay_minutes:.0f}åˆ†é˜")
            
            raw_data = self.fetch_recent_data(latest_time)
            
            # 2. è™•ç†è³‡æ–™
            processed_data = self.process_data(raw_data)
            
            # 3. ä¿å­˜è³‡æ–™
            output_file = self.save_data(processed_data)
            
            # 4. æ›´æ–°ç·©è¡
            self.update_buffer(processed_data)
            
            # 5. è¨˜éŒ„æˆåŠŸ
            self.last_successful_collection = datetime.now()
            self.consecutive_failures = 0
            
            return processed_data, output_file
            
        except Exception as e:
            self.logger.error(f"è³‡æ–™æ”¶é›†å¤±æ•—: {e}")
            self.consecutive_failures += 1
            return pd.DataFrame(), None

    def start_continuous_monitoring(self):
        """å•Ÿå‹•æŒçºŒç›£æ§"""
        self.logger.info("ğŸš€ å•Ÿå‹•æŒçºŒç›£æ§æ¨¡å¼")
        self.logger.info(f"æ”¶é›†é–“éš”: {self.collection_interval} åˆ†é˜")
        self.logger.info(f"æ¸…ç†é »ç‡: æ¯ {self.cleanup_frequency} æ¬¡æ”¶é›†")
        
        self.is_running = True
        
        try:
            while self.is_running:
                self.collection_count += 1
                
                self.logger.info(f"=== ç¬¬ {self.collection_count} æ¬¡æ”¶é›† ===")
                
                # æª¢æŸ¥é€£çºŒå¤±æ•—æ¬¡æ•¸
                if self.consecutive_failures >= self.max_consecutive_failures:
                    self.logger.error(f"é€£çºŒå¤±æ•— {self.consecutive_failures} æ¬¡ï¼Œæš«åœ10åˆ†é˜")
                    time.sleep(600)  # æš«åœ10åˆ†é˜
                    self.consecutive_failures = 0
                
                # åŸ·è¡Œè³‡æ–™æ”¶é›†
                processed_data, output_file = self.single_collection()
                
                # å®šæœŸæ¸…ç†
                if self.collection_count % self.cleanup_frequency == 0:
                    self.cleanup_old_files()
                
                # ç³»çµ±ç‹€æ…‹å ±å‘Š
                buffer_size = sum(len(buffer) for buffer in self.data_buffer.values())
                self.logger.info(f"ç³»çµ±ç‹€æ…‹: ç·©è¡å€ {len(self.data_buffer)} ç«™é», {buffer_size} è¨˜éŒ„")
                
                if not processed_data.empty:
                    self.logger.info(f"âœ… æ”¶é›†æˆåŠŸ: {len(processed_data)} ç­†è¨˜éŒ„")
                else:
                    self.logger.warning("âŒ æœ¬æ¬¡æ”¶é›†ç„¡æœ‰æ•ˆè³‡æ–™")
                
                # ç­‰å¾…ä¸‹æ¬¡æ”¶é›†
                if self.is_running:
                    self.logger.info(f"ç­‰å¾… {self.collection_interval} åˆ†é˜...")
                    time.sleep(self.collection_interval * 60)
                    
        except KeyboardInterrupt:
            self.logger.info("æ”¶åˆ°ä¸­æ–·ä¿¡è™Ÿ")
        finally:
            self.logger.info("ç›£æ§å·²åœæ­¢")
            self.is_running = False

    def get_shock_detection_data(self, station, min_points=6):
        """ç‚ºéœ‡æ³¢æª¢æ¸¬æä¾›è³‡æ–™"""
        if station not in self.data_buffer:
            return pd.DataFrame()
        
        buffer_records = []
        for item in self.data_buffer[station]:
            record = item['data'].copy()
            record['buffer_timestamp'] = item['timestamp']
            buffer_records.append(record)
        
        if len(buffer_records) >= min_points:
            df = pd.DataFrame(buffer_records)
            return df.sort_values(['hour', 'minute'])
        
        return pd.DataFrame()


def run_production_system():
    """åŸ·è¡Œç”Ÿç”¢ç‰ˆç³»çµ±"""
    
    print("=" * 60)
    print("ğŸš€ ç”Ÿç”¢ç‰ˆå³æ™‚äº¤é€šç›£æ§ç³»çµ±")
    print("=" * 60)
    
    # å»ºç«‹ç³»çµ±
    system = ProductionRealtimeSystem()
    
    print("\né¸æ“‡é‹è¡Œæ¨¡å¼:")
    print("1. æ¸¬è©¦æ¨¡å¼ (å–®æ¬¡æ”¶é›†)")
    print("2. ç”Ÿç”¢æ¨¡å¼ (æŒçºŒç›£æ§)")
    print("3. è‡ªå®šç¾©é–“éš”ç›£æ§")
    
    try:
        choice = input("\nè«‹é¸æ“‡ (1/2/3): ").strip()
        
        if choice == "1":
            print("\nğŸ§ª åŸ·è¡Œæ¸¬è©¦æ¨¡å¼...")
            processed_data, output_file = system.single_collection()
            
            if not processed_data.empty:
                print(f"âœ… æ¸¬è©¦æˆåŠŸ!")
                print(f"ğŸ“Š æ”¶é›†åˆ° {len(processed_data)} ç­†è¨˜éŒ„")
                print(f"ğŸ“ æ¶µè“‹ {processed_data['station'].nunique()} å€‹ç«™é»")
                print(f"ğŸ’¾ æª”æ¡ˆ: {output_file}")
            else:
                print("âŒ æ¸¬è©¦å¤±æ•—ï¼Œè«‹æª¢æŸ¥ç¶²è·¯é€£æ¥")
        
        elif choice == "2":
            print("\nğŸš€ å•Ÿå‹•ç”Ÿç”¢æ¨¡å¼ (æ¯5åˆ†é˜æ”¶é›†ä¸€æ¬¡)...")
            print("æŒ‰ Ctrl+C å¯ä»¥å„ªé›…åœæ­¢")
            system.start_continuous_monitoring()
        
        elif choice == "3":
            interval = int(input("è«‹è¼¸å…¥æ”¶é›†é–“éš”(åˆ†é˜): "))
            system.collection_interval = interval
            print(f"\nğŸš€ å•Ÿå‹•è‡ªå®šç¾©ç›£æ§ (æ¯{interval}åˆ†é˜æ”¶é›†ä¸€æ¬¡)...")
            print("æŒ‰ Ctrl+C å¯ä»¥å„ªé›…åœæ­¢")
            system.start_continuous_monitoring()
        
        else:
            print("ç„¡æ•ˆé¸æ“‡")
    
    except KeyboardInterrupt:
        print("\nğŸ‘‹ ç³»çµ±å·²åœæ­¢")
    except Exception as e:
        print(f"\nâŒ ç³»çµ±éŒ¯èª¤: {e}")


if __name__ == "__main__":
    run_production_system()