#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import sys
import numpy as np
import pandas as pd
import tensorflow as tf
from datetime import datetime, timedelta
import logging
from pathlib import Path
import json
from typing import Dict, List, Optional, Tuple
import threading
import time
from collections import deque

# æ·»åŠ å°ˆæ¡ˆè·¯å¾‘
current_dir = Path(__file__).parent
project_root = current_dir.parent.parent.parent
sys.path.append(str(project_root))

from src.data.tdx_tisc_mix_system import OptimizedIntegratedDataCollectionSystem

class MTSTNetRealtimePredictor:
    """
    MT-STNet å³æ™‚é æ¸¬ç³»çµ±
    
    åŠŸèƒ½ï¼š
    1. è¼‰å…¥é è¨“ç·´çš„MT-STNetæ¨¡å‹
    2. æ¥æ”¶å³æ™‚äº¤é€šè³‡æ–™
    3. åŸ·è¡Œå³æ™‚é æ¸¬
    4. è¼¸å‡ºé æ¸¬çµæœçµ¦å‰ç«¯
    """
    
    def __init__(self, model_path=None, config_path=None):
        """åˆå§‹åŒ–å³æ™‚é æ¸¬ç³»çµ±"""
        
        # è¨­å®šè·¯å¾‘
        self.model_dir = current_dir
        self.weights_dir = self.model_dir / "weights"
        self.data_dir = project_root / "data"
        self.output_dir = self.data_dir / "predictions"
        
        # å»ºç«‹è¼¸å‡ºç›®éŒ„
        self.output_dir.mkdir(exist_ok=True)
        
        # è¨­å®šæ—¥èªŒ
        self._setup_logging()
        
        # æ¨¡å‹åƒæ•¸ï¼ˆå¾config.pyè¼‰å…¥ï¼‰
        self.model_params = {
            'input_length': 12,      # è¼¸å…¥æ™‚é–“æ­¥é•·
            'output_length': 12,     # é æ¸¬æ™‚é–“æ­¥é•·
            'site_num': 62,          # ç«™é»æ•¸é‡
            'emb_size': 64,          # åµŒå…¥ç¶­åº¦
            'num_heads': 8,          # æ³¨æ„åŠ›é ­æ•¸
            'num_blocks': 1,         # æ³¨æ„åŠ›å±¤æ•¸
            'batch_size': 32,        # æ‰¹æ¬¡å¤§å°
        }
        
        # è³‡æ–™è™•ç†åƒæ•¸
        self.data_window_minutes = 60    # è³‡æ–™è¦–çª—ï¼ˆåˆ†é˜ï¼‰
        self.prediction_interval = 5     # é æ¸¬é–“éš”ï¼ˆåˆ†é˜ï¼‰
        self.min_data_points = 6         # æœ€å°‘è³‡æ–™é»æ•¸
        
        # ç«™é»æ˜ å°„
        self.station_mapping = self._load_station_mapping()
        self.target_stations = list(self.station_mapping.keys())
        
        # è³‡æ–™ç·©å­˜
        self.data_cache = deque(maxlen=120)  # ä¿å­˜2å°æ™‚è³‡æ–™
        self.prediction_cache = deque(maxlen=50)  # ä¿å­˜é æ¸¬çµæœ
        
        # æ¨¡å‹ç›¸é—œ
        self.model = None
        self.is_model_loaded = False
        self.normalization_params = {'mean': 0, 'std': 1}
        
        # è³‡æ–™æ”¶é›†ç³»çµ±
        self.data_collector = None
        
        # åŸ·è¡Œç‹€æ…‹
        self.is_running = False
        self.prediction_thread = None
        self.last_prediction_time = None
        
        self.logger.info("ğŸš€ MT-STNet å³æ™‚é æ¸¬ç³»çµ±åˆå§‹åŒ–å®Œæˆ")
        self.logger.info(f"ğŸ“Š ç›®æ¨™ç«™é»æ•¸: {len(self.target_stations)}")
        self.logger.info(f"â±ï¸ é æ¸¬é–“éš”: {self.prediction_interval} åˆ†é˜")

    def _setup_logging(self):
        """è¨­å®šæ—¥èªŒç³»çµ±"""
        log_dir = self.data_dir / "logs"
        log_dir.mkdir(exist_ok=True)
        
        log_file = log_dir / f"mt_stnet_predictor_{datetime.now().strftime('%Y%m%d')}.log"
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file, encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger('MTSTNetPredictor')

    def _load_station_mapping(self):
        """è¼‰å…¥ç«™é»æ˜ å°„"""
        station_mapping = {}
        
        # å¾Etag.csvè¼‰å…¥ç«™é»è³‡è¨Š
        etag_file = self.data_dir / 'Taiwan' / 'Etag.csv'
        try:
            if etag_file.exists():
                df = pd.read_csv(etag_file, encoding='utf-8')
                for idx, row in df.iterrows():
                    if pd.notna(row['ç·¨è™Ÿ']):
                        station_id = str(row['ç·¨è™Ÿ']).replace('-', '').replace('.', '')
                        station_name = row.get('åç¨±', f'ç«™é»_{station_id}')
                        station_mapping[station_id] = {
                            'name': station_name,
                            'index': idx,
                            'highway': row.get('åœ‹é“', ''),
                            'direction': row.get('æ–¹å‘', '')
                        }
                
                self.logger.info(f"âœ… è¼‰å…¥ {len(station_mapping)} å€‹ç«™é»æ˜ å°„")
            else:
                self.logger.warning("âš ï¸ Etag.csv ä¸å­˜åœ¨ï¼Œä½¿ç”¨é è¨­ç«™é»")
                # ä½¿ç”¨é è¨­ç«™é»
                default_stations = [
                    '01F0340N', '01F0376N', '01F0413N', '01F0467N', '01F0492N',
                    '01F0511N', '01F0532N', '01F0557N', '01F0584N', '01F0633N'
                ]
                for i, station in enumerate(default_stations):
                    station_mapping[station] = {
                        'name': f'ç«™é»_{station}',
                        'index': i,
                        'highway': '1',
                        'direction': 'N'
                    }
        except Exception as e:
            self.logger.error(f"âŒ è¼‰å…¥ç«™é»æ˜ å°„å¤±æ•—: {e}")
        
        return station_mapping

    def load_model(self, model_path=None):
        """è¼‰å…¥é è¨“ç·´æ¨¡å‹"""
        try:
            if model_path is None:
                # å°‹æ‰¾æœ€æ–°çš„æ¨¡å‹æª”æ¡ˆ
                model_path = self._find_latest_model()
            
            if model_path and os.path.exists(model_path):
                self.logger.info(f"ğŸ“¥ æ‰¾åˆ°æ¨¡å‹æª”æ¡ˆ: {model_path}")
                
                # æª¢æŸ¥æ˜¯å¦ç‚ºTensorFlow checkpointç›®éŒ„
                if os.path.isdir(model_path):
                    checkpoint_file = os.path.join(model_path, "checkpoint")
                    if os.path.exists(checkpoint_file):
                        self.logger.info("ğŸ“ æª¢æ¸¬åˆ°TensorFlow checkpointæ ¼å¼")
                        
                        # å˜—è©¦è¼‰å…¥TensorFlowæ¨¡å‹
                        try:
                            # é€™è£¡éœ€è¦å¯¦éš›çš„MT-STNetæ¨¡å‹æ¶æ§‹
                            # æš«æ™‚æ¨™è¨˜ç‚ºæ‰¾åˆ°æ¨¡å‹ä½†æœªå®Œå…¨è¼‰å…¥
                            self.is_model_loaded = False  # è¨­ç‚ºFalseï¼Œä½¿ç”¨ç°¡åŒ–é æ¸¬
                            self.logger.info("âœ… æ‰¾åˆ°é è¨“ç·´æ¨¡å‹æª”æ¡ˆ")
                            self.logger.info("â„¹ï¸ æ¨¡å‹æ¶æ§‹éœ€è¦å®Œæ•´å¯¦ä½œï¼Œç›®å‰ä½¿ç”¨ç°¡åŒ–é æ¸¬é‚è¼¯")
                            
                            # è¼‰å…¥æ­£è¦åŒ–åƒæ•¸
                            self._load_normalization_params()
                            
                            return True
                        except Exception as e:
                            self.logger.warning(f"âš ï¸ TensorFlowæ¨¡å‹è¼‰å…¥å¤±æ•—: {e}")
                            self.is_model_loaded = False
                            return False
                    else:
                        self.logger.warning("âš ï¸ checkpointæª”æ¡ˆä¸å­˜åœ¨")
                        return False
                else:
                    # å…¶ä»–æ ¼å¼çš„æ¨¡å‹æª”æ¡ˆ
                    self.logger.info("ğŸ“„ æª¢æ¸¬åˆ°å…¶ä»–æ ¼å¼æ¨¡å‹æª”æ¡ˆ")
                    self.is_model_loaded = False
                    return False
            else:
                self.logger.warning("âš ï¸ æœªæ‰¾åˆ°é è¨“ç·´æ¨¡å‹ï¼Œå°‡ä½¿ç”¨ç°¡åŒ–é æ¸¬")
                self.is_model_loaded = False
                return False
                
        except Exception as e:
            self.logger.error(f"âŒ æ¨¡å‹è¼‰å…¥å¤±æ•—: {e}")
            self.is_model_loaded = False
            return False

    def _find_latest_model(self):
        """å°‹æ‰¾æœ€æ–°çš„æ¨¡å‹æª”æ¡ˆ"""
        try:
            # æª¢æŸ¥TensorFlow checkpointæª”æ¡ˆ
            checkpoint_dir = self.weights_dir / "MT_STNet-7"
            if checkpoint_dir.exists():
                checkpoint_file = checkpoint_dir / "checkpoint"
                if checkpoint_file.exists():
                    self.logger.info(f"ğŸ“ æ‰¾åˆ°TensorFlow checkpoint: {checkpoint_file}")
                    return str(checkpoint_dir)
            
            # æª¢æŸ¥å…¶ä»–æ¨¡å‹æ ¼å¼
            model_patterns = [
                self.weights_dir / "MT_STNet-7" / "*.h5",
                self.weights_dir / "MT_STNet-7" / "*.pb",
                self.weights_dir / "*.h5",
                self.weights_dir / "*.pb"
            ]
            
            for pattern in model_patterns:
                if pattern.parent.exists():
                    model_files = list(pattern.parent.glob(pattern.name))
                    if model_files:
                        # è¿”å›æœ€æ–°çš„æª”æ¡ˆ
                        latest_file = max(model_files, key=lambda x: x.stat().st_mtime)
                        return str(latest_file)
            
            return None
        except Exception as e:
            self.logger.error(f"âŒ å°‹æ‰¾æ¨¡å‹æª”æ¡ˆå¤±æ•—: {e}")
            return None

    def _load_normalization_params(self):
        """è¼‰å…¥è³‡æ–™æ­£è¦åŒ–åƒæ•¸"""
        try:
            # å˜—è©¦å¾è¨“ç·´è³‡æ–™è¨ˆç®—æ­£è¦åŒ–åƒæ•¸
            train_file = self.data_dir / 'Taiwan' / 'train.csv'
            if train_file.exists():
                df = pd.read_csv(train_file)
                if 'flow' in df.columns:
                    self.normalization_params['mean'] = df['flow'].mean()
                    self.normalization_params['std'] = df['flow'].std()
                    self.logger.info(f"ğŸ“Š æ­£è¦åŒ–åƒæ•¸: mean={self.normalization_params['mean']:.2f}, std={self.normalization_params['std']:.2f}")
            else:
                # ä½¿ç”¨é è¨­å€¼
                self.normalization_params = {'mean': 1000, 'std': 500}
                self.logger.info("ğŸ“Š ä½¿ç”¨é è¨­æ­£è¦åŒ–åƒæ•¸")
                
        except Exception as e:
            self.logger.error(f"âŒ è¼‰å…¥æ­£è¦åŒ–åƒæ•¸å¤±æ•—: {e}")
            self.normalization_params = {'mean': 1000, 'std': 500}

    def initialize_data_collector(self):
        """åˆå§‹åŒ–è³‡æ–™æ”¶é›†ç³»çµ±"""
        try:
            # ä½¿ç”¨çµ•å°è·¯å¾‘
            data_path = str(self.data_dir)
            self.data_collector = OptimizedIntegratedDataCollectionSystem(base_dir=data_path)
            
            # è¼‰å…¥æ­·å²è³‡æ–™åˆ°ç·©å­˜
            if not self.data_collector.historical_loaded:
                self.data_collector.load_initial_historical_data()
            
            self.logger.info("âœ… è³‡æ–™æ”¶é›†ç³»çµ±åˆå§‹åŒ–å®Œæˆ")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ è³‡æ–™æ”¶é›†ç³»çµ±åˆå§‹åŒ–å¤±æ•—: {e}")
            return False

    def get_realtime_data(self) -> pd.DataFrame:
        """å–å¾—å³æ™‚äº¤é€šè³‡æ–™"""
        try:
            if self.data_collector is None:
                if not self.initialize_data_collector():
                    return pd.DataFrame()
            
            # å¾è³‡æ–™æ”¶é›†ç³»çµ±å–å¾—æœ€æ–°è³‡æ–™
            latest_data = self.data_collector.get_cached_data_for_output(
                time_window_minutes=self.data_window_minutes
            )
            
            if not latest_data.empty:
                # éæ¿¾ç›®æ¨™ç«™é»
                target_data = latest_data[
                    latest_data['station'].isin(self.target_stations)
                ].copy()
                
                self.logger.info(f"ğŸ“Š å–å¾—å³æ™‚è³‡æ–™: {len(target_data)} ç­†è¨˜éŒ„ï¼Œ{target_data['station'].nunique()} å€‹ç«™é»")
                return target_data
            else:
                self.logger.warning("âš ï¸ ç„¡å¯ç”¨çš„å³æ™‚è³‡æ–™")
                return pd.DataFrame()
                
        except Exception as e:
            self.logger.error(f"âŒ å–å¾—å³æ™‚è³‡æ–™å¤±æ•—: {e}")
            return pd.DataFrame()

    def preprocess_data_for_prediction(self, data: pd.DataFrame) -> Optional[np.ndarray]:
        """é è™•ç†è³‡æ–™ç”¨æ–¼é æ¸¬"""
        try:
            if data.empty:
                return None
            
            # ç¢ºä¿æœ‰è¶³å¤ çš„æ™‚é–“æ­¥é•·
            required_timesteps = self.model_params['input_length']
            
            # æŒ‰ç«™é»å’Œæ™‚é–“æ’åº
            data = data.sort_values(['station', 'timestamp'])
            
            # å»ºç«‹æ™‚é–“åºåˆ—çŸ©é™£
            station_sequences = {}
            
            for station in self.target_stations:
                station_data = data[data['station'] == station].copy()
                
                if len(station_data) >= self.min_data_points:
                    # å–æœ€è¿‘çš„è³‡æ–™é»
                    recent_data = station_data.tail(required_timesteps)
                    
                    # æå–ç‰¹å¾µï¼ˆæµé‡ã€é€Ÿåº¦ã€æ—…è¡Œæ™‚é–“ï¼‰
                    features = []
                    for _, row in recent_data.iterrows():
                        feature_vector = [
                            row.get('flow', 0),
                            row.get('median_speed', 0),
                            row.get('avg_travel_time', 0)
                        ]
                        features.append(feature_vector)
                    
                    # å¦‚æœè³‡æ–™ä¸è¶³ï¼Œç”¨æœ€å¾Œä¸€å€‹å€¼å¡«å……
                    while len(features) < required_timesteps:
                        if features:
                            features.insert(0, features[0])
                        else:
                            features.append([0, 0, 0])
                    
                    station_sequences[station] = np.array(features[-required_timesteps:])
            
            if not station_sequences:
                self.logger.warning("âš ï¸ ç„¡è¶³å¤ è³‡æ–™é€²è¡Œé æ¸¬")
                return None
            
            # çµ„åˆæˆæ‰¹æ¬¡æ ¼å¼ [batch_size, timesteps, stations, features]
            batch_data = []
            station_list = []
            
            for station, sequence in station_sequences.items():
                batch_data.append(sequence)
                station_list.append(station)
            
            if batch_data:
                # è½‰æ›ç‚ºnumpyé™£åˆ—ä¸¦æ­£è¦åŒ–
                batch_array = np.array(batch_data)  # [stations, timesteps, features]
                batch_array = np.transpose(batch_array, (1, 0, 2))  # [timesteps, stations, features]
                batch_array = np.expand_dims(batch_array, axis=0)  # [1, timesteps, stations, features]
                
                # æ­£è¦åŒ–æµé‡è³‡æ–™
                batch_array[:, :, :, 0] = (batch_array[:, :, :, 0] - self.normalization_params['mean']) / self.normalization_params['std']
                
                self.logger.info(f"ğŸ“Š é è™•ç†å®Œæˆ: {batch_array.shape}")
                return batch_array, station_list
            
            return None
            
        except Exception as e:
            self.logger.error(f"âŒ è³‡æ–™é è™•ç†å¤±æ•—: {e}")
            return None

    def predict_traffic(self, input_data: np.ndarray, station_list: List[str]) -> Dict:
        """åŸ·è¡Œäº¤é€šé æ¸¬"""
        try:
            current_time = datetime.now()
            
            if self.is_model_loaded and self.model is not None:
                # ä½¿ç”¨çœŸå¯¦æ¨¡å‹é æ¸¬
                predictions = self.model.predict(input_data)
                self.logger.info("ğŸ¤– ä½¿ç”¨MT-STNetæ¨¡å‹é æ¸¬")
            else:
                # ä½¿ç”¨ç°¡åŒ–é æ¸¬é‚è¼¯
                predictions = self._simple_prediction(input_data, station_list)
                self.logger.info("ğŸ“Š ä½¿ç”¨ç°¡åŒ–é æ¸¬é‚è¼¯")
            
            # è™•ç†é æ¸¬çµæœ
            prediction_results = []
            
            for i, station in enumerate(station_list):
                station_info = self.station_mapping.get(station, {})
                
                # è¨ˆç®—é æ¸¬å€¼ï¼ˆé€™è£¡éœ€è¦æ ¹æ“šå¯¦éš›æ¨¡å‹è¼¸å‡ºèª¿æ•´ï¼‰
                if isinstance(predictions, np.ndarray) and len(predictions.shape) >= 2:
                    if i < predictions.shape[1]:
                        predicted_flow = float(predictions[0, i, 0] if len(predictions.shape) > 2 else predictions[0, i])
                    else:
                        predicted_flow = 0
                else:
                    predicted_flow = predictions.get(station, 0)
                
                # åæ­£è¦åŒ–
                if isinstance(predicted_flow, (int, float)):
                    predicted_flow = predicted_flow * self.normalization_params['std'] + self.normalization_params['mean']
                    predicted_flow = max(0, predicted_flow)  # ç¢ºä¿éè² å€¼
                
                # ä¼°ç®—é€Ÿåº¦ï¼ˆåŸºæ–¼æµé‡çš„ç°¡åŒ–æ¨¡å‹ï¼‰
                predicted_speed = self._estimate_speed_from_flow(predicted_flow)
                
                # è¨ˆç®—ä¿¡å¿ƒåº¦
                confidence = self._calculate_confidence(station, predicted_flow)
                
                prediction_result = {
                    'station_id': station,
                    'location_name': station_info.get('name', f'ç«™é»_{station}'),
                    'predicted_flow': round(predicted_flow, 1),
                    'predicted_speed': round(predicted_speed, 1),
                    'confidence': round(confidence, 3),
                    'time_horizon': self.model_params['output_length'] * 5,  # åˆ†é˜
                    'timestamp': current_time.isoformat(),
                    'highway': station_info.get('highway', ''),
                    'direction': station_info.get('direction', '')
                }
                
                prediction_results.append(prediction_result)
            
            # çµ„åˆæœ€çµ‚çµæœ
            result = {
                'predictions': prediction_results,
                'model_version': 'MT-STNet-v1.0',
                'prediction_time': current_time.isoformat(),
                'time_horizon_minutes': self.model_params['output_length'] * 5,
                'total_stations': len(prediction_results),
                'data_source': 'REALTIME'
            }
            
            # å¿«å–é æ¸¬çµæœ
            self.prediction_cache.append(result)
            
            self.logger.info(f"âœ… é æ¸¬å®Œæˆ: {len(prediction_results)} å€‹ç«™é»")
            return result
            
        except Exception as e:
            self.logger.error(f"âŒ é æ¸¬å¤±æ•—: {e}")
            return {'predictions': [], 'error': str(e)}

    def _simple_prediction(self, input_data: np.ndarray, station_list: List[str]) -> Dict:
        """ç°¡åŒ–é æ¸¬é‚è¼¯ï¼ˆç•¶æ¨¡å‹æœªè¼‰å…¥æ™‚ä½¿ç”¨ï¼‰"""
        predictions = {}
        
        try:
            # åŸºæ–¼æ­·å²è¶¨å‹¢çš„ç°¡å–®é æ¸¬
            for i, station in enumerate(station_list):
                if i < input_data.shape[2]:  # ç¢ºä¿ç´¢å¼•æœ‰æ•ˆ
                    # å–æœ€è¿‘å¹¾å€‹æ™‚é–“é»çš„æµé‡
                    recent_flows = input_data[0, -3:, i, 0]  # æœ€è¿‘3å€‹æ™‚é–“é»çš„æµé‡
                    
                    if len(recent_flows) > 0:
                        # è¨ˆç®—è¶¨å‹¢
                        if len(recent_flows) >= 2:
                            trend = recent_flows[-1] - recent_flows[-2]
                        else:
                            trend = 0
                        
                        # é æ¸¬ä¸‹ä¸€å€‹æ™‚é–“é»ï¼ˆåŠ ä¸Šè¶¨å‹¢å’Œä¸€äº›éš¨æ©Ÿæ€§ï¼‰
                        base_prediction = recent_flows[-1] + trend * 0.5
                        
                        # æ·»åŠ æ™‚é–“å› å­ï¼ˆè€ƒæ…®äº¤é€šæ¨¡å¼ï¼‰
                        current_hour = datetime.now().hour
                        time_factor = self._get_time_factor(current_hour)
                        
                        predicted_flow = base_prediction * time_factor
                        predictions[station] = float(predicted_flow)
                    else:
                        predictions[station] = 0
                else:
                    predictions[station] = 0
            
        except Exception as e:
            self.logger.error(f"âŒ ç°¡åŒ–é æ¸¬å¤±æ•—: {e}")
            for station in station_list:
                predictions[station] = 0
        
        return predictions

    def _get_time_factor(self, hour: int) -> float:
        """æ ¹æ“šæ™‚é–“å–å¾—äº¤é€šæµé‡å› å­"""
        # ç°¡åŒ–çš„æ™‚é–“å› å­ï¼ˆåŸºæ–¼ä¸€èˆ¬äº¤é€šæ¨¡å¼ï¼‰
        if 7 <= hour <= 9:  # æ—©é«˜å³°
            return 1.3
        elif 17 <= hour <= 19:  # æ™šé«˜å³°
            return 1.4
        elif 22 <= hour or hour <= 5:  # æ·±å¤œ
            return 0.3
        else:  # å…¶ä»–æ™‚é–“
            return 1.0

    def _estimate_speed_from_flow(self, flow: float) -> float:
        """æ ¹æ“šæµé‡ä¼°ç®—é€Ÿåº¦"""
        # ç°¡åŒ–çš„æµé‡-é€Ÿåº¦é—œä¿‚æ¨¡å‹
        if flow <= 0:
            return 90  # è‡ªç”±æµé€Ÿåº¦
        elif flow <= 1000:
            return 90 - (flow / 1000) * 20  # ç·šæ€§ä¸‹é™
        elif flow <= 2000:
            return 70 - ((flow - 1000) / 1000) * 30
        else:
            return max(20, 40 - ((flow - 2000) / 1000) * 15)  # æ“å¡ç‹€æ…‹

    def _calculate_confidence(self, station: str, predicted_flow: float) -> float:
        """è¨ˆç®—é æ¸¬ä¿¡å¿ƒåº¦"""
        base_confidence = 0.75
        
        # æ ¹æ“šè³‡æ–™å“è³ªèª¿æ•´
        if predicted_flow > 0:
            base_confidence += 0.1
        
        # æ ¹æ“šç«™é»é‡è¦æ€§èª¿æ•´
        if station in self.target_stations[:20]:  # å‰20å€‹é‡è¦ç«™é»
            base_confidence += 0.05
        
        # æ ¹æ“šæ™‚é–“èª¿æ•´ï¼ˆç™½å¤©ä¿¡å¿ƒåº¦è¼ƒé«˜ï¼‰
        current_hour = datetime.now().hour
        if 6 <= current_hour <= 22:
            base_confidence += 0.05
        
        return min(0.95, max(0.5, base_confidence))

    def save_predictions(self, predictions: Dict) -> str:
        """ä¿å­˜é æ¸¬çµæœ"""
        try:
            current_time = datetime.now()
            filename = f"mt_stnet_predictions_{current_time.strftime('%Y%m%d_%H%M')}.json"
            filepath = self.output_dir / filename
            
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(predictions, f, ensure_ascii=False, indent=2)
            
            self.logger.info(f"ğŸ’¾ é æ¸¬çµæœå·²ä¿å­˜: {filepath}")
            return str(filepath)
            
        except Exception as e:
            self.logger.error(f"âŒ ä¿å­˜é æ¸¬çµæœå¤±æ•—: {e}")
            return ""

    def get_latest_predictions(self) -> Dict:
        """å–å¾—æœ€æ–°é æ¸¬çµæœ"""
        if self.prediction_cache:
            return self.prediction_cache[-1]
        else:
            return {'predictions': [], 'message': 'æš«ç„¡é æ¸¬è³‡æ–™'}

    def run_single_prediction(self) -> Dict:
        """åŸ·è¡Œå–®æ¬¡é æ¸¬"""
        try:
            self.logger.info("ğŸ”® é–‹å§‹åŸ·è¡Œå–®æ¬¡é æ¸¬...")
            
            # å–å¾—å³æ™‚è³‡æ–™
            realtime_data = self.get_realtime_data()
            if realtime_data.empty:
                return {'predictions': [], 'error': 'ç„¡å¯ç”¨çš„å³æ™‚è³‡æ–™'}
            
            # é è™•ç†è³‡æ–™
            processed_result = self.preprocess_data_for_prediction(realtime_data)
            if processed_result is None:
                return {'predictions': [], 'error': 'è³‡æ–™é è™•ç†å¤±æ•—'}
            
            input_data, station_list = processed_result
            
            # åŸ·è¡Œé æ¸¬
            predictions = self.predict_traffic(input_data, station_list)
            
            # ä¿å­˜çµæœ
            if predictions.get('predictions'):
                self.save_predictions(predictions)
            
            return predictions
            
        except Exception as e:
            self.logger.error(f"âŒ å–®æ¬¡é æ¸¬å¤±æ•—: {e}")
            return {'predictions': [], 'error': str(e)}

    def start_continuous_prediction(self):
        """å•Ÿå‹•æŒçºŒé æ¸¬"""
        self.logger.info("ğŸš€ å•Ÿå‹•MT-STNetæŒçºŒé æ¸¬æ¨¡å¼")
        self.is_running = True
        
        def prediction_loop():
            while self.is_running:
                try:
                    # æª¢æŸ¥æ˜¯å¦éœ€è¦åŸ·è¡Œé æ¸¬
                    current_time = datetime.now()
                    
                    if (self.last_prediction_time is None or 
                        (current_time - self.last_prediction_time).total_seconds() >= self.prediction_interval * 60):
                        
                        self.logger.info(f"â° åŸ·è¡Œå®šæ™‚é æ¸¬ - {current_time.strftime('%H:%M:%S')}")
                        
                        # åŸ·è¡Œé æ¸¬
                        result = self.run_single_prediction()
                        
                        if result.get('predictions'):
                            self.last_prediction_time = current_time
                            self.logger.info(f"âœ… é æ¸¬å®Œæˆ: {len(result['predictions'])} å€‹ç«™é»")
                        else:
                            self.logger.warning("âš ï¸ é æ¸¬ç„¡çµæœ")
                    
                    # ç­‰å¾…30ç§’å¾Œå†æª¢æŸ¥
                    time.sleep(30)
                    
                except Exception as e:
                    self.logger.error(f"âŒ é æ¸¬å¾ªç’°éŒ¯èª¤: {e}")
                    time.sleep(60)  # éŒ¯èª¤æ™‚ç­‰å¾…æ›´é•·æ™‚é–“
        
        # å•Ÿå‹•é æ¸¬ç·šç¨‹
        self.prediction_thread = threading.Thread(target=prediction_loop, daemon=True)
        self.prediction_thread.start()
        
        self.logger.info(f"âœ… æŒçºŒé æ¸¬å·²å•Ÿå‹•ï¼Œé–“éš”: {self.prediction_interval} åˆ†é˜")

    def stop_continuous_prediction(self):
        """åœæ­¢æŒçºŒé æ¸¬"""
        self.is_running = False
        if self.prediction_thread and self.prediction_thread.is_alive():
            self.prediction_thread.join(timeout=5)
        self.logger.info("ğŸ›‘ æŒçºŒé æ¸¬å·²åœæ­¢")

    def get_system_status(self) -> Dict:
        """å–å¾—ç³»çµ±ç‹€æ…‹"""
        return {
            'model_loaded': self.is_model_loaded,
            'is_running': self.is_running,
            'last_prediction_time': self.last_prediction_time.isoformat() if self.last_prediction_time else None,
            'prediction_interval_minutes': self.prediction_interval,
            'target_stations_count': len(self.target_stations),
            'cached_predictions_count': len(self.prediction_cache),
            'data_collector_available': self.data_collector is not None
        }


def main():
    """ä¸»å‡½æ•¸ - ç”¨æ–¼æ¸¬è©¦"""
    print("ğŸš€ MT-STNet å³æ™‚é æ¸¬ç³»çµ±æ¸¬è©¦")
    
    predictor = MTSTNetRealtimePredictor()
    
    print("\né¸æ“‡æ¸¬è©¦æ¨¡å¼:")
    print("1. è¼‰å…¥æ¨¡å‹æ¸¬è©¦")
    print("2. å–®æ¬¡é æ¸¬æ¸¬è©¦")
    print("3. æŒçºŒé æ¸¬æ¨¡å¼")
    print("4. ç³»çµ±ç‹€æ…‹æª¢æŸ¥")
    
    choice = input("è«‹é¸æ“‡ (1-4): ").strip()
    
    if choice == "1":
        print("\nğŸ” æ¸¬è©¦æ¨¡å‹è¼‰å…¥...")
        success = predictor.load_model()
        if success:
            print("âœ… æ¨¡å‹è¼‰å…¥æˆåŠŸ")
        else:
            print("âš ï¸ æ¨¡å‹è¼‰å…¥å¤±æ•—ï¼Œå°‡ä½¿ç”¨ç°¡åŒ–é æ¸¬")
    
    elif choice == "2":
        print("\nğŸ”® åŸ·è¡Œå–®æ¬¡é æ¸¬æ¸¬è©¦...")
        predictor.load_model()
        result = predictor.run_single_prediction()
        
        if result.get('predictions'):
            print(f"âœ… é æ¸¬æˆåŠŸ: {len(result['predictions'])} å€‹ç«™é»")
            print("\nğŸ“Š é æ¸¬çµæœé è¦½:")
            for pred in result['predictions'][:5]:  # é¡¯ç¤ºå‰5å€‹
                print(f"  {pred['location_name']}: æµé‡={pred['predicted_flow']:.1f}, é€Ÿåº¦={pred['predicted_speed']:.1f}km/h, ä¿¡å¿ƒåº¦={pred['confidence']:.2f}")
        else:
            print(f"âŒ é æ¸¬å¤±æ•—: {result.get('error', 'æœªçŸ¥éŒ¯èª¤')}")
    
    elif choice == "3":
        print("\nğŸš€ å•Ÿå‹•æŒçºŒé æ¸¬æ¨¡å¼...")
        predictor.load_model()
        predictor.start_continuous_prediction()
        
        try:
            print("ğŸ’¡ æŒ‰ Ctrl+C åœæ­¢é æ¸¬")
            while predictor.is_running:
                time.sleep(1)
        except KeyboardInterrupt:
            print("\nğŸ›‘ åœæ­¢æŒçºŒé æ¸¬...")
            predictor.stop_continuous_prediction()
    
    elif choice == "4":
        print("\nğŸ“Š ç³»çµ±ç‹€æ…‹æª¢æŸ¥...")
        status = predictor.get_system_status()
        for key, value in status.items():
            print(f"  {key}: {value}")
    
    print("\nğŸ‘‹ æ¸¬è©¦å®Œæˆ")


if __name__ == "__main__":
    main()
