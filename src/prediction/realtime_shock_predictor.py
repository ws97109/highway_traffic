#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import pandas as pd
import numpy as np
import os
import glob
import pickle
import time
import threading
import logging
from datetime import datetime, timedelta
from collections import defaultdict, deque
import warnings
warnings.filterwarnings('ignore')

# å°å…¥ç¾æœ‰çš„æª¢æ¸¬å™¨
from ..detection.final_optimized_detector import FinalOptimizedShockDetector
from .propagation_system import RealDataShockWavePropagationAnalyzer

class RealtimeShockPredictor:
    """
    å³æ™‚è¡æ“Šæ³¢é æ¸¬ç³»çµ±
    
    åŠŸèƒ½ï¼š
    1. ç›£æ§å³æ™‚è³‡æ–™æª”æ¡ˆ
    2. ä½¿ç”¨è¨“ç·´å¥½çš„æ¨¡å‹é€²è¡Œè¡æ“Šæ³¢æª¢æ¸¬
    3. é æ¸¬è¡æ“Šæ³¢å‚³æ’­è»Œè·¡
    4. æä¾›å³æ™‚é è­¦è³‡è¨Š
    """
    
    def __init__(self, data_dir, config=None):
        """åˆå§‹åŒ–å³æ™‚é æ¸¬ç³»çµ±"""
        
        # åŸºæœ¬è¨­å®š
        self.data_dir = data_dir
        self.realtime_dir = os.path.join(data_dir, "realtime_data")
        self.model_dir = os.path.join(data_dir, "models")
        self.prediction_dir = os.path.join(data_dir, "predictions")
        
        # å»ºç«‹ç›®éŒ„
        os.makedirs(self.prediction_dir, exist_ok=True)
        os.makedirs(self.model_dir, exist_ok=True)
        
        # è¨­å®šæ—¥èªŒ
        self._setup_logging()
        
        # è¼‰å…¥æª¢æ¸¬å™¨å’Œåˆ†æå™¨
        self.detector = FinalOptimizedShockDetector()
        
        # åˆå§‹åŒ–å‚³æ’­åˆ†æå™¨ï¼ˆéœ€è¦ç«™é»è³‡è¨Šï¼‰
        etag_file = os.path.join(data_dir, "Taiwan", "Etag.csv")
        distance_file = os.path.join(data_dir, "Taiwan", "dis.csv")
        
        # æª¢æŸ¥å¤šå€‹å¯èƒ½çš„è·¯å¾‘
        possible_paths = [
            data_dir,
            os.path.join(os.path.dirname(data_dir), "åœ‹é“", "data"),
            os.path.join(os.path.dirname(data_dir), "data"),
            "../data"
        ]
        
        found_etag = False
        found_distance = False
        
        for path in possible_paths:
            test_etag = os.path.join(path, "Taiwan", "Etag.csv")
            test_distance = os.path.join(path, "Taiwan", "dis.csv")
            
            if os.path.exists(test_etag) and os.path.exists(test_distance):
                etag_file = test_etag
                distance_file = test_distance
                found_etag = True
                found_distance = True
                self.logger.info(f"âœ… æ‰¾åˆ°ç«™é»è³‡è¨Šæª”æ¡ˆ: {path}")
                break
        
        if found_etag and found_distance:
            try:
                self.propagation_analyzer = RealDataShockWavePropagationAnalyzer(etag_file, distance_file)
                self.logger.info("âœ… å‚³æ’­åˆ†æå™¨åˆå§‹åŒ–æˆåŠŸ")
            except Exception as e:
                self.propagation_analyzer = None
                self.logger.warning(f"âš ï¸ å‚³æ’­åˆ†æå™¨åˆå§‹åŒ–å¤±æ•—: {e}")
        else:
            self.propagation_analyzer = None
            self.logger.warning("âš ï¸ ç«™é»è³‡è¨Šæª”æ¡ˆæœªæ‰¾åˆ°ï¼Œå‚³æ’­åˆ†æåŠŸèƒ½å°‡å—é™")
            self.logger.info(f"å°‹æ‰¾çš„æª”æ¡ˆè·¯å¾‘: {etag_file}, {distance_file}")
        
        # é…ç½®åƒæ•¸
        default_config = {
            'data_window_minutes': 60,      # åˆ†æè¦–çª—ï¼š60åˆ†é˜
            'min_data_points': 12,          # æœ€å°‘è³‡æ–™é»ï¼š12å€‹ï¼ˆ1å°æ™‚ï¼‰
            'prediction_horizon': 30,       # é æ¸¬æ™‚é–“ç¯„åœï¼š30åˆ†é˜
            'monitoring_interval': 60,      # ç›£æ§é–“éš”ï¼š1åˆ†é˜
            'file_scan_interval': 30,       # æª”æ¡ˆæƒæé–“éš”ï¼š30ç§’
            'max_prediction_distance': 50,  # æœ€å¤§é æ¸¬è·é›¢ï¼š50å…¬é‡Œ
        }
        
        self.config = {**default_config, **(config or {})}
        
        # è³‡æ–™ç·©è¡å€
        self.data_buffer = defaultdict(lambda: deque(maxlen=100))  # æ¯ç«™é»ä¿ç•™100å€‹è³‡æ–™é»
        self.last_processed_files = set()
        self.active_shocks = {}  # æ´»èºçš„è¡æ“Šæ³¢äº‹ä»¶
        self.prediction_history = deque(maxlen=1000)  # é æ¸¬æ­·å²
        
        # ç³»çµ±ç‹€æ…‹
        self.is_running = False
        self.last_prediction_time = None
        
        # ç«™é»åˆ†çµ„ï¼ˆæŒ‰åœ‹é“å’Œæ–¹å‘ï¼‰
        self.station_groups = self._build_station_groups()
        
        self.logger.info("ğŸš€ å³æ™‚è¡æ“Šæ³¢é æ¸¬ç³»çµ±åˆå§‹åŒ–å®Œæˆ")
        self.logger.info(f"ğŸ“Š é…ç½®åƒæ•¸: {self.config}")
        self.logger.info(f"ğŸ“ ç«™é»åˆ†çµ„: {len(self.station_groups)} å€‹ç¾¤çµ„")

    def _setup_logging(self):
        """è¨­å®šæ—¥èªŒ"""
        log_file = os.path.join(self.data_dir, "logs", f"shock_predictor_{datetime.now().strftime('%Y%m%d')}.log")
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file, encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger('ShockPredictor')

    def _build_station_groups(self):
        """å»ºç«‹ç«™é»åˆ†çµ„"""
        groups = {
            '01F_N': [],  # åœ‹é“1è™ŸåŒ—å‘
            '01F_S': [],  # åœ‹é“1è™Ÿå—å‘
            '03F_N': [],  # åœ‹é“3è™ŸåŒ—å‘
            '03F_S': [],  # åœ‹é“3è™Ÿå—å‘
        }
        
        # åŸºæ–¼ç¾æœ‰çš„ç›®æ¨™é–€æ¶æ¸…å–®
        target_stations = [
            # åœ‹é“1è™ŸåŒ—å‘
            '01F0340N', '01F0376N', '01F0413N', '01F0467N', '01F0492N',
            '01F0511N', '01F0532N', '01F0557N', '01F0584N', '01F0633N',
            '01F0664N', '01F0681N', '01F0699N', '01F0750N', '01F0880N',
            '01F0928N', '01F0956N', '01F0980N', '01F1045N',
            # åœ‹é“1è™Ÿå—å‘
            '01F0339S', '01F0376S', '01F0413S', '01F0467S', '01F0492S',
            '01F0511S', '01F0532S', '01F0557S', '01F0578S', '01F0633S',
            '01F0664S', '01F0681S', '01F0699S', '01F0750S', '01F0880S',
            '01F0928S', '01F0950S', '01F0980S', '01F1045S',
            # åœ‹é“3è™ŸåŒ—å‘
            '03F0447N', '03F0498N', '03F0525N', '03F0559N', '03F0648N',
            '03F0698N', '03F0746N', '03F0783N', '03F0846N', '03F0961N',
            '03F0996N', '03F1022N',
            # åœ‹é“3è™Ÿå—å‘
            '03F0447S', '03F0498S', '03F0525S', '03F0559S', '03F0648S',
            '03F0698S', '03F0746S', '03F0783S', '03F0846S', '03F0961S',
            '03F0996S', '03F1022S'
        ]
        
        for station in target_stations:
            if station.startswith('01F') and station.endswith('N'):
                groups['01F_N'].append(station)
            elif station.startswith('01F') and station.endswith('S'):
                groups['01F_S'].append(station)
            elif station.startswith('03F') and station.endswith('N'):
                groups['03F_N'].append(station)
            elif station.startswith('03F') and station.endswith('S'):
                groups['03F_S'].append(station)
        
        # æŒ‰é‡Œç¨‹æ’åº
        for group_name in groups:
            groups[group_name].sort(key=self._extract_mileage)
        
        return groups

    def _extract_mileage(self, station):
        """å¾ç«™é»ç·¨è™Ÿæå–é‡Œç¨‹"""
        try:
            # å¾ 01F0340N æå– 034.0
            mileage_str = station[3:7]  # 0340
            return float(mileage_str[:-1] + '.' + mileage_str[-1])
        except:
            return 0

    def scan_new_data_files(self):
        """æƒææ–°çš„è³‡æ–™æª”æ¡ˆ"""
        pattern = os.path.join(self.realtime_dir, "realtime_shock_data_*.csv")
        all_files = glob.glob(pattern)
        
        new_files = []
        for file_path in all_files:
            if file_path not in self.last_processed_files:
                # æª¢æŸ¥æª”æ¡ˆæ˜¯å¦åœ¨åˆç†æ™‚é–“ç¯„åœå…§
                try:
                    filename = os.path.basename(file_path)
                    timestamp_str = filename.replace('realtime_shock_data_', '').replace('.csv', '')
                    file_time = datetime.strptime(timestamp_str, '%Y%m%d_%H%M')
                    
                    # åªè™•ç†æœ€è¿‘2å°æ™‚çš„æª”æ¡ˆ
                    if datetime.now() - file_time < timedelta(hours=2):
                        new_files.append(file_path)
                except:
                    continue
        
        return new_files

    def load_and_process_file(self, file_path):
        """è¼‰å…¥ä¸¦è™•ç†å–®å€‹æª”æ¡ˆ"""
        try:
            df = pd.read_csv(file_path, encoding='utf-8')
            
            if df.empty:
                return
            
            # ç‚ºæ¯å€‹ç«™é»æ›´æ–°ç·©è¡å€
            for station in df['station'].unique():
                station_data = df[df['station'] == station].sort_values(['hour', 'minute'])
                
                for _, row in station_data.iterrows():
                    data_point = {
                        'timestamp': datetime.now(),
                        'date': row['date'],
                        'hour': row['hour'],
                        'minute': row['minute'],
                        'flow': row['flow'],
                        'median_speed': row['median_speed'],
                        'avg_travel_time': row['avg_travel_time']
                    }
                    
                    self.data_buffer[station].append(data_point)
            
            self.last_processed_files.add(file_path)
            self.logger.info(f"âœ… è™•ç†æª”æ¡ˆ: {os.path.basename(file_path)} ({len(df)} ç­†è¨˜éŒ„)")
            
        except Exception as e:
            self.logger.error(f"âŒ æª”æ¡ˆè™•ç†å¤±æ•—: {file_path} - {e}")

    def detect_shocks_for_station(self, station):
        """ç‚ºç‰¹å®šç«™é»æª¢æ¸¬è¡æ“Šæ³¢"""
        if station not in self.data_buffer or len(self.data_buffer[station]) < self.config['min_data_points']:
            return []
        
        # è½‰æ›ç·©è¡å€è³‡æ–™ç‚ºDataFrame
        buffer_data = list(self.data_buffer[station])
        df = pd.DataFrame([point for point in buffer_data])
        
        # ç¢ºä¿è³‡æ–™æ ¼å¼æ­£ç¢º
        if 'median_speed' not in df.columns or 'flow' not in df.columns:
            return []
        
        # ä½¿ç”¨æª¢æ¸¬å™¨åˆ†æ
        try:
            shocks = self.detector.detect_significant_shocks(df)
            
            # æ·»åŠ é æ¸¬æ™‚é–“æˆ³å’Œç«™é»è³‡è¨Š
            for shock in shocks:
                shock['detection_time'] = datetime.now()
                shock['station'] = station
                shock['prediction_id'] = f"{station}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            
            return shocks
            
        except Exception as e:
            self.logger.error(f"âŒ ç«™é» {station} è¡æ“Šæ³¢æª¢æ¸¬å¤±æ•—: {e}")
            return []

    def predict_shock_propagation(self, shock, group_stations):
        """é æ¸¬è¡æ“Šæ³¢å‚³æ’­"""
        if not self.propagation_analyzer:
            return {}
        
        source_station = shock['station']
        
        # æ‰¾åˆ°æºç«™é»åœ¨ç¾¤çµ„ä¸­çš„ä½ç½®
        if source_station not in group_stations:
            return {}
        
        source_idx = group_stations.index(source_station)
        predictions = {}
        
        # é æ¸¬ä¸‹æ¸¸ç«™é»
        for i in range(source_idx + 1, min(source_idx + 6, len(group_stations))):  # é æ¸¬æœ€å¤š5å€‹ä¸‹æ¸¸ç«™é»
            target_station = group_stations[i]
            
            # è¨ˆç®—è·é›¢
            distance = self.propagation_analyzer.get_station_distance(source_station, target_station)
            if not distance:
                continue
            
            # é æ¸¬åˆ°é”æ™‚é–“ï¼ˆä½¿ç”¨æ­·å²å¹³å‡å‚³æ’­é€Ÿåº¦ï¼‰
            avg_propagation_speed = 25  # km/hï¼Œå¯ä»¥å¾æ­·å²è³‡æ–™ä¸­è¨ˆç®—
            travel_time_minutes = (distance / avg_propagation_speed) * 60
            
            predicted_arrival = datetime.now() + timedelta(minutes=travel_time_minutes)
            
            prediction = {
                'target_station': target_station,
                'source_shock': shock,
                'distance': distance,
                'predicted_arrival': predicted_arrival,
                'travel_time_minutes': travel_time_minutes,
                'propagation_speed': avg_propagation_speed,
                'confidence': self._calculate_confidence(shock, distance),
                'prediction_time': datetime.now()
            }
            
            predictions[target_station] = prediction
        
        return predictions

    def _calculate_confidence(self, shock, distance):
        """è¨ˆç®—é æ¸¬ä¿¡å¿ƒåº¦"""
        # åŸºæ–¼è¡æ“Šæ³¢å¼·åº¦å’Œè·é›¢è¨ˆç®—ä¿¡å¿ƒåº¦
        base_confidence = 0.7
        
        # è¡æ“Šæ³¢ç­‰ç´šå½±éŸ¿
        level_weights = {'mild': 0.6, 'moderate': 0.8, 'severe': 0.9}
        level_factor = level_weights.get(shock['level'], 0.7)
        
        # è·é›¢å½±éŸ¿ï¼ˆè·é›¢è¶Šé ä¿¡å¿ƒåº¦è¶Šä½ï¼‰
        distance_factor = max(0.3, 1 - (distance / 100))  # 100kmä»¥ä¸Šä¿¡å¿ƒåº¦æœ€ä½0.3
        
        # è¡æ“Šæ³¢å¼·åº¦å½±éŸ¿
        strength_factor = min(1.0, shock['shock_strength'] / 50)  # å¼·åº¦50%ä»¥ä¸Šä¿¡å¿ƒåº¦æœ€é«˜
        
        confidence = base_confidence * level_factor * distance_factor * strength_factor
        return min(0.95, max(0.2, confidence))

    def run_single_prediction_cycle(self):
        """åŸ·è¡Œå–®æ¬¡é æ¸¬å¾ªç’°"""
        try:
            self.logger.info("ğŸ” é–‹å§‹é æ¸¬å¾ªç’°...")
            
            # 1. æƒææ–°æª”æ¡ˆ
            new_files = self.scan_new_data_files()
            
            # 2. è™•ç†æ–°æª”æ¡ˆ
            for file_path in new_files:
                self.load_and_process_file(file_path)
            
            # 3. ç‚ºæ¯å€‹ç¾¤çµ„æª¢æ¸¬è¡æ“Šæ³¢
            all_predictions = {}
            total_shocks = 0
            
            for group_name, stations in self.station_groups.items():
                group_shocks = []
                group_predictions = {}
                
                # æª¢æ¸¬æ¯å€‹ç«™é»çš„è¡æ“Šæ³¢
                for station in stations:
                    station_shocks = self.detect_shocks_for_station(station)
                    group_shocks.extend(station_shocks)
                    total_shocks += len(station_shocks)
                    
                    # ç‚ºæ¯å€‹è¡æ“Šæ³¢é æ¸¬å‚³æ’­
                    for shock in station_shocks:
                        propagation_predictions = self.predict_shock_propagation(shock, stations)
                        group_predictions.update(propagation_predictions)
                
                if group_shocks or group_predictions:
                    all_predictions[group_name] = {
                        'shocks': group_shocks,
                        'propagation_predictions': group_predictions
                    }
            
            # 4. ä¿å­˜é æ¸¬çµæœ
            if all_predictions:
                self.save_predictions(all_predictions)
                self.logger.info(f"âœ… é æ¸¬å®Œæˆ: æª¢æ¸¬åˆ° {total_shocks} å€‹è¡æ“Šæ³¢ï¼Œç”Ÿæˆ {sum(len(p['propagation_predictions']) for p in all_predictions.values())} å€‹å‚³æ’­é æ¸¬")
            else:
                self.logger.info("â„¹ï¸ æœ¬æ¬¡å¾ªç’°ç„¡è¡æ“Šæ³¢æª¢æ¸¬çµæœ")
            
            self.last_prediction_time = datetime.now()
            return all_predictions
            
        except Exception as e:
            self.logger.error(f"âŒ é æ¸¬å¾ªç’°å¤±æ•—: {e}")
            return {}

    def save_predictions(self, predictions):
        """ä¿å­˜é æ¸¬çµæœ"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M')
        
        # ä¿å­˜è©³ç´°é æ¸¬çµæœ
        prediction_file = os.path.join(self.prediction_dir, f"shock_predictions_{timestamp}.json")
        
        # è½‰æ›ç‚ºå¯åºåˆ—åŒ–çš„æ ¼å¼
        serializable_predictions = {}
        for group_name, group_data in predictions.items():
            serializable_predictions[group_name] = {
                'shocks': [],
                'propagation_predictions': {}
            }
            
            # è™•ç†è¡æ“Šæ³¢è³‡æ–™
            for shock in group_data['shocks']:
                shock_data = shock.copy()
                # è½‰æ›æ™‚é–“ç‚ºå­—ä¸²
                for time_field in ['detection_time', 'start_time', 'end_time']:
                    if time_field in shock_data and hasattr(shock_data[time_field], 'isoformat'):
                        shock_data[time_field] = shock_data[time_field].isoformat()
                    elif time_field in shock_data and isinstance(shock_data[time_field], str):
                        # å·²ç¶“æ˜¯å­—ä¸²ï¼Œä¿æŒä¸è®Š
                        pass
                serializable_predictions[group_name]['shocks'].append(shock_data)
            
            # è™•ç†å‚³æ’­é æ¸¬
            for station, prediction in group_data['propagation_predictions'].items():
                pred_data = prediction.copy()
                # è½‰æ›æ™‚é–“ç‚ºå­—ä¸²
                for time_field in ['predicted_arrival', 'prediction_time']:
                    if time_field in pred_data and hasattr(pred_data[time_field], 'isoformat'):
                        pred_data[time_field] = pred_data[time_field].isoformat()
                    elif time_field in pred_data and isinstance(pred_data[time_field], str):
                        # å·²ç¶“æ˜¯å­—ä¸²ï¼Œä¿æŒä¸è®Š
                        pass
                
                # è™•ç†source_shockä¸­çš„æ™‚é–“æ¬„ä½
                if 'source_shock' in pred_data and isinstance(pred_data['source_shock'], dict):
                    source_shock = pred_data['source_shock'].copy()
                    for time_field in ['detection_time', 'start_time', 'end_time']:
                        if time_field in source_shock and hasattr(source_shock[time_field], 'isoformat'):
                            source_shock[time_field] = source_shock[time_field].isoformat()
                    pred_data['source_shock'] = source_shock
                
                serializable_predictions[group_name]['propagation_predictions'][station] = pred_data
        
        # å¯«å…¥JSONæª”æ¡ˆ
        import json
        try:
            with open(prediction_file, 'w', encoding='utf-8') as f:
                json.dump(serializable_predictions, f, ensure_ascii=False, indent=2, default=str)
        except Exception as e:
            self.logger.error(f"âŒ JSONå„²å­˜å¤±æ•—: {e}")
            return
        
        # ä¿å­˜ç°¡åŒ–çš„CSVæ ¼å¼ç”¨æ–¼å¿«é€ŸæŸ¥è©¢
        csv_records = []
        for group_name, group_data in predictions.items():
            for prediction in group_data['propagation_predictions'].values():
                try:
                    # å®‰å…¨åœ°ç²å–æ™‚é–“å­—ä¸²
                    predicted_arrival = prediction.get('predicted_arrival', '')
                    if hasattr(predicted_arrival, 'strftime'):
                        arrival_str = predicted_arrival.strftime('%Y-%m-%d %H:%M')
                    elif isinstance(predicted_arrival, str):
                        arrival_str = predicted_arrival[:16] if len(predicted_arrival) >= 16 else predicted_arrival
                    else:
                        arrival_str = str(predicted_arrival)
                    
                    record = {
                        'group': group_name,
                        'source_station': prediction['source_shock']['station'],
                        'target_station': prediction['target_station'],
                        'shock_level': prediction['source_shock']['level'],
                        'shock_strength': prediction['source_shock']['shock_strength'],
                        'predicted_arrival': arrival_str,
                        'travel_time_minutes': prediction['travel_time_minutes'],
                        'distance_km': prediction['distance'],
                        'confidence': prediction['confidence']
                    }
                    csv_records.append(record)
                except Exception as e:
                    self.logger.warning(f"âš ï¸ è™•ç†é æ¸¬è¨˜éŒ„æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                    continue
        
        if csv_records:
            csv_file = os.path.join(self.prediction_dir, f"shock_predictions_summary_{timestamp}.csv")
            try:
                pd.DataFrame(csv_records).to_csv(csv_file, index=False, encoding='utf-8')
                self.logger.info(f"ğŸ’¾ é æ¸¬çµæœå·²ä¿å­˜: {prediction_file}")
                self.logger.info(f"ğŸ“Š é æ¸¬æ‘˜è¦å·²ä¿å­˜: {csv_file}")
            except Exception as e:
                self.logger.error(f"âŒ CSVå„²å­˜å¤±æ•—: {e}")
        else:
            self.logger.info(f"ğŸ’¾ é æ¸¬çµæœå·²ä¿å­˜: {prediction_file} (ç„¡å‚³æ’­é æ¸¬)")

    def start_continuous_prediction(self):
        """å•Ÿå‹•æŒçºŒé æ¸¬"""
        self.logger.info("ğŸš€ å•Ÿå‹•æŒçºŒé æ¸¬æ¨¡å¼")
        self.logger.info(f"â±ï¸ ç›£æ§é–“éš”: {self.config['monitoring_interval']} ç§’")
        
        self.is_running = True
        
        try:
            while self.is_running:
                predictions = self.run_single_prediction_cycle()
                
                # ç­‰å¾…ä¸‹æ¬¡é æ¸¬
                if self.is_running:
                    time.sleep(self.config['monitoring_interval'])
                    
        except KeyboardInterrupt:
            self.logger.info("æ”¶åˆ°ä¸­æ–·ä¿¡è™Ÿ")
        finally:
            self.logger.info("é æ¸¬ç³»çµ±å·²åœæ­¢")
            self.is_running = False

    def get_latest_predictions(self, max_age_minutes=30):
        """ç²å–æœ€æ–°çš„é æ¸¬çµæœ"""
        pattern = os.path.join(self.prediction_dir, "shock_predictions_summary_*.csv")
        files = glob.glob(pattern)
        
        recent_files = []
        cutoff_time = datetime.now() - timedelta(minutes=max_age_minutes)
        
        for file_path in files:
            try:
                filename = os.path.basename(file_path)
                timestamp_str = filename.replace('shock_predictions_summary_', '').replace('.csv', '')
                file_time = datetime.strptime(timestamp_str, '%Y%m%d_%H%M')
                
                if file_time >= cutoff_time:
                    recent_files.append((file_time, file_path))
            except:
                continue
        
        if not recent_files:
            return pd.DataFrame()
        
        # è¼‰å…¥æœ€æ–°çš„æª”æ¡ˆ
        recent_files.sort(reverse=True)
        latest_file = recent_files[0][1]
        
        try:
            return pd.read_csv(latest_file, encoding='utf-8')
        except:
            return pd.DataFrame()

    def stop(self):
        """åœæ­¢é æ¸¬ç³»çµ±"""
        self.is_running = False
        self.logger.info("ğŸ›‘ é æ¸¬ç³»çµ±åœæ­¢æŒ‡ä»¤å·²ç™¼å‡º")


def main():
    """ä¸»å‡½æ•¸"""
    # è¨­å®šè·¯å¾‘
    base_dir = "../data"
    
    # å»ºç«‹é æ¸¬ç³»çµ±
    predictor = RealtimeShockPredictor(base_dir)
    
    print("=" * 60)
    print("ğŸš€ å³æ™‚è¡æ“Šæ³¢é æ¸¬ç³»çµ±")
    print("=" * 60)
    print("\né¸æ“‡é‹è¡Œæ¨¡å¼:")
    print("1. æ¸¬è©¦æ¨¡å¼ (å–®æ¬¡é æ¸¬)")
    print("2. æŒçºŒé æ¸¬æ¨¡å¼")
    print("3. æŸ¥çœ‹æœ€æ–°é æ¸¬çµæœ")
    
    try:
        choice = input("\nè«‹é¸æ“‡ (1/2/3): ").strip()
        
        if choice == "1":
            print("\nğŸ§ª åŸ·è¡Œæ¸¬è©¦é æ¸¬...")
            predictions = predictor.run_single_prediction_cycle()
            
            if predictions:
                print(f"âœ… æ¸¬è©¦æˆåŠŸ!")
                for group_name, group_data in predictions.items():
                    print(f"ğŸ“Š {group_name}: {len(group_data['shocks'])} å€‹è¡æ“Šæ³¢, {len(group_data['propagation_predictions'])} å€‹å‚³æ’­é æ¸¬")
            else:
                print("â„¹ï¸ ç›®å‰ç„¡è¡æ“Šæ³¢æª¢æ¸¬çµæœ")
        
        elif choice == "2":
            print("\nğŸš€ å•Ÿå‹•æŒçºŒé æ¸¬æ¨¡å¼...")
            print("æŒ‰ Ctrl+C å¯ä»¥åœæ­¢")
            predictor.start_continuous_prediction()
        
        elif choice == "3":
            print("\nğŸ“Š è¼‰å…¥æœ€æ–°é æ¸¬çµæœ...")
            latest_predictions = predictor.get_latest_predictions()
            
            if not latest_predictions.empty:
                print(f"âœ… æ‰¾åˆ° {len(latest_predictions)} å€‹é æ¸¬:")
                print(latest_predictions.to_string(index=False))
            else:
                print("â„¹ï¸ ç›®å‰ç„¡å¯ç”¨çš„é æ¸¬çµæœ")
        
        else:
            print("ç„¡æ•ˆé¸æ“‡")
    
    except KeyboardInterrupt:
        print("\nğŸ‘‹ ç³»çµ±å·²åœæ­¢")
    except Exception as e:
        print(f"\nâŒ ç³»çµ±éŒ¯èª¤: {e}")


if __name__ == "__main__":
    main()