"""
RAG ç³»çµ±è©•ä¼°å·¥å…·
"""

import json
import asyncio
import numpy as np
from typing import List, Dict, Any, Tuple
from loguru import logger
from pathlib import Path

class RAGEvaluator:
    """RAG ç³»çµ±è©•ä¼°å™¨"""
    
    def __init__(self, rag_chat_system):
        """åˆå§‹åŒ–è©•ä¼°å™¨"""
        self.rag_chat = rag_chat_system
        self.evaluation_results = []
        
    def load_test_questions(self, test_file: str = None) -> List[Dict[str, Any]]:
        """è¼‰å…¥æ¸¬è©¦å•é¡Œ"""
        if test_file is None:
            # ä½¿ç”¨å…§å»ºæ¸¬è©¦å•é¡Œ
            return self._get_default_test_questions()
        
        with open(test_file, 'r', encoding='utf-8') as f:
            return json.load(f)
    
    def _get_default_test_questions(self) -> List[Dict[str, Any]]:
        """ç²å–é è¨­æ¸¬è©¦å•é¡Œ"""
        return [
            {
                "id": "q1",
                "question": "åœ‹é“ä¸€è™Ÿçš„æ¨™æº–è»Šé“å¯¬åº¦æ˜¯å¤šå°‘ï¼Ÿ",
                "category": "åŸºç¤è¨­æ–½",
                "expected_keywords": ["è»Šé“å¯¬", "3.5", "3.6", "å…¬å°º"],
                "difficulty": "ç°¡å–®"
            },
            {
                "id": "q2", 
                "question": "é«˜é€Ÿå…¬è·¯çš„ç¸±å‘å¡åº¦è¨­è¨ˆæœ‰ä»€éº¼é™åˆ¶ï¼Ÿ",
                "category": "å·¥ç¨‹è¨­è¨ˆ",
                "expected_keywords": ["ç¸±å‘å¡åº¦", "è¨­è¨ˆ", "é™åˆ¶", "%"],
                "difficulty": "ä¸­ç­‰"
            },
            {
                "id": "q3",
                "question": "åœ‹é“ä¸‰è™Ÿç›¸æ¯”åœ‹é“ä¸€è™Ÿåœ¨è·¯é¢è¨­è¨ˆä¸Šæœ‰ä»€éº¼ç‰¹è‰²ï¼Ÿ",
                "category": "æ¯”è¼ƒåˆ†æ", 
                "expected_keywords": ["åœ‹é“ä¸‰è™Ÿ", "åœ‹é“ä¸€è™Ÿ", "å·®ç•°", "ç‰¹è‰²"],
                "difficulty": "å›°é›£"
            },
            {
                "id": "q4",
                "question": "ä»€éº¼æƒ…æ³ä¸‹æœƒè¨­ç½®è¼”åŠ©è»Šé“ï¼Ÿè¼”åŠ©è»Šé“çš„å¯¬åº¦æ¨™æº–æ˜¯ä»€éº¼ï¼Ÿ",
                "category": "å°ˆæ¥­çŸ¥è­˜",
                "expected_keywords": ["è¼”åŠ©è»Šé“", "è¨­ç½®", "å¯¬åº¦", "æ¨™æº–"],
                "difficulty": "ä¸­ç­‰"
            },
            {
                "id": "q5",
                "question": "é«˜é€Ÿå…¬è·¯æ›²ç‡åŠå¾‘çš„è¨­è¨ˆè€ƒé‡æœ‰å“ªäº›ï¼Ÿ",
                "category": "å·¥ç¨‹è¨­è¨ˆ",
                "expected_keywords": ["æ›²ç‡åŠå¾‘", "è¨­è¨ˆ", "å®‰å…¨", "é€Ÿåº¦"],
                "difficulty": "å›°é›£"
            }
        ]
    
    async def evaluate_single_question(self, question_data: Dict[str, Any]) -> Dict[str, Any]:
        """è©•ä¼°å–®ä¸€å•é¡Œ"""
        question = question_data["question"]
        expected_keywords = question_data.get("expected_keywords", [])
        
        logger.info(f"è©•ä¼°å•é¡Œ: {question}")
        
        # ç²å–å›ç­”
        try:
            answer = await self.rag_chat.chat(question)
            
            # è©•ä¼°å›ç­”å“è³ª
            scores = self._evaluate_answer(answer, expected_keywords)
            
            result = {
                "question_id": question_data["id"],
                "question": question,
                "answer": answer,
                "category": question_data.get("category", "æœªåˆ†é¡"),
                "difficulty": question_data.get("difficulty", "æœªçŸ¥"),
                "scores": scores,
                "timestamp": asyncio.get_event_loop().time()
            }
            
            return result
            
        except Exception as e:
            logger.error(f"è©•ä¼°å•é¡Œæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return {
                "question_id": question_data["id"],
                "question": question,
                "answer": f"éŒ¯èª¤: {str(e)}",
                "category": question_data.get("category", "æœªåˆ†é¡"),
                "difficulty": question_data.get("difficulty", "æœªçŸ¥"),
                "scores": {"error": True},
                "timestamp": asyncio.get_event_loop().time()
            }
    
    def _evaluate_answer(self, answer: str, expected_keywords: List[str]) -> Dict[str, float]:
        """è©•ä¼°å›ç­”å“è³ª"""
        scores = {}
        
        # 1. é—œéµè©è¦†è“‹ç‡
        if expected_keywords:
            found_keywords = sum(1 for keyword in expected_keywords 
                               if keyword.lower() in answer.lower())
            scores["keyword_coverage"] = found_keywords / len(expected_keywords)
        else:
            scores["keyword_coverage"] = 0.0
        
        # 2. å›ç­”é•·åº¦è©•åˆ†ï¼ˆé©ä¸­é•·åº¦å¾—åˆ†è¼ƒé«˜ï¼‰
        answer_length = len(answer)
        if 50 <= answer_length <= 500:
            scores["length_score"] = 1.0
        elif answer_length < 50:
            scores["length_score"] = answer_length / 50
        else:
            scores["length_score"] = max(0.5, 1 - (answer_length - 500) / 1000)
        
        # 3. è³‡è¨Šå¯†åº¦ï¼ˆæ ¹æ“šæ¨™é»ç¬¦è™Ÿå’Œçµæ§‹è©•ä¼°ï¼‰
        sentences = len([s for s in answer.split('ã€‚') if s.strip()])
        if sentences > 0:
            avg_sentence_length = answer_length / sentences
            # é©ä¸­çš„å¥å­é•·åº¦å¾—åˆ†è¼ƒé«˜
            if 20 <= avg_sentence_length <= 80:
                scores["info_density"] = 1.0
            else:
                scores["info_density"] = max(0.3, 1 - abs(avg_sentence_length - 50) / 100)
        else:
            scores["info_density"] = 0.0
        
        # 4. å°ˆæ¥­è¡“èªä½¿ç”¨ï¼ˆæª¢æŸ¥æ˜¯å¦åŒ…å«ç›¸é—œè¡“èªï¼‰
        technical_terms = ["å…¬å°º", "è»Šé“", "å¡åº¦", "åŠå¾‘", "è¨­è¨ˆ", "æ¨™æº–", "åœ‹é“", "é«˜é€Ÿå…¬è·¯"]
        found_terms = sum(1 for term in technical_terms if term in answer)
        scores["technical_score"] = min(1.0, found_terms / 5)  # æœ€å¤š5å€‹è¡“èªæ»¿åˆ†
        
        # 5. ç¸½é«”è©•åˆ†ï¼ˆåŠ æ¬Šå¹³å‡ï¼‰
        weights = {
            "keyword_coverage": 0.3,
            "length_score": 0.2, 
            "info_density": 0.2,
            "technical_score": 0.3
        }
        
        scores["overall_score"] = sum(scores[key] * weights[key] for key in weights)
        
        return scores
    
    async def run_full_evaluation(self, test_file: str = None) -> Dict[str, Any]:
        """åŸ·è¡Œå®Œæ•´è©•ä¼°"""
        logger.info("é–‹å§‹åŸ·è¡Œ RAG ç³»çµ±å®Œæ•´è©•ä¼°...")
        
        # è¼‰å…¥æ¸¬è©¦å•é¡Œ
        test_questions = self.load_test_questions(test_file)
        logger.info(f"è¼‰å…¥ {len(test_questions)} å€‹æ¸¬è©¦å•é¡Œ")
        
        # æ¸…é™¤èŠå¤©æ­·å²ä»¥ç¢ºä¿å…¬å¹³æ¸¬è©¦
        self.rag_chat.clear_history()
        
        # è©•ä¼°æ¯å€‹å•é¡Œ
        results = []
        for i, question_data in enumerate(test_questions, 1):
            logger.info(f"è©•ä¼°é€²åº¦: {i}/{len(test_questions)}")
            result = await self.evaluate_single_question(question_data)
            results.append(result)
            
            # çŸ­æš«å»¶é²é¿å…éæ–¼é »ç¹çš„è«‹æ±‚
            await asyncio.sleep(2)
        
        # è¨ˆç®—çµ±è¨ˆçµæœ
        stats = self._calculate_statistics(results)
        
        evaluation_report = {
            "evaluation_time": asyncio.get_event_loop().time(),
            "total_questions": len(test_questions),
            "results": results,
            "statistics": stats
        }
        
        self.evaluation_results.append(evaluation_report)
        logger.info("è©•ä¼°å®Œæˆ")
        
        return evaluation_report
    
    def _calculate_statistics(self, results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """è¨ˆç®—è©•ä¼°çµ±è¨ˆ"""
        # éæ¿¾å‡ºæœ‰æ•ˆçµæœ
        valid_results = [r for r in results if not r["scores"].get("error", False)]
        
        if not valid_results:
            return {"error": "æ²’æœ‰æœ‰æ•ˆçš„è©•ä¼°çµæœ"}
        
        # æå–å„é …è©•åˆ†
        scores = {
            "keyword_coverage": [r["scores"]["keyword_coverage"] for r in valid_results],
            "length_score": [r["scores"]["length_score"] for r in valid_results],
            "info_density": [r["scores"]["info_density"] for r in valid_results], 
            "technical_score": [r["scores"]["technical_score"] for r in valid_results],
            "overall_score": [r["scores"]["overall_score"] for r in valid_results]
        }
        
        # è¨ˆç®—çµ±è¨ˆæŒ‡æ¨™
        stats = {}
        for score_type, values in scores.items():
            stats[score_type] = {
                "mean": np.mean(values),
                "std": np.std(values),
                "min": np.min(values),
                "max": np.max(values),
                "median": np.median(values)
            }
        
        # æŒ‰é›£åº¦åˆ†çµ„çµ±è¨ˆ
        difficulty_stats = {}
        for difficulty in ["ç°¡å–®", "ä¸­ç­‰", "å›°é›£"]:
            difficulty_results = [r for r in valid_results if r["difficulty"] == difficulty]
            if difficulty_results:
                difficulty_scores = [r["scores"]["overall_score"] for r in difficulty_results]
                difficulty_stats[difficulty] = {
                    "count": len(difficulty_results),
                    "mean_score": np.mean(difficulty_scores)
                }
        
        stats["by_difficulty"] = difficulty_stats
        
        # æŒ‰é¡åˆ¥åˆ†çµ„çµ±è¨ˆ
        category_stats = {}
        categories = set(r["category"] for r in valid_results)
        for category in categories:
            category_results = [r for r in valid_results if r["category"] == category]
            category_scores = [r["scores"]["overall_score"] for r in category_results]
            category_stats[category] = {
                "count": len(category_results),
                "mean_score": np.mean(category_scores)
            }
        
        stats["by_category"] = category_stats
        
        return stats
    
    def print_evaluation_report(self, report: Dict[str, Any]):
        """åˆ—å°è©•ä¼°å ±å‘Š"""
        print("\n" + "="*60)
        print("ğŸ” RAG ç³»çµ±è©•ä¼°å ±å‘Š")
        print("="*60)
        
        stats = report["statistics"]
        if "error" in stats:
            print(f"è©•ä¼°å¤±æ•—: {stats['error']}")
            return
        
        # ç¸½é«”çµ±è¨ˆ
        overall = stats["overall_score"]
        print(f"ç¸½é«”è©•åˆ†: {overall['mean']:.3f} Â± {overall['std']:.3f}")
        print(f"è©•åˆ†ç¯„åœ: {overall['min']:.3f} - {overall['max']:.3f}")
        print(f"ä¸­ä½æ•¸: {overall['median']:.3f}")
        
        # å„é …æŒ‡æ¨™
        print(f"\nğŸ“Š è©³ç´°è©•åˆ†:")
        metrics = ["keyword_coverage", "length_score", "info_density", "technical_score"]
        metric_names = ["é—œéµè©è¦†è“‹ç‡", "å›ç­”é•·åº¦", "è³‡è¨Šå¯†åº¦", "å°ˆæ¥­è¡“èª"]
        
        for metric, name in zip(metrics, metric_names):
            score = stats[metric]["mean"]
            print(f"  {name}: {score:.3f}")
        
        # æŒ‰é›£åº¦çµ±è¨ˆ
        if "by_difficulty" in stats and stats["by_difficulty"]:
            print(f"\nğŸ“ˆ æŒ‰é›£åº¦åˆ†çµ„:")
            for difficulty, data in stats["by_difficulty"].items():
                print(f"  {difficulty}: {data['mean_score']:.3f} ({data['count']} é¡Œ)")
        
        # æŒ‰é¡åˆ¥çµ±è¨ˆ
        if "by_category" in stats and stats["by_category"]:
            print(f"\nğŸ“‚ æŒ‰é¡åˆ¥åˆ†çµ„:")
            for category, data in stats["by_category"].items():
                print(f"  {category}: {data['mean_score']:.3f} ({data['count']} é¡Œ)")
        
        print("="*60)
    
    def save_evaluation_report(self, report: Dict[str, Any], output_file: str):
        """å„²å­˜è©•ä¼°å ±å‘Š"""
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2, default=str)
        
        logger.info(f"è©•ä¼°å ±å‘Šå·²å„²å­˜è‡³: {output_file}")

if __name__ == "__main__":
    # é€™è£¡å¯ä»¥æ·»åŠ ç¨ç«‹æ¸¬è©¦ä»£ç¢¼
    print("RAG è©•ä¼°å·¥å…·æ¨¡çµ„")